{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be117eb7",
   "metadata": {},
   "source": [
    "# MSTL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0e9dc",
   "metadata": {},
   "source": [
    "The notebook is structured as follows:\n",
    "1. Overview of the modeling pipeline defined in the custom `heat_forecast.pipeline.mstl` module, which implements the MSTL model and its configuration.\n",
    "2. Hyperparameter tuning through rolling-origin cross-validation.\n",
    "3. Final evaluation on a held-out test set (a comparative analysis of model performance is provided in the separate notebook `Test comparison.ipynb`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c2162",
   "metadata": {},
   "source": [
    "## Import libreries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12d73a",
   "metadata": {},
   "source": [
    "You can run the notebook in two ways:\n",
    "\n",
    "1. **Google Colab**: place the project folder `heat-forecast` in **MyDrive**. The setup cell below will mount Drive and automatically add `MyDrive/heat-forecast/src` to `sys.path` so `import heat_forecast` works out of the box.\n",
    "\n",
    "2. **Local machine**:\n",
    "\n",
    "   * **Installing our package:** from the project root, run `pip install -e .` once (editable install). Then you can open the notebook anywhere and import the package normally.\n",
    "   * **Alternative:** if you’re running the notebook from `.../heat-forecast/notebooks/` without installing the package, the setup cell will detect `../src` and automatically add it to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "# --- Detect if running on Google Colab & Set base dir ---\n",
    "# %cd /home/giovanni.lombardi/heat-forecast/notebooks\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Install required packages only if not already installed\n",
    "def pip_install(pkg: str):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Set base directory and handle environment\n",
    "if in_colab():\n",
    "    # Make sure IPython is modern (avoids the old %autoreload/imp issue if you ever use it)\n",
    "    pip_install(\"ipython>=8.25\")\n",
    "    pip_install(\"ipykernel>=6.29\")\n",
    "    \n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    for pkg in [\"statsmodels\", \"statsforecast\", \"mlforecast\"]:\n",
    "        pip_install(pkg)\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set base directory to your Drive project folder\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/heat-forecast')\n",
    "\n",
    "    # Add `src/` to sys.path for custom package imports\n",
    "    SRC_PATH = BASE_DIR / 'src'\n",
    "    if str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "\n",
    "    # Sanity checks (helpful error messages if path is wrong)\n",
    "    assert SRC_PATH.exists(), f\"Expected '{SRC_PATH}' to exist. Fix BASE_DIR.\"\n",
    "    pkg_dir = SRC_PATH / \"heat_forecast\"\n",
    "    assert pkg_dir.exists(), f\"Expected '{pkg_dir}' package directory.\"\n",
    "    init_file = pkg_dir / \"__init__.py\"\n",
    "    assert init_file.exists(), f\"Missing '{init_file}'. Add it so Python treats this as a package.\"\n",
    "\n",
    "else:\n",
    "    # Local: either rely on editable install (pip install -e .) or add src/ when running from repo\n",
    "    # Assume notebook lives in PROJECT_ROOT/notebooks/\n",
    "    BASE_DIR = Path.cwd().resolve().parent\n",
    "    SRC_PATH = BASE_DIR / \"src\"\n",
    "\n",
    "    added_src = False\n",
    "    if (SRC_PATH / \"heat_forecast\").exists() and str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "        added_src = True\n",
    "\n",
    "# --- Logging setup ---\n",
    "import logging\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR  = (BASE_DIR / \"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / \"run.log\"\n",
    "PREV_LOG = LOG_DIR / \"run.prev.log\"\n",
    "\n",
    "# If there's a previous run.log with content, archive it to run.prev.log\n",
    "if LOG_FILE.exists() and LOG_FILE.stat().st_size > 0:\n",
    "    try:\n",
    "        # Replace old run.prev.log if present\n",
    "        if PREV_LOG.exists():\n",
    "            PREV_LOG.unlink()\n",
    "        LOG_FILE.rename(PREV_LOG)\n",
    "    except Exception as e:\n",
    "        # Fall back to truncating if rename fails (e.g., file locked)\n",
    "        print(f\"[warn] Could not archive previous log: {e}. Truncating current run.log.\")\n",
    "        LOG_FILE.write_text(\"\")\n",
    "\n",
    "# Configure logging: fresh file for this run + echo to notebook/stdout\n",
    "file_handler   = logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\")\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "                        datefmt=\"%m-%d %H:%M:%S\")\n",
    "file_handler.setFormatter(fmt)\n",
    "stream_handler.setFormatter(fmt)\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.handlers[:] = [file_handler, stream_handler]  # replace handlers (important in notebooks)\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "# Use Rome time\n",
    "logging.Formatter.converter = lambda *args: datetime.now(ZoneInfo(\"Europe/Rome\")).timetuple()\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "logging.info(\"=== Logging started (fresh current run) ===\")\n",
    "logging.info(\"Previous run (if any): %s\", PREV_LOG if PREV_LOG.exists() else \"none\")\n",
    "\n",
    "if added_src:\n",
    "    logging.info(\"heat_forecast not installed; added src/ to sys.path\")\n",
    "else:\n",
    "    logging.info(\"heat_forecast imported without modifying sys.path (likely installed)\")\n",
    "\n",
    "OPTUNA_DIR = BASE_DIR / \"results\" / \"finetuning\" / \"lstm\"\n",
    "OPTUNA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(\"BASE_DIR (make sure it's '*/heat-forecast/', else cd and re-run): %s\", BASE_DIR)\n",
    "logging.info(\"LOG_DIR: %s\", LOG_DIR)\n",
    "logging.info(\"OPTUNA_DIR: %s\", OPTUNA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d62d74",
   "metadata": {},
   "source": [
    "Ensure [compatibility with Numba](https://numba.readthedocs.io/en/stable/user/installing.html#numba-support-info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "import sys, numpy, numba\n",
    "logging.info(\"=== Current Environment ===\")\n",
    "logging.info(\"Python : %s\", sys.version.split()[0])\n",
    "logging.info(\"NumPy  : %s\", numpy.__version__)\n",
    "logging.info(\"Numba  : %s\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c92fc8",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Standard Library ---\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import stat\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "\n",
    "# --- Scientific Stack ---\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Statsmodels ---\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "\n",
    "# --- Nixtla ---\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    AutoETS, Naive, SeasonalNaive\n",
    ")\n",
    "from statsforecast.utils import ConformalIntervals\n",
    "\n",
    "# --- Scikit-learn ---\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# --- Utility Libraries ---\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "\n",
    "# --- Custom package: heat_forecast ---\n",
    "from heat_forecast.pipeline import MSTLPipeline, MSTLConfig\n",
    "\n",
    "from heat_forecast.utils.cv_utils import (\n",
    "    get_cv_params_v2, get_cv_params_for_test\n",
    ")\n",
    "from heat_forecast.utils.datasplit import generate_sets\n",
    "from heat_forecast.utils.decomposition import (\n",
    "    add_annual_component, decompose_annual_seasonality, remove_annual_component\n",
    ")\n",
    "from heat_forecast.utils.evaluation import (\n",
    "    barplot_cv, custom_evaluate, \n",
    "    display_cv_summary, display_metrics, plot_cv_metric_by_cutoff\n",
    ")\n",
    "from heat_forecast.utils.plotting import (\n",
    "    configure_time_axes, custom_plot_results, plot_cutoff_results, interactive_plot_cutoff_results\n",
    ")\n",
    "from heat_forecast.utils.transforms import (\n",
    "    get_lambdas, make_transformer, transform_column\n",
    ")\n",
    "\n",
    "# --- Plot Config ---\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "mpl.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 18,\n",
    "    'axes.grid': True,\n",
    "    'axes.grid.which': 'both',\n",
    "})\n",
    "\n",
    "# --- Cleanup Utility ---\n",
    "from heat_forecast.utils.fileshandling import remove_tree\n",
    "\n",
    "# --- YAML Customization ---\n",
    "from heat_forecast.utils.yaml import safe_dump_yaml\n",
    "\n",
    "logging.info(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a397680",
   "metadata": {},
   "source": [
    "Import pre-elaborated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d84daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'heat.csv'\n",
    "aux_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'auxiliary.csv'\n",
    "heat_df = pd.read_csv(heat_path, parse_dates=['ds'])\n",
    "aux_df = pd.read_csv(aux_path, parse_dates=['ds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36fa95",
   "metadata": {},
   "source": [
    "## Tutorial of the model's pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bd748",
   "metadata": {},
   "source": [
    "Define the forecast horizon and split the data into training, validation, and test sets accordingly. For now, the validation period used for model tuning and selection is also referred to as the 'test set'. The true test set (the portion of data reserved exclusively for final evaluation) will cover the period from May 2024 to May 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_type = 'week' # 'day' for one day horizon, 'week' for one week horizon\n",
    "facilities = ['F1', 'F2', 'F3', 'F4', 'F5']  # Specify the facilities to use, e.g. ['F1', 'F2', ... ]\n",
    "\n",
    "# === Do not edit below ===\n",
    "if horizon_type == 'day':\n",
    "    h = 24 \n",
    "elif horizon_type == 'week':\n",
    "    h = 24 * 7\n",
    "else:\n",
    "    raise ValueError(\"Unsupported horizon type. Use 'day' or 'week'.\")\n",
    "\n",
    "start_train = pd.to_datetime('2019-07-01') #'2019-07-01' is the fist date available\n",
    "end_train   = pd.to_datetime('2024-01-18')\n",
    "end_test    = end_train + pd.Timedelta(hours=h) # single validation window\n",
    "\n",
    "heat_facilities_df = heat_df[heat_df['unique_id'].isin(facilities)].copy()\n",
    "uids = heat_facilities_df['unique_id'].unique()\n",
    "n_uids = len(uids)\n",
    "\n",
    "heat_sets, _ = generate_sets(target_df=heat_facilities_df, \n",
    "                             start_train=start_train, \n",
    "                             end_train=end_train, \n",
    "                             end_test=end_test, \n",
    "                             aux_flag=False,\n",
    "                             max_lag=0,\n",
    "                             verbose=True)\n",
    "heat_train_df, heat_test_df = heat_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b081282",
   "metadata": {},
   "source": [
    "### Apply a transformation to stabilize the variance of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure transformation ---\n",
    "transform  = \"boxcox\"   # choices: \"boxcox\", \"log\", \"arcsinh\", \"arcsinh2\" for arcsinh(./2), \"arcsinh10\", or \"none\"\n",
    "lam_method = \"loglik\"   # only used if boxcox, choices: \"guerrero\", \"loglik\"\n",
    "winter_focus = True     # only used if boxcox, whether to focus on winter season\n",
    "season_length = 365     # Must be integer, used for guerrero only\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Transform data ---\n",
    "# pre‐compute lambdas per series if needed\n",
    "if transform == \"boxcox\":\n",
    "    lambdas = get_lambdas(\n",
    "        heat_train_df, \n",
    "        method=lam_method, \n",
    "        winter_focus=winter_focus, \n",
    "        season_length=season_length,\n",
    "        verbose=True\n",
    "    )\n",
    "    name = f\"boxcox_{lam_method} with winter focus\" if winter_focus else f\"boxcox_{lam_method}\"\n",
    "else:\n",
    "    lambdas = None\n",
    "    name = transform if transform != \"none\" else \"no transformation\"\n",
    "\n",
    "# apply the transformation\n",
    "fwd = make_transformer(transform, 'y', lambdas, inv=False)\n",
    "for df in (heat_train_df, heat_test_df):\n",
    "    df[\"y_transformed\"] = transform_column(df, fwd)\n",
    "\n",
    "# --- Plot the tansformed data ---\n",
    "fig, axes = plt.subplots(n_uids, 1, figsize=(15, n_uids*3))\n",
    "if n_uids == 1:\n",
    "    axes = [axes]\n",
    "for uid, ax in zip(heat_train_df[\"unique_id\"].unique(), axes):\n",
    "    grp = heat_train_df.query(\"unique_id == @uid\")\n",
    "    sns.lineplot(data=grp, x=\"ds\", y=\"y_transformed\", ax=ax)\n",
    "    ax.set_title(f\"Series: {uid}\")\n",
    "\n",
    "configure_time_axes(axes, heat_train_df['ds'])\n",
    "\n",
    "fig.suptitle('Heat Demand Transformed with: ' + name)\n",
    "fig.supxlabel('Date')\n",
    "fig.supylabel('Transformed Heat Demand')\n",
    "fig.tight_layout(rect=[0.01, 0.01, 0.99, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70b536",
   "metadata": {},
   "source": [
    "After applying Box–Cox or log/arcsinh transforms, we observe a handful of unusually low values, particularly in series F3, that deviate substantially from the mean. In principle, we could impute them (for example, with forward fill), but I’ve chosen to retain them for two reasons:\n",
    "\n",
    "1. **Seasonal context:** These dips occur almost exclusively during the warmer months, and our forecasting models are specifically calibrated to prevent summer variability from bleeding into, and inflating, our winter forecasts.\n",
    "2. **Built-in robustness of MSTL:** Robust MSTL inherently down-weights abrupt, isolated declines. Moreover, since these outliers occur mostly during summer, well ahead of the high-load period, even if some of them partially leak into the seasonal or trend components, the model typically has sufficient time to recalibrate before peak demand begins.\n",
    "\n",
    "By leaving these summer anomalies intact, we leverage MSTL's outlier-resistance without introducing potential bias from imputation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7055c",
   "metadata": {},
   "source": [
    "### Extract annual seasonality on daily aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7bdc3",
   "metadata": {},
   "source": [
    "Directly applying MSTL to capture annual, weekly, and daily seasonalities at the hourly level is computationally expensive. To address this, we adopt a two-step decomposition strategy.\n",
    "\n",
    "1. We extract annual seasonality from data aggregated at the daily level.\n",
    "2. We then apply MSTL to the deseasonalized hourly series to isolate weekly and daily components only.\n",
    "\n",
    "For extracting the annual seasonal pattern from the daily-aggregated data, we considered three methods:\n",
    "\n",
    "* **Classical Decomposition (CD)**: the simplest approach, where the seasonal component is computed as the average of values separated by one year. This results in a fixed seasonal pattern, perfectly periodic with a yearly cycle.\n",
    "\n",
    "* **STL** ([Cleveland et al., 1990](https://www.math.unm.edu/~lil/Stat581/STL.pdf)): a more flexible method that extracts seasonal and trend components iteratively using a combination of LOESS smoothing and moving averages. Unlike CD, STL allows the seasonal pattern to evolve slowly over time.\n",
    "\n",
    "* **Robust STL** (also discussed in the same paper): an extension of STL designed to detect and down-weight outliers during the iterative decomposition process.\n",
    "\n",
    "Initial experiments revealed that Robust STL is not suitable in our setting. Counterintuitively, the robust weighting tended to amplify the influence of outliers rather than suppress them. This likely stems from the limited number of annual cycles in our dataset (only five years of data) which reduces the algorithm's ability to identify and isolate outliers. Moreover, real-world anomalies such as unexpected cold spells can span multiple days, further complicating the model’s attempt to distinguish them from genuine seasonal structure. As a result, the algorithm mistakenly incorporated such irregular patterns into the seasonal component.\n",
    "\n",
    "STL and CD performed similarly in initial comparisons. Given the limited number of cycles, we concluded that the added complexity of STL does not justify its use. In particular, STL requires careful tuning of multiple parameters (six in the original formulation and even more in `statsmodels`’ implementation). Considering the computational cost and tuning burden, we chose to proceed with Classical Decomposition for extracting annual seasonality.\n",
    "\n",
    "To prevent annual seasonality from absorbing any weekly structure, we also experimented with an optional post-processing LOWESS smoothing step (`smooth_loess` flag). However, early trials showed that this additional smoothing offered no practical benefit. In fact, it occasionally masked real, sharp transitions between high and low demand periods. Consequently, we decided to exclude this step from the final procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5078524",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_method = \"CD\"  # choices: \"STL\", \"CD\" (classical decomposition), \"none\"\n",
    "robust_stl = True            # Set to True if you want to use robust STL decomposition (only used if decomposition_method is \"STL\")\n",
    "smooth_loess = False         # Set to True if you want to smooth the seasonal component using LOWESS after its extraction \n",
    "window_loess = 7             # Window size for LOWESS smoothing (e.g. 7 days) (only used if smooth_loess is True)\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Decompose the series ---\n",
    "# aggregate to daily frequency\n",
    "heat_daily_train_df = (\n",
    "    heat_train_df\n",
    "    .groupby(['unique_id', pd.Grouper(key='ds', freq='D')])\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()  \n",
    ")\n",
    "\n",
    "# apply decomposition + additional smoothing of the seasonal component if requested\n",
    "results_STL = decompose_annual_seasonality(\n",
    "    decomposition_method=decomposition_method,\n",
    "    df=heat_daily_train_df,\n",
    "    target_col='y_transformed',\n",
    "    robust=robust_stl,\n",
    "    seasonal_STL=11,            # default seasonal window length for STL\n",
    "    smooth_loess=smooth_loess,  \n",
    "    window_loess=window_loess,\n",
    "    data_in_dcmp=True           # Include the original data in the decomposition DataFrame\n",
    ")\n",
    "\n",
    "# --- Plot the decomposition ---\n",
    "for uid, dcmp in results_STL.items():\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharey=True)\n",
    "    sns.lineplot(data=dcmp, x='ds', y='data',     ax=axes[0])\n",
    "    sns.lineplot(data=dcmp, x='ds', y='trend',    ax=axes[1])\n",
    "    sns.lineplot(data=dcmp, x='ds', y='seasonal', ax=axes[2])\n",
    "    sns.lineplot(data=dcmp, x='ds', y='remainder',ax=axes[3])\n",
    "\n",
    "    configure_time_axes(axes, dcmp['ds'])\n",
    "    \n",
    "    if decomposition_method == \"STL\":\n",
    "        name = \"STL Decomposition\"\n",
    "    elif decomposition_method == \"CD\":\n",
    "        name = \"Classical Decomposition\"\n",
    "    else:\n",
    "        name = \"No Decomposition\"\n",
    "\n",
    "    configure_time_axes(axes, heat_train_df['ds'])\n",
    "\n",
    "    axes[0].set_ylabel('Data')\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[3].set_ylabel('Remainder')\n",
    "\n",
    "    fig.suptitle(f\"Series {uid}: {name}\")\n",
    "    fig.supxlabel('Date')\n",
    "    fig.tight_layout(rect=[0.01, 0.01, 0.99, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb5436",
   "metadata": {},
   "source": [
    "We observe the seasonal component capturing a modest dip at the end of December, which might be reflecting the impact of winter holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903038b",
   "metadata": {},
   "source": [
    "### Remove annual seasonality from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60afc18",
   "metadata": {},
   "source": [
    "Next, we subtract the annual seasonal component from the hourly data, yielding a deseasonalized series that we’ll use to extract daily and weekly patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1404ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "# ---  Remove the annual component from the training data ---\n",
    "# Build a single DataFrame of the daily seasonal component\n",
    "season_df = (\n",
    "    pd.concat(\n",
    "        [df[['ds','seasonal']].assign(unique_id=uid)\n",
    "         for uid, df in results_STL.items()],\n",
    "        ignore_index=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a new column with the deseasonalized values\n",
    "heat_train_df = remove_annual_component(\n",
    "    df=heat_train_df,\n",
    "    season_df=season_df,\n",
    "    target_col='y_transformed',\n",
    "    target_col_deseason='y_deseason_transformed'\n",
    ")\n",
    "\n",
    "# --- Plot the the deseasonalized data together with its daily average ---\n",
    "fig, axes = plt.subplots(n_uids, 1, figsize=(15, 3*n_uids), sharex=True)\n",
    "if n_uids == 1:\n",
    "    axes = [axes]\n",
    "for (id, group), ax in zip(heat_train_df.groupby('unique_id'), axes):\n",
    "    sns.lineplot(data=group, x='ds', y='y_deseason_transformed', ax=ax, color='blue', alpha=0.5, label='Hourly')\n",
    "    group_daily = (\n",
    "        group\n",
    "        .set_index(['ds'])\n",
    "        .resample('D', level='ds')\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "    sns.lineplot(data=group_daily, x='ds', y='y_deseason_transformed', ax=ax, color='black', label='Daily aggregate')\n",
    "    ax.set_title(f'Deseasonalized Series: {id}')\n",
    "\n",
    "configure_time_axes(axes, heat_train_df['ds'], global_legend=True, legend_fig=fig)\n",
    "    \n",
    "fig.suptitle('Per-ID Hourly & Daily Series with Annual Seasonality Removed')\n",
    "fig.supxlabel('Date Time')\n",
    "fig.tight_layout(rect=[0.01, 0.01, 0.99, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8142111",
   "metadata": {},
   "source": [
    "We observe that during the transitions between summer and the extended-winter period (and vice versa), the deseasonalized series often shows marked deviations from its typical values. This happens because the transition between these two regimes in the original series does not always occur at exactly the same time each year. Unfortunately, addressing this issue is difficult without external variables, such as climatic exogenous inputs, that could help anticipate the regime change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db80bb",
   "metadata": {},
   "source": [
    "### Perform MSTL decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45624aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set params ---\n",
    "train_horizon = 8*7*24          # (in hours) limit the data for MSTL decomposition (and subsequent forecasting) to the last `train_horizon` hours of the train set\n",
    "periods_mstl = [24, 24 * 7]     # Periods for MSTL decomposition: 24 hours and 7 days (the two seasonalities)\n",
    "windows_mstl = [11, 15]          # Choose the windows for the MSTL decomposition. Otherwise, the default is [11, 15] \n",
    "stl_kwargs_mstl = {             \n",
    "    'robust': True              # Use robust MSTL decomposition\n",
    "} \n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Initialize training data and MSTL model ---\n",
    "heat_train_MSTL_df = (\n",
    "    heat_train_df\n",
    "    .groupby('unique_id', group_keys=False)\n",
    "    .tail(train_horizon)\n",
    ")\n",
    "MSTL_model = MSTL( \n",
    "    endog=heat_train_MSTL_df['y_deseason_transformed'],\n",
    "    periods=periods_mstl,\n",
    "    windows=windows_mstl,\n",
    "    lmbda=None, # No Box-Cox transformation\n",
    "    stl_kwargs=stl_kwargs_mstl\n",
    ")\n",
    "\n",
    "# --- Perform MSTL decomposition of each series separately ---\n",
    "results_MSTL = {}\n",
    "for uid, grp in heat_train_MSTL_df.groupby('unique_id'):\n",
    "    # Sort & reset index\n",
    "    grp = grp.sort_values('ds').reset_index(drop=True)\n",
    "    \n",
    "    # Fit MSTL\n",
    "    res = MSTL( \n",
    "        endog=grp['y_deseason_transformed'],\n",
    "        periods=periods_mstl,\n",
    "        windows=windows_mstl,\n",
    "        lmbda=None, # No Box-Cox transformation\n",
    "        stl_kwargs=stl_kwargs_mstl\n",
    "    ).fit()\n",
    "\n",
    "    # Collect into a DataFrame\n",
    "    dcmp = pd.DataFrame({\n",
    "        'ds':       grp['ds'],\n",
    "        'data':     grp['y_transformed'],\n",
    "        'trend':    res.trend,\n",
    "        'seasonal_24h': res.seasonal.iloc[:, 0],  # 24h seasonal component\n",
    "        'seasonal_7d': res.seasonal.iloc[:, 1],   # 7d seasonal component\n",
    "        'remainder': res.resid\n",
    "    })\n",
    "    results_MSTL[uid] = dcmp\n",
    "\n",
    "# --- Plot the decomposition ---\n",
    "for uid, dcmp in results_MSTL.items():\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(12, 10))\n",
    "    color = 'black'\n",
    "    sns.lineplot(data=dcmp, x='ds', y='data', ax=axes[0], color=color)\n",
    "    sns.lineplot(data=dcmp, x='ds', y='trend', ax=axes[1], color=color)\n",
    "    sns.lineplot(data=dcmp, x='ds', y='seasonal_24h', ax=axes[2], color=color)\n",
    "    sns.lineplot(data=dcmp, x='ds', y='seasonal_7d', ax=axes[3], color=color)\n",
    "    sns.lineplot(data=dcmp, x='ds', y='remainder',ax=axes[4], color=color)\n",
    "\n",
    "    configure_time_axes(axes, heat_train_MSTL_df['ds'])\n",
    "\n",
    "    axes[0].set_ylabel('Data')\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[2].set_ylabel('Seasonal 24h')\n",
    "    axes[3].set_ylabel('Seasonal 7d')\n",
    "    axes[4].set_ylabel('Remainder')\n",
    "\n",
    "    fig.suptitle(f\"Series {uid}: MSTL Decomposition for the Annual-Deseasonalized Series\")\n",
    "    fig.supxlabel('Date')\n",
    "    fig.tight_layout(rect=[0.01, 0.01, 0.99, 0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db902092",
   "metadata": {},
   "source": [
    "### Forecast trend+remainder with AutoETS, then add the seasonal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b882f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set params ---\n",
    "trend_forecaster='AutoETS'                                          # choices: 'AutoETS' or 'Naive' for the trend forecaster\n",
    "trend_forecaster_kwargs={\n",
    "    'model': 'ZZN',                                                 # model specification\n",
    "#   'prediction_intervals': ConformalIntervals(n_windows=4, h=h)    # type of prediction intervals to use\n",
    "} \n",
    "level=[]                                                             # prediction intervals to compute\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Forecast trend + remainder ---\n",
    "logging.info(\"Forecasting with horizon: %d hours\", h)\n",
    "\n",
    "# Prepare the DataFrame with the trend + remainder\n",
    "rows = []\n",
    "for uid, dcmp in results_MSTL.items():\n",
    "    y = dcmp['trend'] + dcmp['remainder']\n",
    "    rows.append(pd.DataFrame({\n",
    "        'unique_id': uid,\n",
    "        'ds': dcmp['ds'],\n",
    "        'y_trend_rem': y\n",
    "    }))\n",
    "y_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# Forecast trend + remainder using the specified forecaster\n",
    "if trend_forecaster == 'Naive':\n",
    "    trend_model = Naive(alias='MSTL')\n",
    "    sf_Naive = StatsForecast(\n",
    "        models=[trend_model],\n",
    "        freq='h',\n",
    "    )\n",
    "    trend_forecast_df = sf_Naive.forecast(h=h, df=y_df, target_col='y_trend_rem', level=level)\n",
    "else:\n",
    "    model_spec = trend_forecaster_kwargs.get('model', 'ZZN')\n",
    "    pi_cust = trend_forecaster_kwargs.get('prediction_intervals',\n",
    "                            ConformalIntervals(n_windows=4, h=h))\n",
    "    trend_model = AutoETS(model=model_spec, prediction_intervals=pi_cust, alias='MSTL')\n",
    "    sf_AutoETS = StatsForecast(\n",
    "        models=[trend_model],\n",
    "        freq='h',  # Frequency of the data\n",
    "    )\n",
    "    trend_forecast_df = sf_AutoETS.forecast(h=h, df=y_df, target_col='y_trend_rem', level=level) \n",
    "\n",
    "# --- Forecast seasonal components ---\n",
    "# Build a DataFrame with the daily and weekly seasonal components\n",
    "seas_df = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'unique_id': uid,\n",
    "        'ds':         dcmp['ds'],\n",
    "        'seasonal_24h': dcmp['seasonal_24h'],\n",
    "        'seasonal_7d':  dcmp['seasonal_7d'],\n",
    "    })\n",
    "    for uid, dcmp in results_MSTL.items()\n",
    "], ignore_index=True)\n",
    "\n",
    "# Forecast the seasonal components with a naive model\n",
    "naive_model24h = SeasonalNaive(season_length=24, alias='Naive24h')\n",
    "sf_naive24h = StatsForecast(\n",
    "    models=[naive_model24h], \n",
    "    freq='h'\n",
    ")\n",
    "naive24h_forecast_df = sf_naive24h.forecast(h, seas_df, target_col='seasonal_24h')\n",
    "naive_model7d = SeasonalNaive(season_length=24*7, alias='Naive7d')\n",
    "sf_naive7d = StatsForecast(\n",
    "    models=[naive_model7d], \n",
    "    freq='h'\n",
    ")\n",
    "naive7d_forecast_df = sf_naive7d.forecast(h, seas_df, target_col='seasonal_7d')\n",
    "\n",
    "# --- Add daily and weekly seasonalities to the trend+remainder forecast ---\n",
    "# Merge the seasonal components into the trend + remainder forecast\n",
    "trend_forecast_df = trend_forecast_df.merge(\n",
    "    naive24h_forecast_df[['unique_id', 'ds', 'Naive24h']],\n",
    "    on=['unique_id', 'ds'],\n",
    "    how='left'\n",
    ")\n",
    "trend_forecast_df = trend_forecast_df.merge(\n",
    "    naive7d_forecast_df[['unique_id', 'ds', 'Naive7d']],\n",
    "    on=['unique_id', 'ds'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select the columns to adjust\n",
    "to_adjust = (\n",
    "    trend_forecast_df\n",
    "    .select_dtypes(include='number')  # all numeric columns\n",
    "    .columns\n",
    "    .difference(['unique_id', 'ds', 'Naive24h', 'Naive7d'])  # drop our merge‐keys / seasonal\n",
    ")\n",
    "\n",
    "# Adjust those columns by adding the seasonal components\n",
    "trend_forecast_df[to_adjust] = (\n",
    "    trend_forecast_df[to_adjust]\n",
    "    .add(trend_forecast_df['Naive24h'], axis=0)  # add the 24h seasonal component\n",
    "    .add(trend_forecast_df['Naive7d'], axis=0)   # add the 7d seasonal component\n",
    ")\n",
    "\n",
    "# Now drop the helper columns\n",
    "trend_forecast_df.drop(columns=['Naive24h', 'Naive7d'], inplace=True)\n",
    "\n",
    "# --- Add annual seasonal component back ---\n",
    "# Build a DataFrame with the annual seasonal component\n",
    "season_df = (\n",
    "    pd.concat(\n",
    "        [df[['ds','seasonal']].assign(unique_id=uid)\n",
    "            for uid, df in results_STL.items()],\n",
    "        ignore_index=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add to the forecast\n",
    "forecast_df = add_annual_component(\n",
    "    forecast_df=trend_forecast_df,\n",
    "    season_df=season_df,\n",
    ")\n",
    "\n",
    "# --- Anti-transform the forecasts ---\n",
    "to_transform = forecast_df.select_dtypes(include='number').columns\n",
    "\n",
    "for c in to_transform:\n",
    "    # Apply the transformation function to the forecasted values\n",
    "    bwd = make_transformer(transform, c, lambdas, inv=True)\n",
    "    forecast_df[c] = transform_column(forecast_df, bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c088b",
   "metadata": {},
   "source": [
    "Plot the forecasts alongside predictions from a naive 24-hour baseline model that simply repeats the most recent 24-hour cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5761bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_plot_results(\n",
    "    target_df=heat_df,\n",
    "    forecast_df=forecast_df,\n",
    "    start_offset=14 * 24,\n",
    "    end_offset=3 * 24,\n",
    "    with_naive=True,\n",
    "    target_train_df=heat_train_df,\n",
    "    order_of_models=['Naive24h', 'MSTL'],  # Order of models in the plot\n",
    "    alpha=0.8,  # Transparency of the lines of the forecasts\n",
    "    ids=facilities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b72bd9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78024eff",
   "metadata": {},
   "source": [
    "The `custom_evaluate` function computes evaluation metrics for the provided forecasts. If `with_naive=True`, it also generates naïve baseline forecasts on the fly for comparison. Additionally, if `with_pct_increase=True`, it calculates the percentage increase over the naïve baseline for each metric, appending a new column named `*_pct_inc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35555391",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = custom_evaluate(\n",
    "    forecast_df=forecast_df,\n",
    "    target_df=heat_df,\n",
    "    metrics=['mae', 'rmse', 'mase', 'nmae', 'me'],\n",
    "    with_naive=True,\n",
    "    with_pct_increase=True\n",
    ")\n",
    "display_metrics(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dce88a",
   "metadata": {},
   "source": [
    "## Use of the custom MSTL Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463927ab",
   "metadata": {},
   "source": [
    "I've encapsulated that entire workflow in a custom class. Here’s how to invoke it to perform the same decomposition, modeling, and forecasting steps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create the MSTL configuration ---\n",
    "# Include all parameters used for data transformation and fitting the MSTL model\n",
    "config = MSTLConfig(\n",
    "    transform=transform,\n",
    "    lam_method=lam_method,\n",
    "    winter_focus=winter_focus,\n",
    "    season_length=season_length,\n",
    "    decomposition_method=decomposition_method,\n",
    "    smooth_loess=smooth_loess,\n",
    "    window_loess=window_loess,\n",
    "    robust_stl=robust_stl,\n",
    "    train_horizon=train_horizon,\n",
    "    windows_mstl=windows_mstl,\n",
    "    stl_kwargs_mstl=stl_kwargs_mstl,\n",
    "    periods_mstl=periods_mstl\n",
    ")\n",
    "\n",
    "# --- Instantiate the MSTL pipeline ---\n",
    "mstl_pipeline = MSTLPipeline(\n",
    "    target_df=heat_df,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# --- Fit the MSTL pipeline ---\n",
    "mstl_pipeline.fit(\n",
    "    start_train=start_train,\n",
    "    end_train=end_train,\n",
    ")\n",
    "\n",
    "# --- Forecast with the MSTL pipeline ---\n",
    "if trend_forecaster != 'AutoETS':\n",
    "    logging.warning(\"The MSTL pipeline currently only supports AutoETS as trend forecaster. Overriding trend_forecaster to 'AutoETS'.\")\n",
    "mstl_forecast_df = mstl_pipeline.predict(\n",
    "    h=h, \n",
    "    level=level, \n",
    "    trend_forecaster_kwargs=trend_forecaster_kwargs, \n",
    "        # The trend forecaster is AutoARIMA, the \"naive\" option in not implemented\n",
    ")\n",
    "\n",
    "# --- Plot the MSTL pipeline results ---\n",
    "custom_plot_results(\n",
    "    target_df=heat_df,\n",
    "    forecast_df=mstl_forecast_df,\n",
    "    start_offset=14 * 24,\n",
    "    end_offset=3 * 24,\n",
    "    with_naive=True,\n",
    "    target_train_df=heat_train_df,\n",
    "    order_of_models=['Naive24h', 'MSTL'],  # Order of models in the plot\n",
    "    alpha=0.8,  # Transparency of the lines of the forecasts\n",
    "    ids=facilities,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# --- Evaluate the MSTL pipeline forecasts ---\n",
    "evaluation_df = custom_evaluate(\n",
    "    forecast_df=mstl_forecast_df,\n",
    "    target_df=heat_df,\n",
    "    metrics=['mae', 'rmse', 'mase', 'nmae', 'me'],\n",
    "    with_naive=True,\n",
    "    with_pct_increase=True\n",
    ")\n",
    "display_metrics(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43279e4f",
   "metadata": {},
   "source": [
    "Moreover, we can visualize the complete MSTL decomposition generated during model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mstl_pipeline.plot_decomposition('F3') # Plot the decomposition for a specific facility, e.g. 'F3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e841beb",
   "metadata": {},
   "source": [
    "After a normal `.fit()`/`.predict()` cycle, you might need a forecast **from a later point in time** than the timestamp immediately after the end of the training data, **without refitting** the whole model. That’s exactly what `.forward` is for. It reuses everything learned at fit time (transforms, decomposition, and the ETS trend/remainder forecaster) and simply **re-anchors the seasonal components** to your new `context_end`. In other words, we change the *window we look back on*, not the model’s parameters.\n",
    "\n",
    "Under the hood, `.forward`:\n",
    "\n",
    "* Extends the stored daily/weekly seasonal components up to `context_end` by phase-aligned repetition.\n",
    "* Rebuilds the **context target** $y_{\\text{trend+rem}}$ from raw data by subtracting those re-anchored seasonals.\n",
    "* Calls the cached ETS models’ **`.forward(...)`** on that context to produce the next `h` steps.\n",
    "* Adds back the seasonal pieces and applies the inverse transform to return results on the original scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_end = end_train + pd.Timedelta(hours=24*7) # e.g. one week later\n",
    "\n",
    "mstl_forecast_df = mstl_pipeline.forward(\n",
    "    context_end=context_end,\n",
    "    h=h, \n",
    "    level=level, \n",
    "    trend_forecaster_kwargs=trend_forecaster_kwargs, \n",
    ")\n",
    "\n",
    "custom_plot_results(\n",
    "    target_df=heat_df,\n",
    "    forecast_df=mstl_forecast_df,\n",
    "    start_offset=14 * 24,\n",
    "    end_offset=3 * 24,\n",
    "    with_naive=True,\n",
    "    target_train_df=heat_train_df,\n",
    "    order_of_models=['Naive24h', 'MSTL'],  # Order of models in the plot\n",
    "    alpha=0.8,  # Transparency of the lines of the forecasts\n",
    "    ids=['F3'],#facilities\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6eaad",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73e3d4",
   "metadata": {},
   "source": [
    "The `MSTLPipeline` class makes time-series cross-validation effortless via its `cross_validation` method, which mirrors the identically named routine in StatsForecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080664a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validation = True  # Set to False to skip cross-validation\n",
    "\n",
    "# --- Set params ---\n",
    "# Unique id(s) to use for cross-validation\n",
    "facilities = ['F3']  # Specify a single ID to make cross-validation quicker\n",
    "h = 24 * 7           # Forecast horizon (in hours)\n",
    "\n",
    "# Params to set the cross-validation windows\n",
    "max_n_fits = 5                                # Max number of fits to perform (to limit computation time)\n",
    "step_size = 24 * 7                            # Step size between windows (in hours)\n",
    "start_test_cv = pd.to_datetime('2023-12-10')  # Start of the first forecast horizon (before end_train+1h because we want many windows)\n",
    "end_test_cv = pd.to_datetime('2024-03-10')    # (Upper bound for) End of the last forecast horizon\n",
    "\n",
    "# === Do not edit below ===\n",
    "if run_cross_validation:\n",
    "    # Fitting params\n",
    "    config = MSTLConfig()                         # MSTLConfig object with all the parameters used for MSTL fitting\n",
    "    \n",
    "    levels = []                                   # Prediction‐interval levels to compute, e.g. [80, 90]\n",
    "    input_size = None                             # Size of each training window (in hours). If None, uses all data up to the cutoff.\n",
    "\n",
    "    # --- Set CV windows ---\n",
    "    # Get the cross-validation parameters to pass to cross_validation to use n_windows in the specified test_cv\n",
    "    out = get_cv_params_v2(\n",
    "        start_test_cv=start_test_cv,  \n",
    "        end_test_cv=end_test_cv,                        \n",
    "        step_size=step_size,\n",
    "        max_n_fits=max_n_fits,               \n",
    "        horizon_hours=h,\n",
    "    )\n",
    "    step_size = out['step_size']\n",
    "    test_hours = out['test_hours']\n",
    "    end_test_cv = out['end_test_actual']\n",
    "    n_windows = out['n_windows']\n",
    "    refit = out['refit']\n",
    "\n",
    "    # --- Perform cross-validation with the naive model ---\n",
    "    heat_facilities_df = heat_df[heat_df['unique_id'].isin(facilities)].copy()\n",
    "    naive_model = SeasonalNaive(season_length=24, alias='Naive24h')\n",
    "    sf = StatsForecast(models=[naive_model], freq='h')\n",
    "    full_df = heat_facilities_df[heat_facilities_df['ds'] <= end_test_cv]  # Use all data up to the end of the last forecast horizon\n",
    "    cv_naive = sf.cross_validation(\n",
    "        h=h,\n",
    "        df=full_df,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,    \n",
    "        test_size=test_hours,   # Total size of the hold-out block for testing (from start_test_cv to end_test_cv)\n",
    "        input_size=None,        # Use all data up to the cutoff\n",
    "    )\n",
    "\n",
    "    # --- Perform cross-validation with the MSTL model ---\n",
    "    mstl_pipeline = MSTLPipeline(\n",
    "        target_df=heat_facilities_df,\n",
    "        config=config,\n",
    "    )\n",
    "    logging.info(\"Starting cross-validation...\")\n",
    "    cv_mstl = mstl_pipeline.cross_validation(\n",
    "        h=h,\n",
    "        test_size=test_hours,  \n",
    "        end_test=end_test_cv,  \n",
    "        step_size=step_size,  \n",
    "        input_size=None,        # Size of each training window (in hours). If None, uses all data up to the cutoff.\n",
    "        refit=refit,             # Whether to refit the model for each window\n",
    "        verbose=True,           # Print progress\n",
    "        alias='MSTL',           # Alias for the model in the results\n",
    "    )\n",
    "    logging.info(\"✓ Cross-validation completed.\")\n",
    "\n",
    "    # --- Merge results ---\n",
    "    cv_df = cv_naive.merge(\n",
    "        cv_mstl.drop(columns=['y']),\n",
    "        on=['unique_id', 'ds', 'cutoff'],\n",
    "        how='left',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccffed",
   "metadata": {},
   "source": [
    "The `heat_forecast` package includes dedicated functions for visualizing and evaluating cross‑validation results.\n",
    "\n",
    "Alongside reporting the usual cross-validation means and standard deviations of each metric across rolling forecast windows, I also include loss-difference statistics, defined for each cutoff $t$ and metric $m$ as:\n",
    "\n",
    "$$\n",
    "\\text{LD-Candidate}_m(t) = \\text{Candidate}_m(t) - \\text{Naive}_{24h,m}(t),\n",
    "$$\n",
    "\n",
    "where $\\text{Model}_m(t)$ denotes the value of metric $m$ (e.g., MAE, RMSE) computed over the forecast window $[t+1, t+h]$. In fact, we can replace $\\text{Naive}_{24h,m}$ with any reference model we want.\n",
    "\n",
    "Loss differences are particularly useful for comparing models because their variation reflects how consistently a candidate outperforms (or underperforms) the baseline, rather than just the absolute difficulty of the forecasting task. That is, the standard deviation of the LDs captures fluctuations in the relative performance gap between models across different forecast windows.\n",
    "\n",
    "In contrast, examining the standard deviations of the raw metric values for each model (e.g., MAE or RMSE across rolling windows) is less informative for model comparison. Those standard deviations can be inflated by time-varying forecast difficulty. \n",
    "\n",
    "However, it's important to note that both LD standard deviations and raw metric standard deviations can be deflated by serial correlation across windows. This happens when model errors persist across time, making adjacent evaluations not fully independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, combined_results = custom_evaluate_cv(\n",
    "    cv_df=cv_df,\n",
    "    metrics=['mae', 'rmse', 'mase', 'nmae'],  # List of metrics to compute\n",
    "    target_df=heat_df,  # Training data mase\n",
    ")\n",
    "_ = display_cv_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5557788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and summarise loss-difference (LD) statistics versus the baseline model.\n",
    "summ_ld, combined_ld = compute_loss_diff_stats(combined_results)\n",
    "_ = display_cv_summary(summ_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_cv(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375df368",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cv_metric_by_cutoff(\n",
    "    combined_results=combined_results,\n",
    "    metric='mae',  # Change to 'rmse' or 'me' for other metrics\n",
    "    figsize=(10,6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = sorted(cv_df['cutoff'].unique())\n",
    "fig = plot_cutoff_results(\n",
    "    target_df=heat_df,\n",
    "    cv_df=cv_df,\n",
    "    cutoffs=cutoffs[:3],  # Plot first 3 cutoffs\n",
    "    start_offset=24 * 3,\n",
    "    end_offset=24 * 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a06f05",
   "metadata": {},
   "source": [
    "After cross‑validation, MSTL retains the metadata from the most recent run, which you can retrieve like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = mstl_pipeline.last_cv_metadata\n",
    "print(\n",
    "    yaml.safe_dump(\n",
    "        metadata,\n",
    "        indent=4,              \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fedc96",
   "metadata": {},
   "source": [
    "## MSTL fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fcb13f",
   "metadata": {},
   "source": [
    "In this section, we fine-tune the model's hyperparameters using a rolling-origin cross-validation procedure. The objective is to evaluate different parameter configurations across multiple forecast windows to identify those that yield the best performance. Fine-tuning is conducted independently for each time series (labeled F1 through F5) and for each forecast horizon (daily and weekly). In other words, the optimal hyperparameters are selected separately for each combination of series and forecast horizon.\n",
    "\n",
    "Each configuration is evaluated over the historical period from October 2023, to the end of March 2024, a timeframe that covers most of the \"cold regime\" across all series for that year. For each forecast horizon, we define a sequence of evenly spaced rolling evaluation windows within this period:\n",
    "\n",
    "- 30 windows for the daily forecast horizon;\n",
    "\n",
    "- 20 windows for the weekly forecast horizon.\n",
    "\n",
    "In each cross-validation fold, the model is trained on all data preceding the evaluation window (the training set), and evaluated on the window itself (the test set), using the error metrics described earlier. This process is repeated across all rolling windows and all hyperparameter combinations.\n",
    "\n",
    "Finally, we compute the mean and standard deviation of the error metrics across all folds. These summary statistics are used to compare configurations and select the most robust model for each time series and forecast horizon.\n",
    "\n",
    "As previously noted, the following settings were held fixed during the grid search:\n",
    "\n",
    "* Annual seasonality decomposition was configured using `decomposition_method='CD'`, without final LOESS smoothing.\n",
    "* Global MSTL settings (`stl_kwargs` in the `statsmodels` implementation) were kept at their default values, except for `robust=True`, which was enabled to improve resistance to outliers across all seasonal components.\n",
    "\n",
    "Fine-tuning instead focused on the following hyperparameters:\n",
    "\n",
    "* **`transform`**: the data transformation method, either a log transform or a Box-Cox transform with winter-focused training and log-likelihood-based lambda estimation.\n",
    "* **`train_horizon`**: the number of data points retained for MSTL decomposition and subsequent forecasting. (Note: annual decomposition always uses the full training set).\n",
    "* **`windows_mstl`**: the LOESS smoothing windows applied during MSTL for each seasonal component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune = False  # Set to False to skip fine-tuning\n",
    "\n",
    "horizon_type = 'week' # 'day' for one day horizon, 'week' for one week horizon\n",
    "id = 'F5'             # Specify a single ID for fine tuning\n",
    "\n",
    "# === Do not edit below ===\n",
    "if finetune:\n",
    "    metadata = {}\n",
    "    if horizon_type not in ['day', 'week']: \n",
    "        raise ValueError(\"Unsupported horizon type. Use 'day' or 'week'.\")\n",
    "    h = 24 * 7 if horizon_type == 'week' else 24\n",
    "    heat_facilities_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "\n",
    "    # --- Create directory for fine-tuning results ---\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    run_id = f\"{id}_{horizon_type}_finetuning_mstl_{timestamp}\"\n",
    "    path = BASE_DIR / \"results\" / \"finetuning\" / \"mstl\" / run_id\n",
    "    metadata['run_id'] = run_id\n",
    "\n",
    "    try:\n",
    "        path.mkdir(parents=True, exist_ok=False)\n",
    "        logging.info(f\"Created directory for fine-tuning results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "\n",
    "        # --- Set params grid for fine tuning ---\n",
    "        param_grid_1 = {\n",
    "            'transform':              ['log'],\n",
    "            'lam_method':             [None], \n",
    "            'winter_focus':           [None], \n",
    "            'train_horizon':          [7*7*24, 11*7*24, 15*7*24, 19*7*24, 23*7*24],\n",
    "            'windows_mstl':           [[9, 11], [11, 15], [13, 17], [15, 19], [17, 21], [19, 23]], \n",
    "            'stl_kwargs_mstl':        [{'robust': True}],\n",
    "        }\n",
    "        param_grid_2 = {\n",
    "            'transform':              ['boxcox'],\n",
    "            'lam_method':             ['loglik'], \n",
    "            'winter_focus':           [True],\n",
    "            'train_horizon':          [7*7*24, 11*7*24, 15*7*24, 19*7*24, 23*7*24],\n",
    "            'windows_mstl':           [[9, 11], [11, 15], [13, 17], [15, 19], [17, 21], [19, 23]], \n",
    "            'stl_kwargs_mstl':        [{'robust': True}],\n",
    "        }\n",
    "        grid = list(ParameterGrid(param_grid_1))+list(ParameterGrid(param_grid_2))\n",
    "        logging.info(f\"Total parameter combinations: {len(grid)}\")\n",
    "        metadata['param_grid_1'] = param_grid_1\n",
    "        metadata['param_grid_2'] = param_grid_2\n",
    "\n",
    "        # --- Set windows with get_cv_params ---\n",
    "        n_windows = 30 if horizon_type == 'day' else 20  # Number of windows for cross-validation\n",
    "        start_test_cv = pd.to_datetime('2023-11-10')\n",
    "        end_test_cv = pd.to_datetime('2024-04-10')  \n",
    "        out = get_cv_params_v2(\n",
    "            start_test_cv=pd.to_datetime('2023-11-10'),  \n",
    "            end_test_cv=pd.to_datetime('2024-04-10'),    \n",
    "            n_windows=n_windows,  \n",
    "            horizon_hours=h,  \n",
    "        )\n",
    "        n_windows = out['n_windows'] \n",
    "        step_size = out['step_size']\n",
    "        test_hours = out['test_hours']\n",
    "        end_test_cv = out['end_test_actual']\n",
    "\n",
    "        metadata['for_get_cv_params'] = {\n",
    "            'n_windows': n_windows,\n",
    "            'start_test_cv': str(start_test_cv),\n",
    "            'end_test_cv': str(end_test_cv),\n",
    "        }\n",
    "\n",
    "        # --- Run CV once with the naive model for comparison ---\n",
    "        params_placeholder = {key: None for key in param_grid_1.keys()}\n",
    "        naive_model = SeasonalNaive(season_length=24, alias='Naive24h')\n",
    "        sf = StatsForecast(models=[naive_model], freq='h')\n",
    "        full_df = heat_facilities_df[heat_facilities_df['ds'] <= end_test_cv]  # Use all data up to the end of the last forecast horizon\n",
    "        t0 = pd.Timestamp.now()\n",
    "        cv_naive = sf.cross_validation(\n",
    "            h=h,\n",
    "            df=full_df,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,  \n",
    "            test_size=test_hours,  \n",
    "            input_size=None,  \n",
    "            refit=True,  \n",
    "        )\n",
    "        t1 = pd.Timestamp.now()\n",
    "        elapsed = (t1 - t0).total_seconds()\n",
    "        record = {\n",
    "            'name': 'Naive24h',\n",
    "            'avg_elapsed_sec': elapsed/n_windows,  \n",
    "            **params_placeholder,\n",
    "        }\n",
    "\n",
    "        # store results and corresponding params / name of the model / elapsed time\n",
    "        records = [record]  # Initialize results with the naive model record\n",
    "        cv_frames = [cv_naive]\n",
    "\n",
    "        # --- Run grid search for MSTL model ---\n",
    "        t_strt_cv = pd.Timestamp.now()\n",
    "        for i, params in enumerate(tqdm(grid, desc=\"Grid search\", leave=True)):\n",
    "            mstlconfig = MSTLConfig(**params)\n",
    "            mstlpipeline = MSTLPipeline(target_df=heat_facilities_df, config=mstlconfig)\n",
    "\n",
    "            t0 = pd.Timestamp.now()\n",
    "            cv_mstl = mstlpipeline.cross_validation(\n",
    "                h=h,\n",
    "                test_size=test_hours,  \n",
    "                end_test=end_test_cv,  \n",
    "                step_size=step_size,  \n",
    "                input_size=None, \n",
    "                verbose=False,\n",
    "                alias=f'MSTL_{i}',  \n",
    "            )\n",
    "            t1 = pd.Timestamp.now()\n",
    "            elapsed = (t1 - t0).total_seconds()\n",
    "            cv_mstl.drop(columns=['y'], inplace=True)\n",
    "            cv_frames.append(cv_mstl)\n",
    "            \n",
    "            record = {\n",
    "                'name': f'MSTL_{i}',\n",
    "                **params,   \n",
    "                'avg_elapsed_sec': elapsed/n_windows,\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "        # --- Combine all cross-validation results into a single DataFrame ---\n",
    "        key_cols = [\"unique_id\", \"ds\", \"cutoff\"]\n",
    "        cv_results_df = reduce(\n",
    "            lambda left, right: pd.merge(left, right, on=key_cols, how=\"outer\"),\n",
    "            cv_frames            \n",
    "        )\n",
    "        records_df = pd.DataFrame(records)\n",
    "        results = (records_df, cv_results_df)\n",
    "\n",
    "        t_end_cv = pd.Timestamp.now()\n",
    "        elapsed_cv = str(t_end_cv - t_strt_cv)\n",
    "        metadata['total_elapsed_time'] = elapsed_cv\n",
    "\n",
    "        # --- Final save ---\n",
    "        records_df.to_parquet(path / \"records.parquet\", compression=\"snappy\")\n",
    "        cv_results_df.to_parquet(path / \"cv_results.parquet\", compression=\"snappy\")\n",
    "\n",
    "        metadata_path = path / 'metadata.yaml'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            yaml.safe_dump(\n",
    "                metadata,\n",
    "                f,\n",
    "                default_flow_style=False,    # block style\n",
    "                sort_keys=False              # preserve insertion order\n",
    "            )\n",
    "\n",
    "        logging.info(\"✓ All artifacts saved successfully, fine-tuning completed.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"✗ Interrupted; cleaning up.\")\n",
    "        remove_tree(path, require_within=BASE_DIR)\n",
    "        logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "        raise\n",
    "    except Exception:\n",
    "        logging.exception(\"✗ Error during test for id=%s, horizon=%s; cleaning up.\", id, horizon_type)\n",
    "        remove_tree(path, require_within=BASE_DIR)\n",
    "        logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "        raise\n",
    "\n",
    "    logging.info(f\"✓ Tuning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: pick a run_id to analyze \n",
    "run_id = 'F5_week_finetuning_mstl_20250719T125709'  \n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Load the metadata for the run_id ---\n",
    "path = BASE_DIR / \"results\" / \"finetuning\" / \"mstl\" / run_id\n",
    "records_df = pd.read_parquet(path / \"records.parquet\")\n",
    "cv_results_df = pd.read_parquet(path / \"cv_results.parquet\")\n",
    "\n",
    "from heat_forecast.utils.evaluation import (\n",
    "    evaluate_cv_forecasts, cv_evaluation_summary,\n",
    "    compute_loss_diffs, display_cv_summary\n",
    ")\n",
    "# --- Evaluate results ---\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"DataFrame is highly fragmented\",\n",
    "        category=pd.errors.PerformanceWarning,\n",
    "    )\n",
    "    all_results = evaluate_cv_forecasts(\n",
    "        cv_df=cv_results_df,\n",
    "        target_df=heat_df, \n",
    "        metrics=['mae', 'rmse', 'smape'],  \n",
    "    )\n",
    "    summary = cv_evaluation_summary(all_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabed685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display with optional sorting ---\n",
    "ws = display_cv_summary(summary, sort_metric='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13442d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display with optional sorting ---\n",
    "all_loss_diffs = compute_loss_diffs(\n",
    "    all_results,\n",
    "    baseline_model='Naive24h'\n",
    ")\n",
    "summary_ld = cv_evaluation_summary(all_loss_diffs)\n",
    "_ = display_cv_summary(summary_ld, are_loss_diffs=True, sort_metric='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d931df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display the records of the first n models (based on the previous sorting) ---\n",
    "n = 50     # how many?  \n",
    "\n",
    "# === Do not edit below ===\n",
    "# Select the first n models based on the sorted summary snd the corresponding records\n",
    "s = wide_summary.reset_index()    \n",
    "models_chosen = s.iloc[:n, s.columns.get_loc(\"model\")].to_numpy().reshape(-1).tolist()\n",
    "model_order = {model: i for i, model in enumerate(models_chosen)}\n",
    "filtered_df = records_df.loc[records_df['name'].isin(models_chosen), :].copy()\n",
    "filtered_df['order'] = filtered_df['name'].map(model_order)\n",
    "ordered_df = filtered_df.sort_values('order').drop('order', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Display the ordered DataFrame with all rows and columns\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(ordered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bb7be",
   "metadata": {},
   "source": [
    "It can be useful to explore in more detail the results of the best models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e177e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 3  # how many?\n",
    "\n",
    "# === Do not edit below ===\n",
    "def best_mstl_models_plus_naive(ordered_df, n=3):\n",
    "    \"\"\"Get the best n mstl models based on the ordered DataFrame, plus the Naive24h model.\"\"\"\n",
    "    best_mstl_models = [m for m in ordered_df.loc[:n+1, 'name'] if m.startswith('MSTL_')][:n]\n",
    "    return best_mstl_models + ['Naive24h']  \n",
    "\n",
    "models = best_mstl_models_plus_naive(ordered_df, n=n_models)\n",
    "\n",
    "fig = plot_cv_metric_by_cutoff(\n",
    "    combined_results=combined_results,\n",
    "    metric='mae',  # Change to 'rmse' or 'me' for other metrics\n",
    "    models=models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489b271",
   "metadata": {},
   "source": [
    "#### Selected parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd0606",
   "metadata": {},
   "source": [
    "**Best parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84237661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "mstl_optimal_params = {\n",
    "    'F1': {\n",
    "        'day': { # MSTL_59 from run_id = F1_day_finetuning_mstl_20250719T144130\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'boxcox',\n",
    "            'lam_method': 'loglik',\n",
    "            'winter_focus': True,\n",
    "            'train_horizon': 23*7*24,\n",
    "            'windows_mstl': [19, 23],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "        'week': { # MSTL_59 from run_id = F1_week_finetuning_mstl_20250719T143432\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'boxcox',\n",
    "            'lam_method': 'loglik',\n",
    "            'winter_focus': True,\n",
    "            'train_horizon': 23*7*24,\n",
    "            'windows_mstl': [19, 23],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "    },\n",
    "    'F2': {\n",
    "        'day': { # MSTL_44 from run_id = F2_day_finetuning_mstl_20250716T113343\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'boxcox',\n",
    "            'lam_method': 'loglik',\n",
    "            'winter_focus': True,\n",
    "            'train_horizon': 15*7*24,\n",
    "            'windows_mstl': [17, 21],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "        'week': { # MSTL_38 from run_id = F2_week_finetuning_mstl_20250716T143353\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'boxcox',\t\n",
    "            'lam_method': 'loglik',\t\n",
    "            'winter_focus': True,\t\n",
    "            'train_horizon': 11*7*24,\t\n",
    "            'windows_mstl': [13, 17],\t\n",
    "            'stl_kwargs_mstl': {'robust': True},\t\n",
    "        }\n",
    "    },\n",
    "    'F3': {\n",
    "        'day': { # MSTL_29 from run_id = F3_day_finetuning_mstl_20250717T084909\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'log',\n",
    "            'lam_method': None,\n",
    "            'winter_focus': None,\n",
    "            'train_horizon': 23*7*24,\n",
    "            'windows_mstl': [19, 23],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "        'week': { # MSTL_29 from run_id = F3_week_finetuning_mstl_20250717T001618\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'log',\n",
    "            'lam_method': None,\n",
    "            'winter_focus': None,\n",
    "            'train_horizon': 23*7*24,\n",
    "            'windows_mstl': [19, 23],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        }\n",
    "    },\n",
    "    'F4': {\n",
    "        'day': { # MSTL_23 from run_id = F4_day_finetuning_mstl_20250717T120310\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'log',\n",
    "            'lam_method': None,\n",
    "            'winter_focus': None,\n",
    "            'train_horizon': 19*7*24,\n",
    "            'windows_mstl': [19, 23],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "        'week': { # MSTL_47 from run_id = F4_week_finetuning_mstl_20250719T105932\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'boxcox',\n",
    "            'lam_method': 'loglik',\n",
    "            'winter_focus': True,\n",
    "            'train_horizon': 15*7*24,\n",
    "            'windows_mstl': [19, 23],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "    },\n",
    "    'F5': {\n",
    "        'day': { # MSTL_38 from run_id = F5_day_finetuning_mstl_20250718T002503\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'boxcox',\n",
    "            'lam_method': 'loglik',\n",
    "            'winter_focus': True,\n",
    "            'train_horizon': 11*7*24,\n",
    "            'windows_mstl': [13, 17],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "        'week': { # MSTL_26 from run_id = F5_week_finetuning_mstl_20250719T125709\n",
    "            'decomposition_method': 'CD',\n",
    "            'transform': 'log',\n",
    "            'lam_method': None,\n",
    "            'winter_focus': None,\n",
    "            'train_horizon': 23*7*24,\n",
    "            'windows_mstl': [13, 17],\n",
    "            'stl_kwargs_mstl': {'robust': True},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f7d8b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = False  # Set to False to skip test\n",
    "\n",
    "# === Do not edit below ===\n",
    "if do_test:\n",
    "    grid = list(product(['F1', 'F2', 'F3', 'F4', 'F5'], ['week', 'day']))\n",
    "    for id, horizon_type in tqdm(grid, desc=\"Test\", leave=True):\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        # --- Create directory for test results ---\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        run_id = f\"{id}_{horizon_type}_test_mstl_{timestamp}\"\n",
    "        path = BASE_DIR / \"results\" / \"test\" / \"mstl\" / run_id\n",
    "        metadata['run_id'] = run_id\n",
    "\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "            logging.info(f\"Created directory for test results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "\n",
    "            # --- Set params for cv ---\n",
    "            out = get_cv_params_for_test(horizon_type)\n",
    "            metadata['for_cv'] = {\n",
    "                'step_size': out['step_size'],\n",
    "                'test_hours': out['test_hours'],\n",
    "                'end_test_cv': str(out['end_test_actual']),\n",
    "                'n_windows': out['n_windows'],\n",
    "                'refit': out['refit'],\n",
    "                'input_size': None,\n",
    "            }\n",
    "            h = 24*7 if horizon_type == 'week' else 24  \n",
    "\n",
    "            # --- Run cross-validation with the optimal parameters ---\n",
    "            optimal_params = mstl_optimal_params[id][horizon_type]\n",
    "            metadata['optimal_params'] = optimal_params\n",
    "            config = MSTLConfig(**optimal_params)\n",
    "\n",
    "            heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "            aux_id_df = aux_df[aux_df['unique_id'] == id].copy()\n",
    "            pipeline = MSTLPipeline(\n",
    "                target_df=heat_id_df,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            t0 = pd.Timestamp.now()\n",
    "            cv_df = pipeline.cross_validation(\n",
    "                h=h, \n",
    "                test_size=out['test_hours'],      # Test size in hours\n",
    "                end_test=out['end_test_actual'],  # End of the test period\n",
    "                step_size=out['step_size'],       # Step size in hours\n",
    "                input_size=None,                  # Number of hours to consider for each training\n",
    "                refit=out['refit'],               # Whether to refit the model for each window\n",
    "                verbose=True,\n",
    "            )\n",
    "            t1 = pd.Timestamp.now()\n",
    "\n",
    "            avg_elapsed = (t1 - t0).total_seconds() / out['n_fits']\n",
    "            metadata['avg_el_per_fit'] = avg_elapsed\n",
    "\n",
    "            cv_df.to_parquet(path / \"cv_df.parquet\", compression=\"snappy\")\n",
    "\n",
    "            metadata_path = path / 'metadata.yaml'\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                safe_dump_yaml(\n",
    "                    metadata,\n",
    "                    f,\n",
    "                    indent=4, \n",
    "                )\n",
    "\n",
    "            logging.info(f\"✓ Artifacts saved successfully for id={id}, horizon={horizon_type}.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(\"✗ Interrupted; cleaning up.\")\n",
    "            remove_tree(path, require_within=BASE_DIR)\n",
    "            logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "            raise\n",
    "        except Exception:\n",
    "            logging.exception(\"✗ Error during test for id=%s, horizon=%s; cleaning up.\", id, horizon_type)\n",
    "            remove_tree(path, require_within=BASE_DIR)\n",
    "            logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "            raise\n",
    "\n",
    "    logging.info(\"✓ Test completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b900e5",
   "metadata": {},
   "source": [
    "Check and visualize test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55618b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a run_id to analyze\n",
    "run_id = \"F1_day_test_mstl_20251007T194750\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Load data ---\n",
    "path = BASE_DIR / \"results\" / \"test\" / \"mstl\" / run_id\n",
    "cv_df = pd.read_parquet(path / \"cv_df.parquet\")\n",
    "with open(path / \"metadata.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = yaml.safe_load(f) \n",
    "\n",
    "# --- check cv_df ---\n",
    "from heat_forecast.utils.cv_utils import sanity_cv_df\n",
    "_ = sanity_cv_df(cv_df, metadata, positive_forecasts=True)\n",
    "\n",
    "interactive_plot_cutoff_results(\n",
    "    target_df=heat_df,\n",
    "    cv_df=cv_df,\n",
    "    add_context=True,\n",
    "    figsize=(11, 6)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
