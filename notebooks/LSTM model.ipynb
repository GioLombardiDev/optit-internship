{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8094931a",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8499e",
   "metadata": {},
   "source": [
    "In this notebook, we design and tune an Encoder–Decoder LSTM model for 1-day-ahead and 7-day-ahead forecasting. Separate tuning procedures are performed for each of the five time series, identified by the unique IDs F1 through F5.\n",
    "\n",
    "The notebook is organized into the following sections:\n",
    "\n",
    "1. **Preliminaries**:\n",
    "   Data loading, module imports, seeding, and device setup.\n",
    "\n",
    "2. **Pipeline**:\n",
    "   Walkthrough of the LSTM training/evaluation pipeline.\n",
    "\n",
    "3. **Hyperparameter tuning (Optuna)**:\n",
    "   Start a new study or load and inspect an existing one.\n",
    "\n",
    "4. **Tuning results**:\n",
    "   Procedure details and outcome analysis.\n",
    "\n",
    "5. **Testing**:\n",
    "   Generate final predictions on the test period using the tuned model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840b7be",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf159d30",
   "metadata": {},
   "source": [
    "You can run the notebook in two ways:\n",
    "\n",
    "1. **Google Colab**: place the project folder `heat-forecast` in **MyDrive**. The setup cell below will mount Drive and automatically add `MyDrive/heat-forecast/src` to `sys.path` so `import heat_forecast` works out of the box.\n",
    "\n",
    "2. **Local machine**:\n",
    "\n",
    "   * **Installing our package:** from the project root, run `pip install -e .` once (editable install). Then you can open the notebook anywhere and import the package normally.\n",
    "   * **Alternative:** if you’re running the notebook from `.../heat-forecast/notebooks/` without installing the package, the setup cell will detect `../src` and automatically add it to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da38691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Detect if running on Google Colab & Set base dir ---\n",
    "# %cd /home/giovanni.lombardi/heat-forecast/notebooks\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Install required packages only if not already installed\n",
    "def pip_install(pkg: str):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Set base directory and handle environment\n",
    "if in_colab():\n",
    "    # Make sure IPython is modern (avoids the old %autoreload/imp issue if you ever use it)\n",
    "    pip_install(\"ipython>=8.25\")\n",
    "    pip_install(\"ipykernel>=6.29\")\n",
    "    \n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    for pkg in [\"statsmodels\", \"statsforecast\", \"mlforecast\"]:\n",
    "        pip_install(pkg)\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set base directory to your Drive project folder\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/heat-forecast')\n",
    "\n",
    "    # Add `src/` to sys.path for custom package imports\n",
    "    SRC_PATH = BASE_DIR / 'src'\n",
    "    if str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "\n",
    "    # Sanity checks (helpful error messages if path is wrong)\n",
    "    assert SRC_PATH.exists(), f\"Expected '{SRC_PATH}' to exist. Fix BASE_DIR.\"\n",
    "    pkg_dir = SRC_PATH / \"heat_forecast\"\n",
    "    assert pkg_dir.exists(), f\"Expected '{pkg_dir}' package directory.\"\n",
    "    init_file = pkg_dir / \"__init__.py\"\n",
    "    assert init_file.exists(), f\"Missing '{init_file}'. Add it so Python treats this as a package.\"\n",
    "\n",
    "else:\n",
    "    # Local: either rely on editable install (pip install -e .) or add src/ when running from repo\n",
    "    # Assume notebook lives in PROJECT_ROOT/notebooks/\n",
    "    BASE_DIR = Path.cwd().resolve().parent\n",
    "    SRC_PATH = BASE_DIR / \"src\"\n",
    "\n",
    "    added_src = False\n",
    "    if (SRC_PATH / \"heat_forecast\").exists() and str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "        added_src = True\n",
    "\n",
    "# --- Logging setup ---\n",
    "import logging\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR  = (BASE_DIR / \"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / \"run.log\"\n",
    "PREV_LOG = LOG_DIR / \"run.prev.log\"\n",
    "\n",
    "# If there's a previous run.log with content, archive it to run.prev.log\n",
    "if LOG_FILE.exists() and LOG_FILE.stat().st_size > 0:\n",
    "    try:\n",
    "        # Replace old run.prev.log if present\n",
    "        if PREV_LOG.exists():\n",
    "            PREV_LOG.unlink()\n",
    "        LOG_FILE.rename(PREV_LOG)\n",
    "    except Exception as e:\n",
    "        # Fall back to truncating if rename fails (e.g., file locked)\n",
    "        print(f\"[warn] Could not archive previous log: {e}. Truncating current run.log.\")\n",
    "        LOG_FILE.write_text(\"\")\n",
    "\n",
    "# Configure logging: fresh file for this run + echo to notebook/stdout\n",
    "file_handler   = logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\")\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "                        datefmt=\"%m-%d %H:%M:%S\")\n",
    "file_handler.setFormatter(fmt)\n",
    "stream_handler.setFormatter(fmt)\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.handlers[:] = [file_handler, stream_handler]  # replace handlers (important in notebooks)\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "# Use Rome time\n",
    "logging.Formatter.converter = lambda *args: datetime.now(ZoneInfo(\"Europe/Rome\")).timetuple()\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "logging.info(\"=== Logging started (fresh current run) ===\")\n",
    "logging.info(\"Previous run (if any): %s\", PREV_LOG if PREV_LOG.exists() else \"none\")\n",
    "\n",
    "if added_src:\n",
    "    logging.info(\"heat_forecast not installed; added src/ to sys.path\")\n",
    "else:\n",
    "    logging.info(\"heat_forecast imported without modifying sys.path (likely installed)\")\n",
    "\n",
    "OPTUNA_DIR = BASE_DIR / \"results\" / \"finetuning\" / \"lstm\"\n",
    "OPTUNA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(\"BASE_DIR (make sure it's '*/heat-forecast/', else cd and re-run): %s\", BASE_DIR)\n",
    "logging.info(\"LOG_DIR: %s\", LOG_DIR)\n",
    "logging.info(\"OPTUNA_DIR: %s\", OPTUNA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19673726",
   "metadata": {},
   "source": [
    "Ensure [compatibility with Numba](https://numba.readthedocs.io/en/stable/user/installing.html#numba-support-info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, numpy, numba\n",
    "logging.info(\"=== Current Environment ===\")\n",
    "logging.info(\"Python : %s\", sys.version.split()[0])\n",
    "logging.info(\"NumPy  : %s\", numpy.__version__)\n",
    "logging.info(\"Numba  : %s\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9133e",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Magic Commands ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "os.environ[\"OPTUNA_LOGGING_DISABLE_DEFAULT_HANDLER\"] = \"1\" # prevent Optuna from attaching its handler\n",
    "import stat\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import torch\n",
    "import optuna\n",
    "import copy\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "import yaml\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Plotting Configuration ---\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['axes.grid.which'] = 'both'\n",
    "\n",
    "# --- YAML Customization ---\n",
    "from heat_forecast.utils.yaml import safe_dump_yaml\n",
    "\n",
    "# --- Safe File Deletion Helper ---\n",
    "from heat_forecast.utils.fileshandling import remove_tree\n",
    "\n",
    "# --- Project-Specific Imports ---\n",
    "from heat_forecast.utils.cv_utils import get_cv_params_for_test\n",
    "from heat_forecast.pipeline.lstm import (\n",
    "    NormalizeConfig, ModelConfig, DataConfig, FeatureConfig, TrainConfig, \n",
    "    LSTMRunConfig, LSTMPipeline\n",
    ")\n",
    "from heat_forecast.utils.optuna import (\n",
    "    OptunaStudyConfig, run_study, continue_study, describe_suggester, rename_study, clone_filtered_study\n",
    ")\n",
    "from heat_forecast.utils.plotting import interactive_plot_cutoff_results\n",
    "\n",
    "logging.info(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f3329",
   "metadata": {},
   "source": [
    "Import pre-elaborated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'heat.csv'\n",
    "aux_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'auxiliary.csv'\n",
    "heat_df = pd.read_csv(heat_path, parse_dates=['ds'])\n",
    "aux_df = pd.read_csv(aux_path, parse_dates=['ds'])\n",
    "logging.info(\"Loaded heat data: %s\", heat_path.relative_to(BASE_DIR))\n",
    "logging.info(\"Loaded auxiliary data: %s\", aux_path.relative_to(BASE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f27e8b",
   "metadata": {},
   "source": [
    "Set device (the LSTM pipeline automatically uses \"cuda\" if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69b80d",
   "metadata": {},
   "source": [
    "Set a seed (set to None to skip):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80278336",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "logging.info(f\"Using seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b4504e",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148cb33",
   "metadata": {},
   "source": [
    "### Example of usage of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12455ff6",
   "metadata": {},
   "source": [
    "Example configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose unique_id and filter\n",
    "unique_id = 'F1'\n",
    "heat_id_df = heat_df[heat_df['unique_id'] == unique_id]\n",
    "aux_id_df = aux_df[aux_df['unique_id'] == unique_id]\n",
    "\n",
    "\n",
    "# Set configuration of the pipeline\n",
    "config = LSTMRunConfig(\n",
    "    model=ModelConfig(\n",
    "        input_len=72, \n",
    "        output_len=168, \n",
    "        hidden_size=8, \n",
    "        num_layers=1,\n",
    "        head=\"linear\",\n",
    "        dropout=0.0,\n",
    "        use_ar=\"24h\"\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        batch_size=64,   \n",
    "    ),\n",
    "    features=FeatureConfig(\n",
    "        exog_vars=(\"temperature\",),\n",
    "        hour_averages=(),        \n",
    "        endog_hour_lags=(24, 168,),  \n",
    "        use_differences=False,\n",
    "        use_cold_season=False,\n",
    "        cold_temp_threshold=15,\n",
    "        include_exog_lags=False\n",
    "    ),\n",
    "    train=TrainConfig(\n",
    "        learning_rate=5e-4, \n",
    "        n_epochs=20, \n",
    "        patience=5, \n",
    "        tf_drop_epochs=4,\n",
    "        tf_mode=\"linear\",\n",
    "    ),\n",
    "    norm=NormalizeConfig(mode=\"global\"),\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "pipe = LSTMPipeline(target_df=heat_id_df, config=config, aux_df=aux_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97cb6e",
   "metadata": {},
   "source": [
    "Generate loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose params for data split\n",
    "end_train = pd.Timestamp(\"2023-11-01 23:00\")\n",
    "end_val   = pd.Timestamp(\"2024-04-01 23:00\")  # val will start at end_train + 1h automatically\n",
    "\n",
    "# Build datasets and load\n",
    "train_loader, val_loader = pipe.make_loaders(\n",
    "    end_train=end_train,\n",
    "    end_val=end_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b676db9",
   "metadata": {},
   "source": [
    "Description of the prepared dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data = pipe.describe_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a38ce",
   "metadata": {},
   "source": [
    "Description of the prepared model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb37f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_model = pipe.describe_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee67c9",
   "metadata": {},
   "source": [
    "The floowing method `sanity_overfit_one_batch` performs a **sanity check** by trying to overfit the model on a tiny, fixed subset of the training data. It trains for up to `max_epochs` without early stopping, then checks whether the training loss drops by at least a required relative amount (`tol_rel_drop`).\n",
    "\n",
    "If successful, it signals the pipeline and optimization are working correctly; otherwise, it may indicate bugs or misconfigurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba498906",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pipe.sanity_overfit_one_batch(\n",
    "    train_loader,\n",
    "    max_epochs=300,         # bump if needed\n",
    "    tol_rel_drop=0.8,      # percentage drop in ORIGINAL units (MAE by default) to consider as \"passed\"\n",
    "    restore_weights=True,   # keep your model “fresh” after the check\n",
    "    n_samples=16,           # tiny fixed subset\n",
    "    gen_seed=SEED           # deterministic subset selection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aab2d9",
   "metadata": {},
   "source": [
    "This method trains the model under the current configuration.\n",
    "If a validation loader is provided, it monitors validation loss, applies early stopping, and logs the best epoch reached; otherwise, it trains for the full number of epochs and reports final training loss.\n",
    "In all cases, it applies the configured learning schedule (teacher forcing, LR drop, gradient clipping), records history and metrics, and returns a results dictionary with training outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf1da6",
   "metadata": {},
   "source": [
    "The `predict` method produces forecasts on the original target scale. It can run in two modes:\n",
    "\n",
    "* Batch mode: pass a `val_loader` to predict multiple windows, returning `unique_id`, `ds`, `cutoff`, and predictions.\n",
    "* Single-horizon mode: pass a `cutoff` to predict one forecast horizon, returning `unique_id`, `ds`, and predictions.\n",
    "\n",
    "We use the first mode to compute predictions over several months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure this holds: start_test >= end_train + gap + 1h, else will raise\n",
    "start_test = pd.Timestamp(\"2024-10-01 00:00:00\")\n",
    "end_test   = pd.Timestamp(\"2024-12-01 23:00:00\")\n",
    "gap        = int(pipe.config.data.gap_hours) # default is 0\n",
    "\n",
    "# When norm.mode=\"global\", statistics are computed from the training slice. \n",
    "# If no training range is provided, the pipeline will reuse cached stats (if available). \n",
    "# To avoid surprises, it is recommended to always specify the train dates explicitly.\n",
    "_, test_loader = pipe.make_loaders(\n",
    "    end_train=end_train,\n",
    "    start_val=start_test,\n",
    "    end_val=end_test,\n",
    ")\n",
    "\n",
    "# Copute predictions on the given loader\n",
    "preds = pipe.predict(val_loader=test_loader, alias=\"LSTM\")\n",
    "display(preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a794a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_forecasts_with_exog(\n",
    "    target_df=heat_df,\n",
    "    cv_df=preds,\n",
    "    aux_df=aux_df,\n",
    "    exog_vars=[\"temperature\"],\n",
    "    n_windows=1,\n",
    "    add_context=True,\n",
    "    only_aligned_to_day=True,\n",
    "    figsize=(11, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75fce8",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning (Optuna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109e1dc",
   "metadata": {},
   "source": [
    "We tune the model using **Optuna**, a Python framework for hyperparameter optimization that supports both **grid search** and more efficient, adaptive methods. With Optuna we can easily run evaluations over a fixed hyperparameter grid, or use its **TPE (Tree-structured Parzen Estimator)** sampler to explore continuous, large and / or conditional search spaces in a data-driven way. The TPE sampler fits separate probabilistic models to \"good\" and \"bad\" parameter sets and selects new trials that maximize the ratio of the two (i.e. promising configurations).\n",
    "\n",
    "For more details on TPE and Optuna, see:\n",
    "\n",
    "* [Optuna main site](https://optuna.org/)\n",
    "* [TPESampler documentation](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) \n",
    "* [This article, for a deeper understanding of the TPESampler](https://arxiv.org/abs/2304.11127)\n",
    "\n",
    "Overview of the following subsections:\n",
    "* **\"Start a new study\"**: how to create and run a new optimization study from scratch.\n",
    "* **\"Review past studies\"**: how to load a completed Optuna study in order to delete, copy, continue, or inspect its results in detail.\n",
    "\n",
    "A detailed review and commentary of the tuning results is provided in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa710f",
   "metadata": {},
   "source": [
    "### Start a new study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6450f",
   "metadata": {},
   "source": [
    "As a first step to start a study, select the time series (`unique_id`) you want to optimize. Next, choose a **suggester function** (or define a new one in `heat_forecast/suggesters/lstm.py`) that specifies the hyperparameter search space. Finally, configure the Optuna study by specifying the sampler, pruner, study name, optimization objective, and other parameters that control the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Select series ---\n",
    "unique_id = 'F2'\n",
    "\n",
    "# --- Step 2: Define search space ---\n",
    "suggester_name = \"final_v2_F2\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# Set path based on unique_id\n",
    "db_path = OPTUNA_DIR / f\"optuna_{unique_id}.db\"  # single DB file for each id\n",
    "storage_url = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "# Describe suggester\n",
    "desc = describe_suggester(suggester_name)\n",
    "logging.info(f\"Suggester used:\\n{desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Configure study ---\n",
    "optuna_cfg = OptunaStudyConfig(\n",
    "    # General\n",
    "    study_name='final_study_v2_NAR_F1',\n",
    "    objective=\"avg_near_best\", # \"best\" or \"last\" (based on val metric)\n",
    "    n_trials=200,           # number of trials, use None for grid search\n",
    "    timeout=None,            # timeout for the study (max time per trial in seconds)\n",
    "    seed=SEED,               # seed for reproducibility of the optuna sampler\n",
    "    storage=storage_url,     # storage URL for the study \n",
    "    pruner=\"percentile\",            # type of pruner: \"percentile\", \"median\", \"nop\"\n",
    "    sampler=\"tpe\",          # type of sampler: \"tpe\", \"grid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930d3f4",
   "metadata": {},
   "source": [
    "Run the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Run the study ---\n",
    "do_run = False\n",
    "\n",
    "# === Do not edit below ===\n",
    "if do_run:\n",
    "    # Set path based on unique_id\n",
    "    db_path = OPTUNA_DIR / f\"optuna_{unique_id}.db\"  # single DB file for each id\n",
    "    storage_url = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "    # Set base configuration of the pipeline\n",
    "    base_cfg = LSTMRunConfig(\n",
    "        model=ModelConfig(),\n",
    "        data=DataConfig(),\n",
    "        features=FeatureConfig(),\n",
    "        train=TrainConfig(),\n",
    "        norm=NormalizeConfig(),\n",
    "    )\n",
    "\n",
    "    start_train = None # -> first date available \n",
    "    start_val = pd.Timestamp(\"2023-11-01 00:00\") \n",
    "    end_train = pd.Timestamp(\"2023-10-31 23:00\")  \n",
    "    end_val = pd.Timestamp(\"2024-04-01 23:00\")\n",
    "\n",
    "    optuna_cfg.storage = storage_url\n",
    "\n",
    "    study = run_study(\n",
    "        unique_id,\n",
    "        heat_df, \n",
    "        aux_df, \n",
    "        base_cfg,\n",
    "        start_train=start_train, end_train=end_train, \n",
    "        start_val=start_val, end_val=end_val,\n",
    "        optuna_cfg=optuna_cfg,\n",
    "        suggest_config_name=suggester_name,\n",
    "    )\n",
    "    print(\"Best value (val loss):\", study.best_value)\n",
    "    print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595d3b8",
   "metadata": {},
   "source": [
    "### Review past studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0889e4f",
   "metadata": {},
   "source": [
    "In this section, we load an existing study and choose whether to continue, delete, rename, copy, or inspect it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a8e63",
   "metadata": {},
   "source": [
    "#### Load and/or modify a study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72468187",
   "metadata": {},
   "source": [
    "See available studies for a fixed ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose unique_id\n",
    "unique_id = 'F5'\n",
    "\n",
    "# === Do not edit below ===\n",
    "# Set path based on unique_id\n",
    "db_path = OPTUNA_DIR / f\"optuna_{unique_id}.db\"  # single DB file for each id\n",
    "storage_url = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "# Get all study summaries\n",
    "study_summaries = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "# Print study summaries\n",
    "study_summaries = sorted(study_summaries, key=lambda s: s.study_name)\n",
    "lines = [f\"Study name: {s.study_name}, trials: {s.n_trials}\" for s in study_summaries]\n",
    "logging.info(\"Available studies:\\n\\n\" + \"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01211f8",
   "metadata": {},
   "source": [
    "Choose a study to continue/delete/review by selecting its name below. Then view a description of the search space used for that study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"preliminary_study_v4_F5\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# View detailed description of the search space for the study (the suggester documentation)\n",
    "study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "desc = describe_suggester(study.user_attrs.get(\"suggest_config_name\", \"\"))\n",
    "parent_study = study.user_attrs.get(\"_parent_study\", \"\")\n",
    "txt = f\"parent study ({parent_study}):\" if parent_study else 'this study:'\n",
    "logging.info(f\"Suggester used in {txt} \\n{desc}\\n\")\n",
    "if parent_study:\n",
    "    filter = yaml.dump(study.user_attrs.get('_substudy_filter', ''), indent=4, sort_keys=False)\n",
    "    logging.info(f\"This substudy was obtained though the filter: \\n{filter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b76463",
   "metadata": {},
   "source": [
    "Optionally continue the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_continue = False\n",
    "\n",
    "# Choose how many trials to add\n",
    "n_new_trials = 30\n",
    "\n",
    "# === Do not edit below ===\n",
    "if do_continue:\n",
    "    # Continue the loaded study with additional trials\n",
    "    study = continue_study(\n",
    "        study_name,\n",
    "        storage_url,\n",
    "        n_new_trials=n_new_trials,\n",
    "        target_df=heat_df,\n",
    "        aux_df=aux_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522adfa",
   "metadata": {},
   "source": [
    "Optionally create a sub-study selecting trials based on a predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9105a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_create = False\n",
    "\n",
    "if do_create:\n",
    "    study = clone_filtered_study(\n",
    "        storage_url=storage_url,\n",
    "        src_name=study_name,\n",
    "        new_name=\"\",\n",
    "        save_to_storage=True,\n",
    "        predicate=lambda t: (\n",
    "            t.params.get(\"train.learning_rate\") is not None\n",
    "            and float(t.params[\"train.learning_rate\"]) > 2.2e-4\n",
    "        ),\n",
    "        dry_run=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c316a59",
   "metadata": {},
   "source": [
    "Optionally delete the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6921387",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_delete = False\n",
    "\n",
    "if do_delete:\n",
    "    optuna.delete_study(\n",
    "        study_name=\"\", # change to `study_name` if you are really sure you want to proceed \n",
    "        storage=storage_url,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d30808",
   "metadata": {},
   "source": [
    "Optionally rename the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_rename = False\n",
    "\n",
    "if do_rename:\n",
    "    study = rename_study(\n",
    "        old_name=\"\",\n",
    "        new_name=\"\",\n",
    "        storage_url=storage_url,\n",
    "        keep_old=False,\n",
    "        dry_run=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68e7ec",
   "metadata": {},
   "source": [
    "#### View study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heat_forecast.utils.optuna import (\n",
    "    trials_df, trials_df_for_display, summarize_params_coverage, \n",
    "    plot_intermediate_values, plot_optimization_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334e24a",
   "metadata": {},
   "source": [
    "Optionally filter based on a predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd96607",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_filter = False\n",
    "\n",
    "if do_filter:\n",
    "    study_filtered = clone_filtered_study(\n",
    "        src_study=study,\n",
    "        predicate=lambda t: (\n",
    "            t is not None\n",
    "            #and t.params.get(\"train.drop_epoch\") == 7\n",
    "            and float(t.params.get(\"train.learning_rate\")) > 7e-4\n",
    "            and float(t.params.get(\"train.learning_rate\")) < 1.5e-3\n",
    "            #and t.user_attrs.get(\"n_params\") is not None\n",
    "            #and 10000 < float(t.user_attrs[\"n_params\"]) < 200000\n",
    "            and t.params.get(\"model.dropout\") < .08\n",
    "            #and t.params.get(\"model.hidden_size\") > 48\n",
    "            #and not ((t.params.get(\"model.hidden_size\") == 8)\n",
    "            #or (t.params.get(\"model.hidden_size\") == 40 and t.params.get(\"model.num_layers\") == 1)\n",
    "            #or (t.params.get(\"model.hidden_size\") == 32 and t.params.get(\"model.num_layers\") == 2))\n",
    "            #and t.params.get(\"model.hidden_size\") == 112\n",
    "            #and t.params.get(\"model.num_layers\") == 2\n",
    "            #and t.params.get(\"train.learning_rate\") < 1.5e-3\n",
    "            #and t.params.get(\"model.dropout\") < .05\n",
    "            #and t.params.get(\"model.dropout\") > 0.1\n",
    "            #and t.params.get(\"model.hidden_size\") <= 12\n",
    "            #and t.params.get(\"model.num_layers\") == 1\n",
    "\n",
    "            ),\n",
    "    )\n",
    "else:\n",
    "    study_filtered = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fa036",
   "metadata": {},
   "source": [
    "View best trials in the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of all trials\n",
    "df, val_name = trials_df(study_filtered)\n",
    "\n",
    "# === Do not edit below ===\n",
    "logging.info(f\"Trials total={len(df)}, complete={sum(df['state']=='COMPLETE')}, pruned={sum(df['state']=='PRUNED')}, \" \\\n",
    "             f\"fail={sum(df['state']=='FAIL')}, running={sum(df['state']=='RUNNING')}, waiting={sum(df['state']=='WAITING')}\")\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.max_rows\", None):\n",
    "    display(trials_df_for_display(df, val_name).head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d1af",
   "metadata": {},
   "source": [
    "View values distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a29fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column\n",
    "data = df[val_name].dropna()\n",
    "\n",
    "# Compute stats\n",
    "stats = {\n",
    "    \"Min\": data.min(),\n",
    "    \"Q1 (25%)\": data.quantile(0.25),\n",
    "    \"Median (Q2)\": data.median(),\n",
    "    \"Mean\": data.mean(),\n",
    "    \"Q3 (75%)\": data.quantile(0.75),\n",
    "    \"Max\": data.max(),\n",
    "    \"Std Dev\": data.std(),\n",
    "    \"Count\": data.count()\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "stats_df = pd.DataFrame(stats, index=[val_name])\n",
    "display(stats_df)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(data, bins=30)\n",
    "plt.title(f\"Distribution of {val_name}\", fontsize=14)\n",
    "plt.xlabel(val_name)\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56849f14",
   "metadata": {},
   "source": [
    "View optimization history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cfd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468a13b",
   "metadata": {},
   "source": [
    "View the intermediate validation losses for each trial. The function also supports filtering curves based on specific trial parameters or attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_values(\n",
    "    study_filtered, \n",
    "    # --- Apply custom filtering here if needed ---\n",
    "    #include_params={'features.lags_key': 'none'}, \n",
    "    predicate=lambda t: (\n",
    "        t is not None\n",
    "        and t.params.get(\"train.learning_rate\") > 3e-4\n",
    "        #and t.params.get(\"train.learning_rate\") < 1e-3\n",
    "        and t.params.get(\"model.dropout\") < .15\n",
    "        #and t.params.get(\"model.dropout\") > 0.01\n",
    "        #and (t.params.get(\"model.hidden_size\") == 12)\n",
    "        #and t.params.get(\"model.num_layers\") == 2\n",
    "        #and t.params.get(\"train.use_weight_decay\") == False\n",
    "        #and float(t.params.get(\"train.learning_rate\")) > 7e-4\n",
    "        #and float(t.params.get(\"train.learning_rate\")) < 1.5e-3\n",
    "        #and t.user_attrs.get(\"n_params\") is not None\n",
    "        #and 10000 < float(t.user_attrs[\"n_params\"]) < 200000\n",
    "        #and t.params.get(\"model.dropout\") < .2\n",
    "    ),\n",
    "    dim_excluded=False,\n",
    "    dim_factor=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a94598",
   "metadata": {},
   "source": [
    "Analysis of coverage of the parameters space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c699a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "num_sty, cat_sty = summarize_params_coverage(study, df, val_name)\n",
    "\n",
    "with pd.option_context(\"display.float_format\", lambda v: f\"{v:,.4f}\"):\n",
    "    if num_sty:\n",
    "        logging.info(\"Coverage summary of numeric parameters:\")\n",
    "        display(num_sty)\n",
    "    else:\n",
    "        logging.info(\"No numeric parameters found for the study.\")\n",
    "    if cat_sty:\n",
    "        logging.info(\"Coverage summary of categorical parameters:\")\n",
    "        display(cat_sty)\n",
    "    else:\n",
    "        logging.info(\"No categorical parameters found for the study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee6867",
   "metadata": {},
   "source": [
    "Counts of best epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# === Do not edit below ===\n",
    "best_epochs = df[df['state'] == 'COMPLETE']['user_attrs_best_epoch']\n",
    "\n",
    "s = pd.Series(pd.to_numeric(best_epochs, errors=\"coerce\")).dropna().astype(int)\n",
    "if not s.any():\n",
    "    logging.info(\"No best_epoch found.\")\n",
    "else:\n",
    "    # count each integer and include missing integers with count=0\n",
    "    lo, hi = int(s.min()), int(s.max())\n",
    "    counts = s.value_counts().sort_index()\n",
    "    counts = counts.reindex(range(lo, hi + 1), fill_value=0)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(9, 3.5), constrained_layout=True)\n",
    "    ax.bar(counts.index, counts.values, width=0.8)\n",
    "    ax.set_xlabel(\"Best epoch\")\n",
    "    ax.set_ylabel(\"Count of trials\")\n",
    "    ax.set_title(f\"Best epoch counts\")\n",
    "    ax.set_xticks(counts.index)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))  # integer ticks only\n",
    "\n",
    "    # log best\n",
    "    logging.info(f\"Best epoch by median: {s.median() :.0f}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5af1de",
   "metadata": {},
   "source": [
    "#### View results marginalized on single hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0444f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heat_forecast.utils.optuna import (\n",
    "    plot_marginals_1d, plot_param_importances, display_marginals_1d\n",
    ")\n",
    "import optuna.importance as imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5f994",
   "metadata": {},
   "source": [
    "View fANOVA importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "# get importances\n",
    "imps = imp.get_param_importances(study_filtered)\n",
    "imps = pd.Series(imps, dtype=float).sort_values(ascending=False)\n",
    "\n",
    "# plot\n",
    "if imps.any():\n",
    "    fig, ax = plot_param_importances(imps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959f0ad",
   "metadata": {},
   "source": [
    "Compute and plot 1D marginal distributions. Many of the following functions also accept arguments such as `non_params_to_allow`, which lets you include selected trial user attributes in the marginal computations (treating them as parameters), and `objective`, which allows you to replace the objective value with any other numeric user attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marginals_1d(\n",
    "    df, val_name,\n",
    "    bins_numeric=7,\n",
    "    non_params_to_allow=[\"user_attrs_n_params\"],\n",
    "    #objective=\"user_attrs_avg_near_best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaef843",
   "metadata": {},
   "source": [
    "Plot only a subset of marginals (e.g. most important):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ea21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_params = imps[:1].index.tolist()\n",
    "\n",
    "plot_marginals_1d(\n",
    "    df, val_name,\n",
    "    params=most_imp_params,\n",
    "    bins_numeric=12,\n",
    "    #non_params_to_allow=[\"user_attrs_n_params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24212da0",
   "metadata": {},
   "source": [
    "Detailed summaries for each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09db694",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20          # Will show the fraction of trials for each parameter choice that belongs to the top_k trials\n",
    "top_frac = 0.20     # Will show the fraction of trials for each parameter choice that belongs to the top_frac trials\n",
    "params = None\n",
    "\n",
    "tbls_sty = display_marginals_1d(\n",
    "    df, val_name,\n",
    "    #params=[],\n",
    "    non_params_to_allow=[\"user_attrs_n_params\"],\n",
    "    #objective=\"user_attrs_avg_near_best\",\n",
    "    top_k=top_k,\n",
    "    top_frac=top_frac,\n",
    "    binning_numeric=\"quantile\",\n",
    "    custom_edges_dict={\"user_attrs_n_params\": [40000, 110000, 160000]},\n",
    "    bins_numeric=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8251ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def bootstrap_region_ci(\n",
    "    df,\n",
    "    val_name,\n",
    "    region_mask,\n",
    "    stat: Callable[[np.ndarray], float] = np.mean,\n",
    "    n_boot: int = 2000,\n",
    "    objective: str | None = None,\n",
    "    difference: bool = True,\n",
    "    seed: int = 0,\n",
    "    dropna: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstrap CI for a region's mean (difference=False) or for the difference of means\n",
    "    between region and its complement (difference=True), using COMPLETE trials only.\n",
    "\n",
    "    Returns:\n",
    "        point_estimate: float  (mean or mean difference)\n",
    "        ci: tuple (low, high)\n",
    "        sizes: dict {'n_region': int, 'n_complement': int}\n",
    "    \"\"\"\n",
    "    if objective is None:\n",
    "        objective = val_name\n",
    "\n",
    "    # Split region vs complement and filter COMPLETE trials\n",
    "    dfc_reg = df[region_mask & (df[\"state\"] == \"COMPLETE\") & df[objective].notna()].copy()\n",
    "    dfc_compl = df[~region_mask & (df[\"state\"] == \"COMPLETE\") & df[objective].notna()].copy()\n",
    "\n",
    "    # Extract values\n",
    "    x = dfc_reg[objective].to_numpy(dtype=float)\n",
    "    y = dfc_compl[objective].to_numpy(dtype=float)\n",
    "\n",
    "    if dropna:\n",
    "        x = x[np.isfinite(x)]\n",
    "        y = y[np.isfinite(y)]\n",
    "\n",
    "    n_reg, n_compl = len(x), len(y)\n",
    "\n",
    "    # Basic sanity checks\n",
    "    if n_reg == 0:\n",
    "        raise ValueError(\"No COMPLETE rows in the selected region.\")\n",
    "    if difference and n_compl == 0:\n",
    "        raise ValueError(\"No COMPLETE rows in the complement region for difference CIs.\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Compute bootstrap replicates\n",
    "    boots = np.empty(n_boot, dtype=float)\n",
    "    if difference:\n",
    "        # Δ = mean(region) - mean(complement)\n",
    "        for b in range(n_boot):\n",
    "            bx = stat(rng.choice(x, size=n_reg, replace=True))\n",
    "            by = stat(rng.choice(y, size=n_compl, replace=True))\n",
    "            boots[b] = bx - by\n",
    "        point = stat(x) - stat(y)\n",
    "        label_suffix = \" difference\"\n",
    "    else:\n",
    "        for b in range(n_boot):\n",
    "            bx = stat(rng.choice(x, size=n_reg, replace=True))\n",
    "            boots[b] = bx\n",
    "        point = stat(x)\n",
    "        label_suffix = \"\"\n",
    "\n",
    "    # Percentile CI\n",
    "    low, high = np.percentile(boots, [2.5, 97.5])\n",
    "\n",
    "    logging.info(\n",
    "        f\"Bootstrap samples ({val_name}{label_suffix}):\\n\"\n",
    "        f\"point estimate                ={point:.2f}{f\" (SD={x.std(ddof=1):.2f})\" if not difference else ''},\\n\"\n",
    "        f\"95% CI                        =[{low:.2f}, {high:.2f}],\\n\"\n",
    "        f\"point +- symmetric half-width ={point:.2f} +- {((high - low) / 2):.2f},\\n\"\n",
    "        f\"actual half-widths            ={(high - point):.2f} / {(point - low):.2f},\\n\"\n",
    "        f\"n_region                      ={n_reg},\\n\"\n",
    "        f\"n_complement                  ={n_compl},\\n\"\n",
    "        f\"boot_mean                     ={boots.mean():.2f}, \\n\"\n",
    "        f\"boot_std                      ={boots.std(ddof=1):.2f}\\n\"\n",
    "    )\n",
    "\n",
    "    return point, (low, high), {\"n_region\": n_reg, \"n_complement\": n_compl}\n",
    "\n",
    "\n",
    "bm, (blo, bhi), (n_reg, n_tot) = bootstrap_region_ci(\n",
    "    df, val_name,\n",
    "    region_mask=(df['params_model.use_ar_prev']==True),\n",
    "    stat=np.mean,\n",
    "    n_boot=10000,\n",
    "    objective=val_name,\n",
    "    difference=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba14d82",
   "metadata": {},
   "source": [
    "#### Study interactions between hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7683d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from heat_forecast.utils.optuna import marginal_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfdb7e",
   "metadata": {},
   "source": [
    "Show parallel coordinate plot (mostly userful when using continuous params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frac = 0.2   # color only top_frac% of trials\n",
    "params = None    # choose params to plot\n",
    "# params = imps[:6].index.tolist()  # alternative: pick only the most important\n",
    "\n",
    "# === Do not edit below ===\n",
    "fig = plot_parallel_coordinate(study_filtered, params=params)\n",
    "\n",
    "vals = df[val_name].dropna().to_list()\n",
    "th = np.quantile(vals, top_frac)\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study_filtered, params=params)\n",
    "fig.data[0].dimensions[0].constraintrange = [min(vals), th]\n",
    "fig.update_coloraxes(cmin=min(vals), cmax=max(vals))  \n",
    "\n",
    "html = fig.to_html(include_plotlyjs=\"inline\", full_html=False)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8b189",
   "metadata": {},
   "source": [
    "Below we can visualize pairwise relationships between parameters or user-defined attributes by creating 2D marginal plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heat_forecast.utils.optuna import plot_marginals_2d\n",
    "\n",
    "fig, pivots = plot_marginals_2d(\n",
    "    df, val_name,\n",
    "    #objective=\"user_attrs_avg_near_best\",\n",
    "    #params=[\"model.hidden_size\", \"model.num_layers\", \"train.drop_epoch\", \"model.dropout\"], #imps.index[:3].tolist(),\n",
    "    #params=[\"model.dropout\", \"train.learning_rate\"], #imps.index[:3].tolist(),\n",
    "    as_first=\"model.input_len\", #imps.index[0],\n",
    "    statistic=\"mean\",\n",
    "    binning=\"quantile\",\n",
    "    show_text=True,\n",
    "    bins_a=4,\n",
    "    bins_b=4,\n",
    "    non_params_to_allow=[\"user_attrs_n_params\"],\n",
    "    custom_edges_dict_a={\"user_attrs_n_params\": [110000, 160000]},\n",
    "    custom_edges_dict_b={\"user_attrs_n_params\": [110000, 160000]},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353fee6",
   "metadata": {},
   "source": [
    "Display the tables plotted above, or choose a different statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08202e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic = \"mean\"\n",
    "n_max = 10 # max number of tables to display\n",
    "\n",
    "# === Do not edit below ===\n",
    "for key, pivs in list(pivots.items())[:n_max]:\n",
    "    logging.info(f\"2D marginal for {key}, statistic = '{statistic}':\")\n",
    "    piv = pivs.get(statistic)\n",
    "    display(piv if piv is not None else f\"(Not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44955580",
   "metadata": {},
   "source": [
    "## Tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_final_cfgs = LSTMRunConfig(\n",
    "    model=ModelConfig(\n",
    "        input_len=72, \n",
    "        head=\"linear\",\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        batch_size=64,\n",
    "    ),\n",
    "    features=FeatureConfig(\n",
    "        include_exog_lags=True\n",
    "    ),\n",
    "    train=TrainConfig(\n",
    "        grad_clip_max_norm=10.0,\n",
    "        use_lr_drop=True,\n",
    "        lr_drop_factor=0.3,\n",
    "        tf_drop_epochs=4,\n",
    "        lr_drop_epoch=4,\n",
    "        tf_mode=\"linear\",\n",
    "    ),\n",
    "    norm=NormalizeConfig(mode=\"global\"),\n",
    ")\n",
    "\n",
    "def final_cfgs(unique_id: str, horizon_type: str) -> dict:\n",
    "    # Define final configurations for different series\n",
    "    if unique_id not in ('F1', 'F2', 'F3', 'F4', 'F5') or horizon_type not in ('day', 'week'):\n",
    "        raise ValueError(\"Invalid unique_id or horizon_type.\")\n",
    "    cfg = copy.deepcopy(base_final_cfgs)\n",
    "    if unique_id == 'F1':\n",
    "        cfg.model.hidden_size = 64\n",
    "        cfg.model.num_layers = 2\n",
    "        cfg.model.dropout = 0.0\n",
    "        cfg.train.learning_rate = 8e-4 if horizon_type == 'week' else 1e-3\n",
    "        cfg.train.n_epochs = 7         if horizon_type == 'week' else 12\n",
    "        cfg.model.use_ar = \"24h\"       if horizon_type == 'week' else \"none\"\n",
    "        return cfg\n",
    "    elif unique_id == 'F2':\n",
    "        cfg.model.hidden_size = 112\n",
    "        cfg.model.num_layers = 1\n",
    "        cfg.train.learning_rate = 8e-4\n",
    "        cfg.train.n_epochs = 7  \n",
    "        cfg.model.dropout = 0.05    if horizon_type == 'week' else 0.0\n",
    "        cfg.model.use_ar = \"24h\"    if horizon_type == 'week' else \"none\"\n",
    "        return cfg\n",
    "    elif unique_id == 'F3':\n",
    "        cfg.model.hidden_size = 32\n",
    "        cfg.model.num_layers = 1\n",
    "        cfg.model.dropout = 0.25\n",
    "        cfg.train.learning_rate = 8e-4 if horizon_type == 'week' else 1e-3\n",
    "        cfg.train.n_epochs = 7         if horizon_type == 'week' else 12\n",
    "        cfg.model.use_ar = \"24h\"       if horizon_type == 'week' else \"none\"\n",
    "        return cfg\n",
    "    elif unique_id == 'F4':\n",
    "        cfg.model.hidden_size = 16\n",
    "        cfg.model.num_layers = 1\n",
    "        cfg.model.dropout = 0.1        if horizon_type == 'week' else 0.0\n",
    "        cfg.train.use_lr_drop = False\n",
    "        cfg.train.tf_drop_epochs = 8\n",
    "        cfg.train.learning_rate = 4e-4 if horizon_type == 'week' else 1e-3\n",
    "        cfg.train.n_epochs = 4         if horizon_type == 'week' else 10\n",
    "        cfg.model.use_ar = \"24h\"       if horizon_type == 'week' else \"none\"\n",
    "        return cfg\n",
    "    elif unique_id == 'F5':\n",
    "        cfg.model.hidden_size = 8\n",
    "        cfg.model.num_layers = 1\n",
    "        cfg.model.dropout = 0.0\n",
    "        cfg.train.learning_rate = 1.7e-3 if horizon_type == 'week' else 7e-4\n",
    "        cfg.train.n_epochs = 3           if horizon_type == 'week' else 16\n",
    "        cfg.model.use_ar = \"24h\"         if horizon_type == 'week' else \"none\"\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8080ec",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e9909",
   "metadata": {},
   "source": [
    "Code for final testing with the tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = False\n",
    "grid = list(product(['F5'], ['day']))\n",
    "\n",
    "if do_test:\n",
    "    for id, horizon_type in tqdm(grid, desc=\"Test\", leave=True):\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        # --- Create directory for test results ---\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        run_id = f\"{id}_{horizon_type}_test_lstm_{timestamp}\"\n",
    "        path = BASE_DIR / \"results\" / \"test\" / \"lstm\" / run_id\n",
    "        metadata['run_id'] = run_id\n",
    "\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "            logging.info(f\"Created directory for test results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "\n",
    "            # --- Set params for cv ---\n",
    "            out = get_cv_params_for_test(horizon_type)\n",
    "            metadata['for_cv'] = {\n",
    "                'step_size': out['step_size'],\n",
    "                'test_hours': out['test_hours'],\n",
    "                'end_test_cv': str(out['end_test_actual']),\n",
    "                'n_windows': out['n_windows'],\n",
    "                'refit': out['refit'],\n",
    "                'n_fits': out['n_fits'],\n",
    "            }\n",
    "\n",
    "            # ------------- Run cross-validation with the optimal parameters -------------\n",
    "            # create pipeline and generate futures\n",
    "            heat_id_df = heat_df[heat_df['unique_id'] == id]\n",
    "            aux_id_df = aux_df[aux_df['unique_id'] == id]\n",
    "            config = final_cfgs(id, horizon_type)\n",
    "            pipe = LSTMPipeline(\n",
    "                target_df=heat_id_df, \n",
    "                config=config, \n",
    "                aux_df=aux_id_df, \n",
    "            )\n",
    "            metadata['model_config'] = config.to_dict()\n",
    "            metadata['device'] = DEVICE.type\n",
    "\n",
    "            # Run cv\n",
    "            t0 = pd.Timestamp.now()\n",
    "            cv_df = pipe.cross_validation(\n",
    "                test_size=out['test_hours'],  # Test size in hours\n",
    "                end_test=out['end_test_actual'],  # End of the test period\n",
    "                step_size=out['step_size'],   # Step size in hours\n",
    "                refit=out['refit'],  # Do not refit the model on each window\n",
    "                verbose=True,\n",
    "            )\n",
    "            t1 = pd.Timestamp.now()\n",
    "\n",
    "            avg_elapsed = (t1 - t0).total_seconds() / out['n_fits']\n",
    "            metadata['avg_el_per_fit'] = avg_elapsed\n",
    "\n",
    "            cv_df.to_parquet(path / \"cv_df.parquet\", compression=\"snappy\")\n",
    "\n",
    "            metadata_path = path / 'metadata.yaml'\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                safe_dump_yaml(\n",
    "                    metadata,\n",
    "                    f,\n",
    "                    indent=4, \n",
    "                )\n",
    "\n",
    "            logging.info(f\"✓ Artifacts saved successfully for id={id}, horizon={horizon_type}.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(\"✗ Interrupted; cleaning up.\")\n",
    "            remove_tree(path, require_within=BASE_DIR)\n",
    "            logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "            raise\n",
    "        except Exception:\n",
    "            logging.exception(\"✗ Error during test for id=%s, horizon=%s; cleaning up.\", id, horizon_type)\n",
    "            remove_tree(path, require_within=BASE_DIR)\n",
    "            logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "            raise\n",
    "\n",
    "    logging.info(f\"✓ Test completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a run_id to analyze\n",
    "run_id = \"F5_day_test_sarimax_20251007T100300\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Load data ---\n",
    "path = BASE_DIR / \"results\" / \"test\" / \"sarimax\" / run_id\n",
    "cv_df = pd.read_parquet(path / \"cv_df.parquet\")\n",
    "with open(path / \"metadata.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = yaml.safe_load(f) \n",
    "\n",
    "# --- check cv_df ---\n",
    "from heat_forecast.utils.cv_utils import sanity_cv_df\n",
    "_ = sanity_cv_df(cv_df, metadata, positive_forecasts=True)\n",
    "\n",
    "interactive_plot_cutoff_results(\n",
    "    target_df=heat_df,\n",
    "    cv_df=cv_df,\n",
    "    add_context=False,\n",
    "    figsize=(11, 3)\n",
    ")\n",
    "\n",
    "# Pick a run_id to analyze\n",
    "run_id = \"F5_day_test_sarimax_alternative_20251007T033353\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Load data ---\n",
    "path = BASE_DIR / \"results\" / \"test\" / \"sarimax\" / run_id\n",
    "cv_df = pd.read_parquet(path / \"cv_df.parquet\")\n",
    "with open(path / \"metadata.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = yaml.safe_load(f) \n",
    "\n",
    "# --- check cv_df ---\n",
    "from heat_forecast.utils.cv_utils import sanity_cv_df\n",
    "_ = sanity_cv_df(cv_df, metadata, positive_forecasts=True)\n",
    "\n",
    "# --- visual check ---\n",
    "interactive_plot_cutoff_results(\n",
    "    target_df=heat_df,\n",
    "    cv_df=cv_df,\n",
    "    add_context=False,\n",
    "    figsize=(11, 3)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
