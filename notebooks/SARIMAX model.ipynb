{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83064710",
   "metadata": {},
   "source": [
    "# SARIMAX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b911d29",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755c538",
   "metadata": {},
   "source": [
    "This notebook presents our SARIMAX-based forecasting pipeline, implemented using Nixtla’s **`statsforecast`** library. The model combines a linear regression on carefully engineered exogenous variables (such as temperature, calendar features, and Fourier terms) with a **Seasonal ARIMA** (SARIMA) component that captures residual autocorrelation. The primary goal is to build, tune, and evaluate forecasting models for five previously analyzed time series (F1 to F5), across both **daily** and **weekly** forecast horizons. The entire modeling process is encapsulated within the `SARIMAXPipeline` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f00c7",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64716292",
   "metadata": {},
   "source": [
    "Given a target series $y_t$ and a vector of $k$ exogenous regressors $\\mathbf{x}_t=(x_{1,t},\\dots,x_{k,t})^{\\top}$, the SARIMAX model is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\boldsymbol{\\beta}^{\\!\\top} \\mathbf{x}_t + \\eta_t,\\qquad\n",
    "\\eta_t \\sim \\text{SARIMA}(p,d,q)\\times(P,D,Q)_s,\n",
    "$$\n",
    "\n",
    "with the SARIMA residual term $\\eta_t$ satisfying:\n",
    "\n",
    "$$ \\Phi_P(B^{s})\\,\\phi_p(B)\\,(1-B)^d(1-B^{s})^{D}\\,\\eta_t = \n",
    "\\Theta_Q(B^{s})\\,\\theta_q(B)\\,\\varepsilon_t,\\qquad\n",
    "\\varepsilon_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^{2}),\n",
    "$$\n",
    "\n",
    "where $B$ denotes the backshift operator, $B\\eta_t = \\eta_{t-1}$. The polynomials are defined as:\n",
    "\n",
    "* **Non-seasonal AR:** $\\phi_p(B) = 1 - \\phi_1 B - \\dots - \\phi_pB^p$\n",
    "* **Seasonal AR:** $\\Phi_P(B^{s}) = 1 - \\Phi_1 B^{s} - \\dots - \\Phi_PB^{Ps}$\n",
    "* **Non-seasonal MA:** $\\theta_q(B) = 1 + \\theta_1 B + \\dots + \\theta_qB^q$\n",
    "* **Seasonal MA:** $\\Theta_Q(B^{s}) = 1 + \\Theta_1 B^{s} + \\dots + \\Theta_QB^{Qs}$\n",
    "\n",
    "This model captures both short-term dynamics and recurring seasonal patterns. The autoregressive (AR) terms capture the influence of past observations, while the moving average (MA) terms account for past forecast errors. Differencing terms (both seasonal and non-seasonal) help make the series stationary and easier to model.\n",
    "\n",
    "Nixtla’s implementation estimates model parameters via exact maximum likelihood. Additionally, the API provides an `AutoARIMA` routine that automatically selects optimal $(p,d,q,P,D,Q)$ values by minimizing the corrected Akaike Information Criterion (AICc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653476f",
   "metadata": {},
   "source": [
    "### Why SARIMAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3e225",
   "metadata": {},
   "source": [
    "SARIMAX was chosen for its balance of interpretability, flexibility, and compatibility with exogenous features. Unlike pure time series models, SARIMAX allows the inclusion of covariates, making it particularly well-suited for structured forecasting tasks where external drivers (e.g., weather) are important.\n",
    "\n",
    "While the model offers strong performance and transparency, it does have limitations. In particular, feature engineering is manual and critical: the linear regression component cannot automatically extract useful signals from raw features. This stands in contrast to deep learning models, which can often discover non-linear patterns in high-dimensional feature spaces with minimal preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e952837",
   "metadata": {},
   "source": [
    "**Key Features of Our Implementation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc9f0b",
   "metadata": {},
   "source": [
    "* **Daily seasonality** is captured through the SARIMA residual component, enabling a flexible and adaptive structure for evolving intraday dynamics, unlike fixed-pattern methods such as Fourier series.\n",
    "* **Weekly seasonality** is explicitly modeled using exogenous Fourier terms, which effectively encode recurring weekly cycles.\n",
    "* **Long-term (e.g., annual) seasonality** is captured via exogenous temperature variables, which reflect underlying climate-driven trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8c115",
   "metadata": {},
   "source": [
    "## Import Libreries and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a2aa7",
   "metadata": {},
   "source": [
    "You can run the notebook in two ways:\n",
    "\n",
    "1. **Google Colab**: place the project folder `heat-forecast` in **MyDrive**. The setup cell below will mount Drive and automatically add `MyDrive/heat-forecast/src` to `sys.path` so `import heat_forecast` works out of the box.\n",
    "\n",
    "2. **Local machine**:\n",
    "\n",
    "   * **Installing our package:** from the project root, run `pip install -e .` once (editable install). Then you can open the notebook anywhere and import the package normally.\n",
    "   * **Alternative:** if you’re running the notebook from `.../heat-forecast/notebooks/` without installing the package, the setup cell will detect `../src` and automatically add it to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c85714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Detect if running on Google Colab & Set base dir ---\n",
    "# %cd /home/giovanni.lombardi/heat-forecast/notebooks\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Install required packages only if not already installed\n",
    "def pip_install(pkg: str):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Set base directory and handle environment\n",
    "if in_colab():\n",
    "    # Make sure IPython is modern (avoids the old %autoreload/imp issue if you ever use it)\n",
    "    pip_install(\"ipython>=8.25\")\n",
    "    pip_install(\"ipykernel>=6.29\")\n",
    "    \n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    for pkg in [\"statsmodels\", \"statsforecast\", \"mlforecast\"]:\n",
    "        pip_install(pkg)\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set base directory to your Drive project folder\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/heat-forecast')\n",
    "\n",
    "    # Add `src/` to sys.path for custom package imports\n",
    "    SRC_PATH = BASE_DIR / 'src'\n",
    "    if str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "\n",
    "    # Sanity checks (helpful error messages if path is wrong)\n",
    "    assert SRC_PATH.exists(), f\"Expected '{SRC_PATH}' to exist. Fix BASE_DIR.\"\n",
    "    pkg_dir = SRC_PATH / \"heat_forecast\"\n",
    "    assert pkg_dir.exists(), f\"Expected '{pkg_dir}' package directory.\"\n",
    "    init_file = pkg_dir / \"__init__.py\"\n",
    "    assert init_file.exists(), f\"Missing '{init_file}'. Add it so Python treats this as a package.\"\n",
    "\n",
    "else:\n",
    "    # Local: either rely on editable install (pip install -e .) or add src/ when running from repo\n",
    "    # Assume notebook lives in PROJECT_ROOT/notebooks/\n",
    "    BASE_DIR = Path.cwd().resolve().parent\n",
    "    SRC_PATH = BASE_DIR / \"src\"\n",
    "\n",
    "    added_src = False\n",
    "    if (SRC_PATH / \"heat_forecast\").exists() and str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "        added_src = True\n",
    "\n",
    "# --- Logging setup ---\n",
    "import logging\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR  = (BASE_DIR / \"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / \"run.log\"\n",
    "PREV_LOG = LOG_DIR / \"run.prev.log\"\n",
    "\n",
    "# If there's a previous run.log with content, archive it to run.prev.log\n",
    "if LOG_FILE.exists() and LOG_FILE.stat().st_size > 0:\n",
    "    try:\n",
    "        # Replace old run.prev.log if present\n",
    "        if PREV_LOG.exists():\n",
    "            PREV_LOG.unlink()\n",
    "        LOG_FILE.rename(PREV_LOG)\n",
    "    except Exception as e:\n",
    "        # Fall back to truncating if rename fails (e.g., file locked)\n",
    "        print(f\"[warn] Could not archive previous log: {e}. Truncating current run.log.\")\n",
    "        LOG_FILE.write_text(\"\")\n",
    "\n",
    "# Configure logging: fresh file for this run + echo to notebook/stdout\n",
    "file_handler   = logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\")\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "                        datefmt=\"%m-%d %H:%M:%S\")\n",
    "file_handler.setFormatter(fmt)\n",
    "stream_handler.setFormatter(fmt)\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.handlers[:] = [file_handler, stream_handler]  # replace handlers (important in notebooks)\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "# Use Rome time\n",
    "logging.Formatter.converter = lambda *args: datetime.now(ZoneInfo(\"Europe/Rome\")).timetuple()\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "logging.info(\"=== Logging started (fresh current run) ===\")\n",
    "logging.info(\"Previous run (if any): %s\", PREV_LOG if PREV_LOG.exists() else \"none\")\n",
    "\n",
    "if added_src:\n",
    "    logging.info(\"heat_forecast not installed; added src/ to sys.path\")\n",
    "else:\n",
    "    logging.info(\"heat_forecast imported without modifying sys.path (likely installed)\")\n",
    "\n",
    "OPTUNA_DIR = BASE_DIR / \"results\" / \"finetuning\" / \"lstm\"\n",
    "OPTUNA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(\"BASE_DIR (make sure it's '*/heat-forecast/', else cd and re-run): %s\", BASE_DIR)\n",
    "logging.info(\"LOG_DIR: %s\", LOG_DIR)\n",
    "logging.info(\"OPTUNA_DIR: %s\", OPTUNA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52934c21",
   "metadata": {},
   "source": [
    "Ensure [compatibility with Numba](https://numba.readthedocs.io/en/stable/user/installing.html#numba-support-info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fed2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, numpy, numba\n",
    "logging.info(\"=== Current Environment ===\")\n",
    "logging.info(\"Python : %s\", sys.version.split()[0])\n",
    "logging.info(\"NumPy  : %s\", numpy.__version__)\n",
    "logging.info(\"Numba  : %s\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a21008",
   "metadata": {},
   "source": [
    "Main imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Magic Commands ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Standard Library ---\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "import yaml\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import SeasonalNaive\n",
    "from mlforecast import MLForecast\n",
    "\n",
    "# --- Plotting Configuration ---\n",
    "interactive = False  # Set to False for static plots\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['axes.grid.which'] = 'both'\n",
    "\n",
    "# --- Safe File Deletion Helper ---\n",
    "from heat_forecast.utils.fileshandling import remove_tree\n",
    "\n",
    "# --- YAML Customization ---\n",
    "from heat_forecast.utils.yaml import safe_dump_yaml\n",
    "\n",
    "# --- Project-Specific Imports ---\n",
    "from heat_forecast.utils.plotting import (\n",
    "    configure_time_axes, custom_plot_results, scatter_temp_vs_target_hourly, interactive_plot_cutoff_results\n",
    ")\n",
    "\n",
    "from heat_forecast.utils.evaluation import (\n",
    "    custom_evaluate_cv, display_cv_summary, mae_over_thr_score,\n",
    "    compute_loss_diff_stats, adj_r2_score, aicc_score, underforecast_over_th_score,\n",
    "    plotly_cv_metric_by_cutoff, display_scrollable\n",
    ")\n",
    "from heat_forecast.utils.cv_utils import get_cv_params_for_test, get_cv_params_v2\n",
    "from heat_forecast.pipeline import SARIMAXPipeline, SARIMAXConfig\n",
    "\n",
    "logging.info(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4336fd",
   "metadata": {},
   "source": [
    "Import pre-elaborated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1291c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'heat.csv'\n",
    "aux_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'auxiliary.csv'\n",
    "heat_df = pd.read_csv(heat_path, parse_dates=['ds'])\n",
    "aux_df = pd.read_csv(aux_path, parse_dates=['ds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf894f7",
   "metadata": {},
   "source": [
    "## Exploring the relationship between temperature and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c831937",
   "metadata": {},
   "source": [
    "An important initial step in building the model is identifying potentially informative auxiliary variables. Since temperature exhibits the strongest correlation with the target variable (as seen in `\"Series analysis.ipynb\"`), we begin by incorporating only this exogenous feature. This analysis will be performed on a per-series basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edc38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a single unique_id\n",
    "id = 'F1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f58472",
   "metadata": {},
   "source": [
    "We plot heat demand against temperature, with each point colored by the corresponding hour of the day. For each hour (i.e., each color), the plot includes two regression lines: one fitted using data from the \"winter\" period (defined as mid-November to the end of March), and the other using data from the \"non-winter\" period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642153ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "mask = (\n",
    "    (heat_df['ds'] <= pd.Timestamp('2024-06-01')) \n",
    "    & (heat_df['unique_id'] == id)\n",
    ")\n",
    "heat_train_id_df = heat_df[mask].copy()\n",
    "mask = (\n",
    "    (aux_df['ds'] <= pd.Timestamp('2024-06-01')) \n",
    "    & (aux_df['unique_id'] == id)\n",
    ")\n",
    "aux_train_id_df = aux_df[mask].copy()\n",
    "\n",
    "corr_df = scatter_temp_vs_target_hourly(\n",
    "    target_df=heat_train_id_df,\n",
    "    aux_df=aux_train_id_df,\n",
    "    date_range=pd.date_range(start='2022-05-01', end='2024-05-01', freq='h'),\n",
    "    interactive=interactive\n",
    ")\n",
    "\n",
    "# Display the correlation dataframe\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(\"Correlation by season and hour:\")\n",
    "    display(\n",
    "        corr_df.style\n",
    "        .background_gradient(cmap='Blues', axis=None, gmap=corr_df.abs(), vmax=1.0, vmin=0.0)  # Add a blue gradient\n",
    "        .format(precision=2)\n",
    "        .set_caption(\"Correlation between Temperature and Heat Demand by Season and Hour\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be0177",
   "metadata": {},
   "source": [
    "The figure reveals a distinct difference in the temperature-demand relationship between winter and non-winter, underscoring the importance of explicitly accounting for this seasonal separation. This finding aligns with intuition, given that the target variable reflects heat demand. \n",
    "\n",
    "Additionally, in winter, the slope of the regression line varies noticeably by hour, indicating that daily seasonality is not driven solely by temperature. This again is expected, as human behavior plays a major role in shaping daily heating patterns. However, this complexity in daily seasonality will be addressed later by the SARIMAX residual component of the model, so for now we focus on the winter/non-winter separation only.\n",
    "\n",
    "A closer look at the target series pointed to the need for a dynamic treatment of seasonality. As already noted with the MSTL results, the transition from warm to cold regimes (and back) does not occur on fixed calendar dates; it is at least partly **temperature-driven**. Rather than imposing a fixed \"winter\" window, we detect the cold season adaptively from recent temperatures. Concretely, a timestamp is labeled as belonging to the *cold semester* if the **4-day trailing mean temperature** falls below a **series-specific threshold**. We tune this threshold per time series, yielding a more responsive, data-driven delineation of seasonal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99add866",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167db23b",
   "metadata": {},
   "source": [
    "The `SARIMAXPipeline.prepare_data` method provides a flexible framework for configuring exogenous variables, making it easy to experiment with and compare different modeling strategies. The supported exogenous inputs fall into two primary categories:\n",
    "\n",
    "* **Climate-based variables**, sourced from the `aux_df` dataset (e.g., temperature).\n",
    "* **Seasonal Fourier terms**, used to capture periodic patterns such as daily or weekly seasonality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407233f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "def piecewise_sigmoid(x, mid, lower, upper, k_left=0.5, k_right=None):\n",
    "    if not (lower < mid < upper):\n",
    "        raise ValueError(\"Must satisfy: lower < mid < upper\")\n",
    "    \n",
    "    x = np.asarray(x)\n",
    "\n",
    "    if k_right is None:\n",
    "        k_right = k_left * (mid - lower) / (upper - mid)\n",
    "\n",
    "    left_piece = lower + 2*(mid - lower) / (1 + np.exp(-k_left * (x - mid)))\n",
    "    right_piece = upper - 2*(upper - mid) / (1 + np.exp(k_right * (x - mid)))\n",
    "    \n",
    "    result = np.where(x <= mid, left_piece, right_piece)\n",
    "    return result.item() if result.ndim == 0 else result\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "def plot_piecewise_sigmoid(mid=16, lower=0, upper=24, k_left=0.2, k_right=None):\n",
    "    x_vals = np.linspace(lower - 10, upper + 10, 500)\n",
    "    y_vals = piecewise_sigmoid(x_vals, mid=mid, lower=lower, upper=upper, k_left=k_left, k_right=k_right)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x_vals, y_vals, label='piecewise_sigmoid', color='blue')\n",
    "    plt.axvline(mid, color='gray', linestyle='--', alpha=0.6, label='mid')\n",
    "    plt.axhline(lower, color='green', linestyle=':', alpha=0.5, label='lower')\n",
    "    plt.axhline(upper, color='red', linestyle=':', alpha=0.5, label='upper')\n",
    "    plt.title(\"Piecewise Sigmoid Function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Try the default parameters ---\n",
    "plot_piecewise_sigmoid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113b5e9",
   "metadata": {},
   "source": [
    "The `SARIMAXPipeline.prepare_data` method also supports transforming the target variable (`y`). This can help stabilize variance and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ceb31e",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0818ba",
   "metadata": {},
   "source": [
    "### Choosing an appropriate threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79375d79",
   "metadata": {},
   "source": [
    "A first, crucial parameter to choose is the temperature `threshold` parameter (for seasonal separation). To choose it, we perform a simple linear regression, regressing the target variable with the temperature with cold/warm seasonal separation (i.e., only three variables: `temperature_cold`, `temperature_warm` and the binary `is_cold`), and see which threshold works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an id and a candidate threshold \n",
    "id = 'F1'\n",
    "threshold = 13.5 # Example threshold\n",
    "\n",
    "# === Do not edit below ===\n",
    "# Initialize configuration for the desired data preparation\n",
    "sconfig = SARIMAXConfig(\n",
    "    with_exog=True,\n",
    "    exog_vars=['temperature'],\n",
    "    threshold=threshold,  \n",
    "    transform='none'\n",
    ")\n",
    "\n",
    "# Create the SARIMAX pipeline with the specified configuration and generate the data\n",
    "mask = (\n",
    "    (heat_df['ds'] <= pd.Timestamp('2024-06-01')) \n",
    "    & (heat_df['unique_id'] == id)\n",
    ")\n",
    "heat_train_id_df = heat_df[mask].copy()\n",
    "mask = (\n",
    "    (aux_df['ds'] <= pd.Timestamp('2024-06-01')) \n",
    "    & (aux_df['unique_id'] == id)\n",
    ")\n",
    "aux_train_id_df = aux_df[mask].copy()\n",
    "spipeline = SARIMAXPipeline(\n",
    "    target_df=heat_train_id_df,\n",
    "    aux_df=aux_train_id_df,\n",
    "    config=sconfig,\n",
    ")\n",
    "spipeline.prepare_data()\n",
    "prepared_data = spipeline.prepared_data\n",
    "\n",
    "spipeline.describe_prepared_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you want to overlay warm data as red vertical lines?\n",
    "highlight_warm_data = False\n",
    "\n",
    "# === Do not edit below ===\n",
    "# Fit model\n",
    "mf = MLForecast(models=LinearRegression(), freq=\"h\")\n",
    "mf.fit(prepared_data, fitted=True, static_features=[])\n",
    "\n",
    "# Get fitted values and calculate evaluation metrics\n",
    "fitted_values = mf.forecast_fitted_values()\n",
    "insample_forecasts = fitted_values['LinearRegression']\n",
    "insample_y = prepared_data['y']\n",
    "\n",
    "T = len(insample_y)\n",
    "K_par = len(mf.models_[\"LinearRegression\"].coef_) # Number of parameters in the linear model\n",
    "\n",
    "logging.info(f\"Results for threshold {threshold} and unique_id {id}:\")\n",
    "logging.info(f\"> Adjusted R^2 score  : {adj_r2_score(insample_y, insample_forecasts, T, K_par):.3f}\")\n",
    "logging.info(f\"> AICc score          : {aicc_score(insample_y, insample_forecasts, T, K_par):.3f}\")\n",
    "logging.info(f\"> MAE at-peak score   : {mae_over_thr_score(insample_y, insample_forecasts, y_th=17):.3f}\")\n",
    "logging.info(f\"> Underfit score      : {underforecast_over_th_score(insample_y, insample_forecasts, y_th=17):.3f}\")\n",
    "\n",
    "# plot the results\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].plot(prepared_data['ds'], insample_y, label='Transformed Target', color='black', alpha=1.0)\n",
    "axes[0].plot(fitted_values['ds'], insample_forecasts, label='Fitted Values', color='blue', alpha=0.7)\n",
    "axes[0].set_title('Fitted Values vs Target (kWh)')\n",
    "\n",
    "filtered_aux_train_df = aux_df.loc[\n",
    "    aux_df['ds'].isin(prepared_data['ds']), \n",
    "    ['ds', 'unique_id', 'temperature']\n",
    "]\n",
    "filtered_aux_train_df.sort_values(by='ds', inplace=True)\n",
    "\n",
    "axes[1].plot(filtered_aux_train_df['ds'], filtered_aux_train_df['temperature'], label='Temperature', color='blue', alpha=1.)\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "\n",
    "filtered_aux_train_df['temp_fewdays_avg'] = (\n",
    "    filtered_aux_train_df\n",
    "    .groupby('unique_id')['temperature']\n",
    "    .rolling(window=24*4, min_periods=1)  \n",
    "    .mean()\n",
    "    .reset_index(0, drop=True)\n",
    ")\n",
    "\n",
    "axes[2].plot(filtered_aux_train_df['ds'], filtered_aux_train_df['temp_fewdays_avg'], label='Temp 4-Day Avg', color='blue', alpha=1.)\n",
    "axes[2].set_title('Temp 4-Day Average (°C)')\n",
    "\n",
    "# Overlay missing timestamps as red vertical lines\n",
    "if highlight_warm_data:\n",
    "    cold_df = prepared_data[prepared_data['is_cold'] == 1].copy()\n",
    "    all = pd.date_range(start=prepared_data['ds'].min(), end=prepared_data['ds'].max(), freq='h')\n",
    "    actual = pd.Series(1, index=cold_df['ds'])\n",
    "    missing = all.difference(actual.index)\n",
    "    axes[0].vlines(\n",
    "        x=missing,\n",
    "        ymin=cold_df['y'].min(),  \n",
    "        ymax=cold_df['y'].max(), \n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        alpha=0.5,\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "configure_time_axes(axes, prepared_data['ds'])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096eec5",
   "metadata": {},
   "source": [
    "Based on a manual evaluation of several thresholds, we chose suitable thresholds for each series, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = {\n",
    "    'F1': 13.5,\n",
    "    'F2': 13.8,\n",
    "    'F3': 13.8,\n",
    "    'F4': 12.8,\n",
    "    'F5': 13.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ceafd",
   "metadata": {},
   "source": [
    "### Choosing temperature and target transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664f957",
   "metadata": {},
   "source": [
    "After selecting an appropriate threshold, the second step involves choosing suitable transformations for both the target and the temperature variables. We adopt the following strategy:\n",
    "\n",
    "* **No transformation of temperature**: We opt to use raw temperature values, as the time series do not show evidence of saturation effects on the cold side (i.e., there is no visible plateau in heat demand at very low temperatures). Additionally, potential saturation effects on the warm side are already accounted for by the seasonal (cold/warm) separation in the modeling pipeline.\n",
    "\n",
    "* **Box-Cox transform of the target (`boxcold_cold`)**: We apply a Box–Cox transformation to stabilize variance, with parameter lambda tuned to the extended-winter (cold-regime) segment of the series. Because the weekly seasonal regressors (Fourier terms) are estimated separately for cold vs. warm regimes but are **fixed within each regime**, we need daily/weekly seasonality to have roughly constant amplitude inside those windows, especially in the cold period. Emphasizing the cold regime in the transform reduces heteroscedasticity there, making the seasonal pattern more stable and improving fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975c26b",
   "metadata": {},
   "source": [
    "### Choosing climate-based features and SARIMA parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8b44c",
   "metadata": {},
   "source": [
    "Performing an exhaustive grid search over SARIMA parameters is impractical due to prohibitively long training times. Instead, we adopt a more efficient, staged approach:\n",
    "\n",
    "- **Step 1: Initial Model Selection**  \n",
    "  Begin by fixing a promising SARIMA structure for the residuals based on early experimentation. In our case, preliminary tests consistently pointed to:  \n",
    "  $$\n",
    "  \\text{SARIMA}(p,d,q)\\times(P,D,Q)_{s} = \\text{SARIMA}(1,0,0)\\times(1,1,1)_{24}\n",
    "  $$  \n",
    "  This structure was identified by first selecting simple but reasonable auxiliary regression variables, fitting a linear model, and analyzing the ACF and PACF plots of the residuals. These plots suggested a similar model, differing only by $Q = 0$. Subsequent cross-validation with minimal parameter adjustments confirmed the above configuration as a robust starting point. Additionally, running `AutoARIMA` on series F1 (with conservative starting values) repeatedly returned either the same model or slight variations, further supporting this selection.\n",
    "\n",
    "- **Step 2: Targeted Cross-Validation**  \n",
    "  Perform cross-validation over a limited set of SARIMA parameter combinations, gradually increasing model complexity to identify the best-performing configuration.\n",
    "\n",
    "- **Step 3 (Optional): AutoARIMA Refinement**  \n",
    "  Optionally, apply an AutoARIMA procedure initialized with the best parameters from Step 2 to explore nearby configurations. If improvements are found, repeat the cross-validation process.\n",
    "\n",
    "- **Step 4 (Optional): Expand Exogenous Variables**  \n",
    "  Optionally, repeat the entire process using additional exogenous variables beyond temperature to assess potential gains in predictive performance.\n",
    "\n",
    "In practice, however, neither Step 3 nor Step 4 led to significant improvements in our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b6b40",
   "metadata": {},
   "source": [
    "Below, we carry out **Step 2** of the tuning procedure described above. We consider 12 configurations of increasing complexity. For each unique ID and forecast horizon (either 1-day or 7-day ahead), every configuration was evaluated over 30 rolling windows for the day-ahead forecast and 20 windows for the week-ahead forecast, covering the extended winter period of 2023/2024. We then compared performance statistics across several error metrics (primarily MAE and RMSE) to identify the best candidate model for each ID and horizon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune = False  # Set to False to skip fine-tuning\n",
    "\n",
    "# === Do not edit below ===\n",
    " # Forecast horizon in hours (1 week). We only use this forecast horizon for fine-tuning\n",
    "            # because it allows as to consider less windows and since we are testing a very limited set of parameters\n",
    "            # in which we study the effectiveness of different variables in explaining the target,\n",
    "            # we don't expect to get meaningfully different results with shorter horizons\n",
    "\n",
    "# Note: by default we have:\n",
    "# - `tranform`='boxcox_cold';\n",
    "# - `lam_method`='loglik';\n",
    "# - `k_week_only_when_cold`=True;\n",
    "\n",
    "all_params = [\n",
    "    { # 0: Simplest parameters, should still work fairly well for all facilities\n",
    "        'k_week': 4,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1],\n",
    "        'drop_hourly_exog': True,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': False, \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 1: Basic parameters, only added hourly exogenous\n",
    "        'k_week': 4,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': False, \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 2: More Fourier terms\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': False,\n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 3: Added 4-day averages, separation of the raw exog using peak hours; removed some fourier terms\n",
    "        'k_week': 5,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 4: Added 1 lag of temperature\n",
    "        'k_week': 5,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 5: More lags of tempearture\n",
    "        'k_week': 5,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1, 2, 3],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 6: More fourier terms, heaviest configuration\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1, 2, 3],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 7: Back to the 5th configuration (SARIMAX_4), but with more fourier and without peak hours ad lag exog\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': False,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 8: Added peak hours separation\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 9: Added a lag of temperature, removed peak hours\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': False,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 10: Added peak hours\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    { # 11: Added an additional year of data in the training set\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24 * 2,\n",
    "    },\n",
    "    { # 12: Back to SARIMAX_10, but with more fourier terms\n",
    "        'k_week': 9,\n",
    "        'threshold': best_thresholds[id],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "]\n",
    "\n",
    "if finetune:\n",
    "    # ------------- Choose parameters for cross-validation -------------\n",
    "    # For get_cv_params:\n",
    "    step_size = 24  # 24h step size\n",
    "    max_n_fits = 15 # upper bound on number of fits\n",
    "    h = 168         # forecast horizon in hours (1 week)\n",
    "    start_test_cv = pd.to_datetime('2023-11-01')\n",
    "    end_test_cv = pd.to_datetime('2024-04-01')\n",
    "\n",
    "    metadata = {f'params_set_{i}': all_params[i] for i in range(len(all_params))}\n",
    "    metadata['for_get_cv_params'] = {\n",
    "        'step_size': step_size,\n",
    "        'max_n_fits': max_n_fits,\n",
    "        'forecast_horizon': h,\n",
    "        'start_test_cv': str(start_test_cv),\n",
    "        'end_test_cv': str(end_test_cv),\n",
    "    }\n",
    "\n",
    "    # ------------- Create directory for fine-tuning results -------------\n",
    "    timestamp = datetime.now(ZoneInfo(\"Europe/Rome\")).strftime(\"%Y%m%dT%H%M%S\")\n",
    "    run_id = f\"{id}_finetuning_sarimax_{timestamp}\"\n",
    "    path = BASE_DIR / \"results\" / \"finetuning\" / \"sarimax\" / run_id\n",
    "    metadata['run_id'] = run_id\n",
    "\n",
    "    try:\n",
    "        path.mkdir(parents=True, exist_ok=False)\n",
    "        logging.info(f\"Created directory for fine-tuning results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "        # ------------- Set windows with get_cv_params -------------  \n",
    "        cv_params = get_cv_params_v2(\n",
    "            start_test_cv=start_test_cv,\n",
    "            end_test_cv=end_test_cv,\n",
    "            step_size=step_size,\n",
    "            max_n_fits=max_n_fits,\n",
    "            horizon_hours=h,\n",
    "        )\n",
    "        metadata['cv_params'] = cv_params\n",
    "        end_test_cv = cv_params['end_test_actual']\n",
    "        n_windows = cv_params['n_windows']\n",
    "        test_hours = cv_params['test_hours']\n",
    "        refit = cv_params['refit']\n",
    "        n_fits = cv_params['n_fits']\n",
    "\n",
    "        # ------------- Run CV once with the naive model for comparison -------------\n",
    "        naive_model = SeasonalNaive(season_length=24, alias='Naive24h')\n",
    "        sf = StatsForecast(models=[naive_model], freq='h')\n",
    "        heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "        full_df = heat_id_df[heat_id_df['ds'] <= end_test_cv]  # Use all data up to the end of the last forecast horizon\n",
    "        t0 = pd.Timestamp.now()\n",
    "        cv_naive = sf.cross_validation(\n",
    "            h=h,\n",
    "            df=full_df,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,  \n",
    "            test_size=test_hours,  \n",
    "            input_size=None,  \n",
    "        )\n",
    "        t1 = pd.Timestamp.now()\n",
    "        elapsed = (t1 - t0).total_seconds()\n",
    "        params_placeholder = {key: None for key in all_params[0].keys()}\n",
    "        params_except_sarima = {k:v for k,v in params_placeholder.items() if k != 'sarima_kwargs'}\n",
    "        record = {\n",
    "            'name': 'Naive24h',\n",
    "            'avg_elapsed_per_fit_sec': elapsed/n_windows,  \n",
    "            **params_except_sarima,  # Don't need to retain the SARIMA parameters for records\n",
    "        }\n",
    "\n",
    "        # store results and corresponding params / name of the model / elapsed time\n",
    "        records = [record]  \n",
    "        cv_frames = [cv_naive]\n",
    "\n",
    "        # ------------- Run grid search for MSTL model -------------\n",
    "        aux_id_df = aux_df[aux_df['unique_id'] == id].copy()\n",
    "        t_start_cv = pd.Timestamp.now()\n",
    "        for i, params in enumerate(tqdm(all_params, desc=\"Grid search\", leave=True)):\n",
    "\n",
    "            sconfig = SARIMAXConfig(**params)\n",
    "            spipeline = SARIMAXPipeline(\n",
    "                target_df=heat_id_df,\n",
    "                aux_df=aux_id_df,\n",
    "                config=sconfig,\n",
    "            )\n",
    "            t0 = pd.Timestamp.now()\n",
    "            cv_sarimax = spipeline.cross_validation(\n",
    "                h=h,                   # Forecast horizon in hours\n",
    "                test_size=test_hours,  # Test size in hours\n",
    "                end_test=end_test_cv,  # End of the test period\n",
    "                step_size=step_size,   # Step size in hours\n",
    "                refit=refit,           # Whether to refit the model at each window\n",
    "                verbose=False,\n",
    "                alias=f'SARIMAX_{i}',  # Alias for the model\n",
    "                # NB: the input size is not used here, it should be specified in the SARIMAXConfig\n",
    "            )\n",
    "            t1 = pd.Timestamp.now()\n",
    "            elapsed = (t1 - t0).total_seconds()\n",
    "            cv_sarimax.drop(columns=['y'], inplace=True)\n",
    "            cv_frames.append(cv_sarimax)\n",
    "\n",
    "            params_except_sarima = {k:v for k,v in params.items() if k != 'sarima_kwargs'}\n",
    "            record = {\n",
    "                'name': f'SARIMAX_{i}',  \n",
    "                'avg_elapsed_per_fit_sec': elapsed/n_fits,\n",
    "                **params_except_sarima,  \n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "        # ------------- Combine all cross-validation results into a single DataFrame -------------\n",
    "        key_cols = [\"unique_id\", \"ds\", \"cutoff\"]\n",
    "        cv_results_df = reduce(\n",
    "            lambda left, right: pd.merge(left, right, on=key_cols, how=\"outer\", copy=False),\n",
    "            cv_frames            \n",
    "        )\n",
    "        records_df = pd.DataFrame(records)\n",
    "        results = (records_df, cv_results_df)\n",
    "\n",
    "        t_end_cv = pd.Timestamp.now()\n",
    "        elapsed_cv = str(t_end_cv - t_start_cv)\n",
    "        metadata['total_elapsed_time'] = elapsed_cv\n",
    "\n",
    "        # ------------- Final save -------------\n",
    "        records_df.to_parquet(path / \"records.parquet\", compression=\"snappy\")\n",
    "        cv_results_df.to_parquet(path / \"cv_results.parquet\", compression=\"snappy\")\n",
    "\n",
    "        metadata_path = path / 'metadata.yaml'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            safe_dump_yaml(\n",
    "                metadata,\n",
    "                f,\n",
    "                indent=4, # indentation for nested structures\n",
    "            )\n",
    "\n",
    "        logging.info(\"✓ All artifacts saved successfully, fine-tuning completed.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"✗ Interrupted; cleaning up.\")\n",
    "        remove_tree(path, require_within=BASE_DIR)\n",
    "        logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "        raise\n",
    "    except Exception:\n",
    "        logging.exception(\"✗ Error during test for id=%s; cleaning up.\", id)\n",
    "        remove_tree(path, require_within=BASE_DIR)\n",
    "        logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c546a4",
   "metadata": {},
   "source": [
    "Load a completed study from `<base>/results/finetuning/sarimax` and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a88072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: pick a run_id to analyze \n",
    "run_id = 'F5_finetuning_sarimax_20250828T120959'  \n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Load the metadata for the run_id ---\n",
    "path = BASE_DIR / \"results\" / \"finetuning\" / \"sarimax\" / run_id\n",
    "records_df = pd.read_parquet(path / \"records.parquet\")\n",
    "cv_results_df = pd.read_parquet(path / \"cv_results.parquet\")\n",
    "\n",
    "real_cutoffs = ['2023-10-31T23:00:00', '2023-11-10T23:00:00', '2023-11-20T23:00:00', '2023-11-30T23:00:00', '2023-12-10T23:00:00', \\\n",
    "                '2023-12-20T23:00:00', '2023-12-30T23:00:00', '2024-01-09T23:00:00', '2024-01-19T23:00:00', '2024-01-29T23:00:00', \\\n",
    "                '2024-02-08T23:00:00', '2024-02-18T23:00:00', '2024-02-28T23:00:00', '2024-03-09T23:00:00', '2024-03-19T23:00:00']\n",
    "real_cutoffs = pd.to_datetime(real_cutoffs)\n",
    "cv_results_df = cv_results_df[cv_results_df['cutoff'].isin(real_cutoffs)].copy()\n",
    "\n",
    "# --- Evaluate results ---\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"DataFrame is highly fragmented\",\n",
    "        category=pd.errors.PerformanceWarning,\n",
    "    )\n",
    "    summary, combined_results = custom_evaluate_cv(\n",
    "        cv_df=cv_results_df,\n",
    "        metrics=['mae', 'rmse', 'mase', 'nmae'],  # List of metrics to compute\n",
    "        target_df=heat_df,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c8154",
   "metadata": {},
   "source": [
    "View statistical summaries of evaluation metrics across windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbcb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display with optional sorting ---\n",
    "wide_summary = display_cv_summary(\n",
    "    summary,\n",
    "    sort_metric='mae',  \n",
    "    sort_stat='mean',   # Sort by the mean of the metric\n",
    "    ascending=True,     # Sort by ascending order of the metric\n",
    "    by_panel=True,\n",
    "    show_row_numbers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display the records of the first n models ---\n",
    "n = 50     # how many?  \n",
    "\n",
    "# === Do not edit below ===\n",
    "# Select the first n models based on the sorted summary snd the corresponding records\n",
    "s = wide_summary.reset_index()    \n",
    "models_chosen = s.iloc[:n, s.columns.get_loc(\"model\")].to_numpy().reshape(-1).tolist()\n",
    "model_order = {model: i for i, model in enumerate(models_chosen)}\n",
    "filtered_df = records_df.loc[records_df['name'].isin(models_chosen), :].copy()\n",
    "filtered_df['order'] = filtered_df['name'].map(model_order)\n",
    "ordered_df = filtered_df.sort_values('order').drop('order', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Display the ordered DataFrame with all rows and columns\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(ordered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b60f84",
   "metadata": {},
   "source": [
    "View statistic of differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose how many models to display, or pick them individually ---\n",
    "n_models = 20  # how many? (from the top of the sorted summary) \n",
    "models = None  # or specify a list of model names, e.g., ['SARIMAX_0', 'SARIMAX_3', 'Naive24h']\n",
    "\n",
    "baseline_model = 'SARIMAX_9'\n",
    "\n",
    "# === Do not edit this box ===\n",
    "def top_n_models(wide_summary, n, baseline): \n",
    "    all_models = list(wide_summary.index.get_level_values('model').unique())\n",
    "    top_models = all_models[:n_models]\n",
    "    return top_models\n",
    "\n",
    "def ensure_baseline_included(models, baseline_model):\n",
    "    all_models = list(wide_summary.index.get_level_values('model').unique())\n",
    "    if baseline_model not in all_models:\n",
    "        raise ValueError(f\"Baseline model '{baseline_model}' not found in the results.\")\n",
    "    if baseline_model not in models:\n",
    "        models.append(baseline_model)\n",
    "\n",
    "def filter_combined(combined_results, models):\n",
    "    cols = ['unique_id', 'metric', 'cutoff'] + models\n",
    "    filtered_combined = combined_results.loc[:, cols].copy()\n",
    "    return filtered_combined\n",
    "\n",
    "if models is None:\n",
    "    models = top_n_models(wide_summary, n_models, baseline_model)\n",
    "ensure_baseline_included(models, baseline_model)\n",
    "filtered_combined = filter_combined(combined_results, models)\n",
    "\n",
    "summary_ld, combined_ld = compute_loss_diff_stats(\n",
    "    combined_results=filtered_combined,\n",
    "    baseline_model=baseline_model\n",
    ")\n",
    "# ============================\n",
    "\n",
    "# --- Display the results ---\n",
    "wide_summary_ld = display_cv_summary(\n",
    "    summary_ld,\n",
    "    sort_metric='mae',  \n",
    "    sort_stat='mean',   # Sort by the mean of the metric\n",
    "    ascending=True,     # Sort by ascending order of the metric\n",
    "    by_panel=True,\n",
    "    show_row_numbers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose how many models to display, or pick them individually ---\n",
    "n_models = 3  # how many? (from the top of the sorted summary) \n",
    "models = ['SARIMAX_12']  # or specify a list of model names, e.g., ['SARIMAX_0', 'SARIMAX_3', 'Naive24h']\n",
    "\n",
    "baseline_model = 'SARIMAX_9'\n",
    "\n",
    "# === Do not edit this box ===\n",
    "if models is None:\n",
    "    models = top_n_models(wide_summary, n_models, baseline_model)\n",
    "ensure_baseline_included(models, baseline_model)\n",
    "filtered_combined = filter_combined(combined_results, models)\n",
    "summary_ld, combined_ld = compute_loss_diff_stats(\n",
    "    combined_results=filtered_combined,\n",
    "    baseline_model=baseline_model\n",
    ")\n",
    "models_diff = [f\"LD-{m}\" for m in models if m != baseline_model]\n",
    "# ============================\n",
    "\n",
    "# --- Plot the original models ---\n",
    "fig = plotly_cv_metric_by_cutoff(\n",
    "    combined_results=filtered_combined,\n",
    "    metric=\"mae\",\n",
    "    models=models,         \n",
    "    width=1000,          \n",
    ")\n",
    "display_scrollable(fig)  \n",
    "\n",
    "# --- Plot differences ---\n",
    "fig = plotly_cv_metric_by_cutoff(\n",
    "    combined_results=combined_ld,\n",
    "    metric=\"mae\",\n",
    "    models=models_diff,         \n",
    "    width=1000, \n",
    "    title_suffix=f\"Normalized by {baseline_model}\"          \n",
    ")\n",
    "display_scrollable(fig)  \n",
    "\n",
    "interactive_plot_cutoff_results(\n",
    "    target_df=heat_df,\n",
    "    cv_df=cv_results_df,\n",
    "    add_context=True,\n",
    "    figsize=(11, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281e48e",
   "metadata": {},
   "source": [
    "### Final optimal parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False  # Set to False to skip saving the parameters\n",
    "\n",
    "# === Do not edit below ===\n",
    "sarimax_optimal_params = {\n",
    "    'F1': { # SARIMAX_9 from run_id = 'F1_finetuning_sarimax_20250827T115146' \n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds['F1'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    'F2': { # SARIMAX_6 from run_id = 'F2_finetuning_sarimax_20250827T115253'\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds['F2'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1, 2, 3],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    }, \n",
    "    'F3': { # SARIMAX_7 from run_id = 'F3_finetuning_sarimax_20250827T153215'\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds['F3'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': False,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    'F4': { # SARIMAX_3 from run_id = 'F4_finetuning_sarimax_20250827T153251'\n",
    "        'k_week': 5,\n",
    "        'threshold': best_thresholds['F4'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    'F5': { # SARIMAX_12 from run_id = 'F5_finetuning_sarimax_20250828T120959'\n",
    "        'k_week': 9,\n",
    "        'threshold': best_thresholds['F5'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "}\n",
    "\n",
    "if save:\n",
    "    params_path = BASE_DIR / \"models\" / \"sarimax_optimal_params.yaml\"\n",
    "    with open(params_path, 'w') as f:\n",
    "        safe_dump_yaml(\n",
    "            sarimax_optimal_params,\n",
    "            f,\n",
    "            indent=4,                    # indentation for nested structures\n",
    "        )\n",
    "    logging.info(\"✓ Optimal parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a90e7",
   "metadata": {},
   "source": [
    "### AutoARIMA additional model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179b804",
   "metadata": {},
   "source": [
    "In this section, we apply Nixtla’s AutoARIMA routine to identify the model that minimizes the AICc criterion, using the optimal features determined in the previous tuning step. We then perform cross-validation with the newly selected model on the same windows as before, allowing a direct comparison of its predictive accuracy against the previously tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an id \n",
    "id = 'F1'\n",
    "\n",
    "# Set to True to run the search for optimal SARIMA parameters\n",
    "do_search = False\n",
    "\n",
    "# === Do not edit this box ===\n",
    "if do_search:\n",
    "    # Initialize configuration\n",
    "    optimal_params = sarimax_optimal_params[id]\n",
    "    sconfig = SARIMAXConfig(**optimal_params)\n",
    "\n",
    "    # Construct datasets\n",
    "    mask = (heat_df['unique_id'] == id)\n",
    "    heat_train_id_df = heat_df[mask].copy()\n",
    "    mask = (aux_df['unique_id'] == id)\n",
    "    aux_train_id_df = aux_df[mask].copy()\n",
    "\n",
    "    # Construct the SARIMAX pipeline using the best config\n",
    "    spipeline = SARIMAXPipeline(\n",
    "        target_df=heat_train_id_df,\n",
    "        aux_df=aux_train_id_df,\n",
    "        config=sconfig,\n",
    "    )\n",
    "    spipeline.prepare_data()\n",
    "    spipeline.describe_prepared_data()\n",
    "\n",
    "    # Create a directory for saving results\n",
    "    timestamp = datetime.now(ZoneInfo(\"Europe/Rome\")).strftime(\"%Y%m%dT%H%M%S\")\n",
    "    run_id = f\"{id}_search_opt_sarimax_{timestamp}\"\n",
    "    path = BASE_DIR / \"results\" / \"finetuning\" / \"sarimax\" / run_id\n",
    "    \n",
    "    try:\n",
    "        path.mkdir(parents=True, exist_ok=False)\n",
    "        logging.info(f\"Created directory for search results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "        t0 = pd.Timestamp.now()\n",
    "        end_train = pd.Timestamp('2023-06-30T23:00:00')\n",
    "        result = spipeline.search_optimal_sarima_model(end_train=end_train, n_models=94)\n",
    "        t1 = pd.Timestamp.now()\n",
    "        elapsed = (t1 - t0).total_seconds()\n",
    "        \n",
    "        result_path = path / 'result.yaml'\n",
    "        with open(result_path, 'w') as f:\n",
    "            safe_dump_yaml(\n",
    "                result,\n",
    "                f,\n",
    "                indent=4, \n",
    "            )\n",
    "\n",
    "        metadata = {\n",
    "            'params': optimal_params,\n",
    "            'end_train': end_train,\n",
    "            'run_id': run_id,\n",
    "            'avg_elapsed_sec': elapsed\n",
    "        }\n",
    "        metadata_path = path / 'metadata.yaml'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            safe_dump_yaml(\n",
    "                metadata,\n",
    "                f,\n",
    "                indent=4, \n",
    "            )\n",
    "\n",
    "        logging.info(\"✓ All artifacts saved successfully.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"✗ Interrupted; cleaning up.\")\n",
    "        remove_tree(path, require_within=BASE_DIR)\n",
    "        logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "        raise\n",
    "    except Exception:\n",
    "        logging.exception(\"✗ Error during test for id=%s; cleaning up.\", id)\n",
    "        remove_tree(path, require_within=BASE_DIR)\n",
    "        logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15b869",
   "metadata": {},
   "source": [
    "Perform cross-validation with the new models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a unique id\n",
    "id = \"F5\"\n",
    "do_cv = True\n",
    "\n",
    "best_sarima = {\n",
    "    'F1': {'order': (5, 0, 0), 'season_length': 24, 'seasonal_order': (1, 1, 1)},\n",
    "    'F2': {'order': (1, 0, 5), 'season_length': 24, 'seasonal_order': (2, 1, 2)},\n",
    "    'F3': {'order': (1, 1, 2), 'season_length': 24, 'seasonal_order': (0, 0, 2)},\n",
    "    'F4': {'order': (1, 0, 0), 'season_length': 24, 'seasonal_order': (1, 1, 1)},\n",
    "    'F5': {'order': (5, 1, 1), 'season_length': 24, 'seasonal_order': (2, 0, 0)},\n",
    "}\n",
    "\n",
    "if do_cv:\n",
    "    # ------------- Choose parameters for cross-validation -------------\n",
    "    # For get_cv_params:\n",
    "    n_windows = 15  \n",
    "    h = 168        \n",
    "    start_test_cv = pd.to_datetime('2023-11-01')\n",
    "    end_test_cv = pd.to_datetime('2024-04-01')\n",
    "\n",
    "    # ------------- Set windows with get_cv_params -------------  \n",
    "    cv_params = get_cv_params_v2(\n",
    "        start_test_cv=start_test_cv,\n",
    "        end_test_cv=end_test_cv,\n",
    "        n_windows=n_windows,\n",
    "        horizon_hours=h,\n",
    "    )\n",
    "    step_size = cv_params['step_size']\n",
    "    end_test_cv = cv_params['end_test_actual']\n",
    "    n_windows = cv_params['n_windows']\n",
    "    test_hours = cv_params['test_hours']\n",
    "\n",
    "    # ------------- Run CV loop -------------\n",
    "    # Filter dataset for id\n",
    "    heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "    full_df = heat_id_df[heat_id_df['ds'] <= end_test_cv]\n",
    "    aux_id_df = aux_df[aux_df['unique_id'] == id].copy()\n",
    "\n",
    "    optimal_params = sarimax_optimal_params[id]\n",
    "    optimal_params['sarima_kwargs'] = best_sarima[id]\n",
    "\n",
    "    sconfig = SARIMAXConfig(**optimal_params)\n",
    "    spipeline = SARIMAXPipeline(\n",
    "        target_df=heat_id_df,\n",
    "        aux_df=aux_id_df,\n",
    "        config=sconfig,\n",
    "    )\n",
    "    spipeline.prepare_data()\n",
    "    t0 = pd.Timestamp.now()\n",
    "    cv_sarimax_df = spipeline.cross_validation(\n",
    "        h=h,                   # Forecast horizon in hours\n",
    "        test_size=test_hours,  # Test size in hours\n",
    "        end_test=end_test_cv,  # End of the test period\n",
    "        step_size=step_size,   # Step size in hours\n",
    "        verbose=True,\n",
    "        alias='SARIMAX',  # Alias for the model\n",
    "        # NB: the input size is not set here, as it defaults to the one specified in SARIMAXConfig\n",
    "    )\n",
    "    t1 = pd.Timestamp.now()\n",
    "    elapsed = (t1 - t0).total_seconds()\n",
    "\n",
    "    params_except_sarima = {k:v for k,v in optimal_params.items() if k != 'sarima_kwargs'}\n",
    "    record = {\n",
    "        'name': f'SARIMAX_new',  \n",
    "        'avg_elapsed_per_fit_sec': elapsed/n_fits,\n",
    "        **params_except_sarima,  \n",
    "    }\n",
    "    record_df = pd.DataFrame([record])\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    summary, combined_results = custom_evaluate_cv(\n",
    "        cv_df=cv_sarimax_df,\n",
    "        metrics=['mae', 'rmse', 'mase', 'nmae'],  # List of metrics to compute\n",
    "        target_df=heat_df,  \n",
    "    )\n",
    "\n",
    "    # Display wide summary\n",
    "    wide_summary = display_cv_summary(\n",
    "        summary,\n",
    "        sort_metric='mae',  \n",
    "        sort_stat='mean',   # Sort by the mean of the metric\n",
    "        ascending=True,     # Sort by ascending order of the metric\n",
    "        by_panel=True,\n",
    "        show_row_numbers=True\n",
    "    )\n",
    "\n",
    "    # Display record\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        display(record_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae98feb",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "| model new/old | id | avg elapsed per fit (sec) | MAE mean | MAE std | MASE mean | MASE std | NMAE mean | NMAE std | RMSE mean | RMSE std |\n",
    "|---------------|----|----------------------|----------|---------|-----------|----------|-----------|----------|-----------|----------|\n",
    "| new           | F1 | 69.0                 | 33.84    | 8.14    | 1.47      | 0.37     | 9.00%     | 3.83%    | 43.37     | 9.87     |\n",
    "| old           | F1 | 60.1                 | 33.47    | 7.18    | 1.46      | 0.33     | 8.87%     | 3.58%    | 43.18     | 8.89     |\n",
    "| new           | F2 | 206.2                | 32.49    | 8.32    | 1.91      | 0.48     | 10.70%    | 4.51%    | 44.22     | 11.44    |\n",
    "| old           | F2 | 76.6                 | 31.39    | 7.55    | 1.85      | 0.43     | 10.25%    | 3.89%    | 42.94     | 10.16    |\n",
    "| new           | F3 | 55.3                 | 53.56    | 18.44   | 1.41      | 0.49     | 12.85%    | 3.99%    | 65.88     | 20.16    |\n",
    "| old           | F3 | 55.4                 | 45.08    | 14.44   | 1.19      | 0.39     | 11.16%    | 4.87%    | 56.20     | 17.42    |\n",
    "| new           | F5 | 56.7                 | 51.69    | 15.46   | 1.35      | 0.43     | 23.25%    | 8.35%    | 68.74     | 19.03    |\n",
    "| old           | F5 | 80.5                 | 49.79    | 20.87   | 1.30      | 0.57     | 22.57%    | 11.50%   | 64.25     | 23.92    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6e329",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f31e81",
   "metadata": {},
   "source": [
    "Final model testing is conducted using a cross-validation procedure focused on the cold season of 2024–2025. For the **weekly forecast horizon**, approximately **20 test windows** are used, while the **daily horizon** utilizes around **35 windows**. The number of windows for the weekly horizon is deliberately limited to minimize correlation between adjacent test periods.\n",
    "\n",
    "The exact number of windows, as well as the start and end dates of the test period, may vary slightly between time series. In each case, the test range and number of windows are chosen to:\n",
    "\n",
    "* start when heat demand begins to rise after the summer period and end as demand declines with the return of warmer weather;\n",
    "* use a step size of 9 for weekly forecasts and 5 for daily forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final parameters per facility ---\n",
    "save = False  # Set to False to skip saving the parameters\n",
    "\n",
    "sarimax_optimal_params = {\n",
    "    'F1': { # SARIMAX_9 from run_id = 'F1_finetuning_sarimax_20250827T115146' \n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds['F1'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    'F2': { # SARIMAX_6 from run_id = 'F2_finetuning_sarimax_20250827T115253'\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds['F2'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1, 2, 3],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    }, \n",
    "    'F3': { # SARIMAX_7 from run_id = 'F3_finetuning_sarimax_20250827T153215'\n",
    "        'k_week': 7,\n",
    "        'threshold': best_thresholds['F3'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': False,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    'F4': { # SARIMAX_3 from run_id = 'F4_finetuning_sarimax_20250827T153251'\n",
    "        'k_week': 5,\n",
    "        'threshold': best_thresholds['F4'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': None,\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "    'F5': { # SARIMAX_12 from run_id = 'F5_finetuning_sarimax_20250828T120959', with updated SARIMA orders\n",
    "        'k_week': 9,\n",
    "        'threshold': best_thresholds['F5'],\n",
    "        'days_averages': [1, 4],\n",
    "        'drop_hourly_exog': False,\n",
    "        'lags_exog': [1],\n",
    "        'use_peak_hours': True,  \n",
    "        'sarima_kwargs': {\n",
    "            'order': (1, 0, 0),\n",
    "            'season_length': 24,\n",
    "            'seasonal_order': (1, 1, 1)\n",
    "        },\n",
    "        'input_size': 365 * 24,\n",
    "    },\n",
    "}\n",
    "\n",
    "if save:\n",
    "    params_path = BASE_DIR / \"models\" / \"sarimax_optimal_params.yaml\"\n",
    "    with open(params_path, 'w') as f:\n",
    "        safe_dump_yaml(\n",
    "            sarimax_optimal_params,\n",
    "            f,\n",
    "            indent=4,                    # indentation for nested structures\n",
    "        )\n",
    "    logging.info(\"✓ Optimal parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = True  # Set to False to skip test\n",
    "\n",
    "# DONE:\n",
    "\n",
    "if do_test:\n",
    "    grid = list(product(['F1'], ['week']))\n",
    "    for id, horizon_type in tqdm(grid, desc=\"Test\", leave=True):\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        # ------------- Create directory for test results -------------\n",
    "        timestamp = datetime.now(ZoneInfo(\"Europe/Rome\")).strftime(\"%Y%m%dT%H%M%S\")\n",
    "        run_id = f\"{id}_{horizon_type}_test_sarimax_{timestamp}\"\n",
    "        path = BASE_DIR / \"results\" / \"test\" / \"sarimax\" / run_id\n",
    "        metadata['run_id'] = run_id\n",
    "\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "            logging.info(f\"Created directory for test results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "\n",
    "            # ------------- Set params for cv -------------\n",
    "            out = get_cv_params_for_test(horizon_type)\n",
    "            metadata['for_cv'] = {\n",
    "                'step_size': out['step_size'],\n",
    "                'test_hours': out['test_hours'],\n",
    "                'end_test_cv': str(out['end_test_actual']),\n",
    "                'n_windows': out['n_windows'],\n",
    "                'refit': out['refit'],\n",
    "            }\n",
    "            h = 24*7 if horizon_type == 'week' else 24  \n",
    "\n",
    "            # ------------- Run cross-validation with the optimal parameters -------------\n",
    "            optimal_params = sarimax_optimal_params[id]\n",
    "            metadata['optimal_params'] = optimal_params\n",
    "            sconfig = SARIMAXConfig(**optimal_params)\n",
    "\n",
    "            heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "            aux_id_df = aux_df[aux_df['unique_id'] == id].copy()\n",
    "            spipeline = SARIMAXPipeline(\n",
    "                target_df=heat_id_df,\n",
    "                aux_df=aux_id_df,\n",
    "                config=sconfig,\n",
    "            )\n",
    "\n",
    "            t0 = pd.Timestamp.now()\n",
    "            cv_df = spipeline.cross_validation(\n",
    "                h=h, \n",
    "                test_size=out['test_hours'],      # Test size in hours\n",
    "                end_test=out['end_test_actual'],  # End of the test period\n",
    "                step_size=out['step_size'],       # Step size in hours\n",
    "                input_size=None,                  # inferred from sarimax_optimal_params[id]\n",
    "                refit=out['refit'],               # Whether to refit the model at each window\n",
    "                verbose=True,\n",
    "            )\n",
    "            t1 = pd.Timestamp.now()\n",
    "\n",
    "            avg_elapsed = (t1 - t0).total_seconds() / out['n_fits']\n",
    "            metadata['avg_el_per_fit'] = avg_elapsed\n",
    "\n",
    "            cv_df.to_parquet(path / \"cv_df.parquet\", compression=\"snappy\")\n",
    "\n",
    "            metadata_path = path / 'metadata.yaml'\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                safe_dump_yaml(\n",
    "                    metadata,\n",
    "                    f,\n",
    "                    indent=4, \n",
    "                )\n",
    "\n",
    "            logging.info(f\"✓ Artifacts saved successfully for id={id}, horizon={horizon_type}.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(\"✗ Interrupted; cleaning up.\")\n",
    "            remove_tree(path, require_within=BASE_DIR)\n",
    "            logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "            raise\n",
    "        except Exception:\n",
    "            logging.exception(\"✗ Error during test for id=%s, horizon=%s; cleaning up.\", id, horizon_type)\n",
    "            remove_tree(path, require_within=BASE_DIR)\n",
    "            logging.info(\"✓ Removed %s\", Path(path).relative_to(BASE_DIR) if BASE_DIR in Path(path).resolve().parents else path)\n",
    "            raise\n",
    "\n",
    "    logging.info(f\"✓ Test completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c797928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a run_id to analyze\n",
    "run_id = \"F1_week_test_sarimax_20251006T201817\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# --- Load data ---\n",
    "path = BASE_DIR / \"results\" / \"test\" / \"sarimax\" / run_id\n",
    "cv_df = pd.read_parquet(path / \"cv_df.parquet\")\n",
    "with open(path / \"metadata.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = yaml.safe_load(f) \n",
    "\n",
    "# --- check cv_df ---\n",
    "from heat_forecast.utils.cv_utils import sanity_cv_df\n",
    "_ = sanity_cv_df(cv_df, metadata, positive_forecasts=True)\n",
    "\n",
    "interactive_plot_cutoff_results(\n",
    "    target_df=heat_df,\n",
    "    cv_df=cv_df,\n",
    "    add_context=True,\n",
    "    figsize=(11, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce664949",
   "metadata": {},
   "source": [
    "## Eample of fit and forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fafac0",
   "metadata": {},
   "source": [
    "For completeness, here is an example of a simple fit + forecast with the SARIMAX custom pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an id\n",
    "id = 'F1'\n",
    "\n",
    "# Initialize configuration for the desired data preparation\n",
    "sconfig = SARIMAXConfig(**sarimax_optimal_params[id])\n",
    "\n",
    "# Create the SARIMAX pipeline with the specified configuration and generate the data\n",
    "heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "aux_id_df = aux_df[aux_df['unique_id'] == id].copy()\n",
    "spipeline = SARIMAXPipeline(\n",
    "    target_df=heat_id_df,\n",
    "    config=sconfig,\n",
    "    aux_df=aux_id_df,\n",
    ")\n",
    "spipeline.prepare_data()\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "start_train = pd.Timestamp('2023-01-06 23:00:00')\n",
    "end_train = pd.Timestamp('2024-01-06 23:00:00')\n",
    "spipeline.fit(\n",
    "    end_train=end_train,\n",
    ")\n",
    "\n",
    "# Generate forecasts and plot the results\n",
    "forecasts = spipeline.predict(h=24*7)\n",
    "custom_plot_results(\n",
    "    target_df=heat_id_df,\n",
    "    forecast_df=forecasts,\n",
    "    start_offset=24*7,\n",
    "    end_offset=24*2,\n",
    "    with_naive=True,\n",
    "    target_train_df=heat_id_df[heat_id_df['ds'] <= end_train],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da64c8",
   "metadata": {},
   "source": [
    "## Eample of cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25746432",
   "metadata": {},
   "source": [
    "For completeness, here is an example of a simple fit + forecast with the SARIMAX custom pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ab0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cv = True  # Set to False to skip cross-validation\n",
    "\n",
    "# Pick an id\n",
    "id = 'F1'\n",
    "\n",
    "if do_cv:\n",
    "    # Filter dataset for the chosen id\n",
    "    heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "    aux_id_df = aux_df[aux_df['unique_id'] == id].copy()\n",
    "\n",
    "    # Run get_cv_params to set the parameters for cross-validation\n",
    "    n_windows = 15  \n",
    "    h = 168        \n",
    "    start_test_cv = pd.to_datetime('2023-11-01')\n",
    "    end_test_cv = pd.to_datetime('2024-04-01')\n",
    "    cv_params = get_cv_params_v2(\n",
    "        start_test_cv=start_test_cv,\n",
    "        end_test_cv=end_test_cv,\n",
    "        n_windows=n_windows,\n",
    "        horizon_hours=h,\n",
    "    )\n",
    "\n",
    "    # Run the cross-validation with the SeasonalNaive model\n",
    "    naive_model = SeasonalNaive(season_length=24, alias='Naive24h')\n",
    "    sf = StatsForecast(models=[naive_model], freq='h')\n",
    "    heat_id_df = heat_df[heat_df['unique_id'] == id].copy()\n",
    "    t0 = pd.Timestamp.now()\n",
    "    cv_naive = sf.cross_validation(\n",
    "        h=h,\n",
    "        df=heat_id_df[heat_id_df['ds'] <= end_test_cv], # Use all data up to the end of the last forecast horizon\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,  \n",
    "        test_size=test_hours,  \n",
    "        input_size=None,  \n",
    "        refit=True,  \n",
    "    )\n",
    "\n",
    "    # Run the cross-validation with the SARIMAX model\n",
    "    params = sarimax_optimal_params[id]\n",
    "    sconfig = SARIMAXConfig(**params)\n",
    "    spipeline = SARIMAXPipeline(\n",
    "        target_df=heat_id_df,\n",
    "        aux_df=aux_id_df,\n",
    "        config=sconfig,\n",
    "    )\n",
    "    logging.info(f\"Running cross-validation for SARIMAX...\")\n",
    "    t0 = pd.Timestamp.now()\n",
    "    spipeline.prepare_data()\n",
    "    cv_sarimax = spipeline.cross_validation(\n",
    "        h=h,                   # Forecast horizon in hours\n",
    "        test_size=test_hours,  # Test size in hours\n",
    "        end_test=end_test_cv,  # End of the test period\n",
    "        step_size=step_size,   # Step size in hours\n",
    "        verbose=True,\n",
    "        alias=f'SARIMAX',  # Alias for the model\n",
    "        # NB: the input size is not used here, it should be specified in the SARIMAXConfig\n",
    "    )\n",
    "    t1 = pd.Timestamp.now()\n",
    "    elapsed = (t1 - t0).total_seconds()\n",
    "    logging.info(f\"✓ Cross-validation completed.\")\n",
    "    logging.info(f\"Elapsed time: {elapsed:.2f} seconds\")\n",
    "    cv_sarimax.drop(columns=['y'], inplace=True)\n",
    "\n",
    "    # Combine the results of the cross-validation\n",
    "    key_cols = [\"unique_id\", \"ds\", \"cutoff\"]\n",
    "    cv_df = pd.merge(\n",
    "        cv_naive, cv_sarimax, on=key_cols, how=\"outer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b443785",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_cv:\n",
    "    # Compute evaluation metrics and display the summary\n",
    "    summary, combined_results = custom_evaluate_cv(\n",
    "        cv_df=cv_df,\n",
    "        metrics=['mae', 'rmse', 'mase', 'nmae'],  # List of metrics to compute\n",
    "        target_df=heat_df,  # Training data mase\n",
    "    )\n",
    "    _ = display_cv_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_cv:\n",
    "    # Plot the cross-validation metrics by cutoff\n",
    "    fig = plotly_cv_metric_by_cutoff(\n",
    "        combined_results=combined_results,\n",
    "        metric='mae',  # Change to 'rmse' or 'me' for other metrics\n",
    "        figsize=(10,6)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
