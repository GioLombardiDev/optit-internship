{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afff0f77",
   "metadata": {},
   "source": [
    "# TFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41711b0e",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e78920",
   "metadata": {},
   "source": [
    "You can run the notebook in two ways:\n",
    "\n",
    "1. **Google Colab**: place the project folder `heat-forecast` in **MyDrive**. The setup cell below will mount Drive and automatically add `MyDrive/heat-forecast/src` to `sys.path` so `import heat_forecast` works out of the box.\n",
    "\n",
    "2. **Local machine**:\n",
    "\n",
    "   * **Installing our package:** from the project root, run `pip install -e .` once (editable install). Then you can open the notebook anywhere and import the package normally.\n",
    "   * **Alternative:** if you’re running the notebook from `.../heat-forecast/notebooks/` without installing the package, the setup cell will detect `../src` and automatically add it to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419528c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Detect if running on Google Colab & Set base dir ---\n",
    "# %cd /home/giovanni.lombardi/heat-forecast/notebooks\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Install required packages only if not already installed\n",
    "def pip_install(pkg: str):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Set base directory and handle environment\n",
    "if in_colab():\n",
    "    # Make sure IPython is modern (avoids the old %autoreload/imp issue if you ever use it)\n",
    "    pip_install(\"ipython>=8.25\")\n",
    "    pip_install(\"ipykernel>=6.29\")\n",
    "    \n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    for pkg in [\"statsmodels\", \"statsforecast\", \"mlforecast\"]:\n",
    "        pip_install(pkg)\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set base directory to your Drive project folder\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/heat-forecast')\n",
    "\n",
    "    # Add `src/` to sys.path for custom package imports\n",
    "    SRC_PATH = BASE_DIR / 'src'\n",
    "    if str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "\n",
    "    # Sanity checks (helpful error messages if path is wrong)\n",
    "    assert SRC_PATH.exists(), f\"Expected '{SRC_PATH}' to exist. Fix BASE_DIR.\"\n",
    "    pkg_dir = SRC_PATH / \"heat_forecast\"\n",
    "    assert pkg_dir.exists(), f\"Expected '{pkg_dir}' package directory.\"\n",
    "    init_file = pkg_dir / \"__init__.py\"\n",
    "    assert init_file.exists(), f\"Missing '{init_file}'. Add it so Python treats this as a package.\"\n",
    "\n",
    "else:\n",
    "    # Local: either rely on editable install (pip install -e .) or add src/ when running from repo\n",
    "    # Assume notebook lives in PROJECT_ROOT/notebooks/\n",
    "    BASE_DIR = Path.cwd().resolve().parent\n",
    "    SRC_PATH = BASE_DIR / \"src\"\n",
    "\n",
    "    added_src = False\n",
    "    if (SRC_PATH / \"heat_forecast\").exists() and str(SRC_PATH) not in sys.path:\n",
    "        sys.path.append(str(SRC_PATH))\n",
    "        added_src = True\n",
    "\n",
    "# --- Logging setup ---\n",
    "import logging\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR  = (BASE_DIR / \"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / \"run.log\"\n",
    "PREV_LOG = LOG_DIR / \"run.prev.log\"\n",
    "\n",
    "# If there's a previous run.log with content, archive it to run.prev.log\n",
    "if LOG_FILE.exists() and LOG_FILE.stat().st_size > 0:\n",
    "    try:\n",
    "        # Replace old run.prev.log if present\n",
    "        if PREV_LOG.exists():\n",
    "            PREV_LOG.unlink()\n",
    "        LOG_FILE.rename(PREV_LOG)\n",
    "    except Exception as e:\n",
    "        # Fall back to truncating if rename fails (e.g., file locked)\n",
    "        print(f\"[warn] Could not archive previous log: {e}. Truncating current run.log.\")\n",
    "        LOG_FILE.write_text(\"\")\n",
    "\n",
    "# Configure logging: fresh file for this run + echo to notebook/stdout\n",
    "file_handler   = logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\")\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "                        datefmt=\"%m-%d %H:%M:%S\")\n",
    "file_handler.setFormatter(fmt)\n",
    "stream_handler.setFormatter(fmt)\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.handlers[:] = [file_handler, stream_handler]  # replace handlers (important in notebooks)\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "# Use Rome time\n",
    "logging.Formatter.converter = lambda *args: datetime.now(ZoneInfo(\"Europe/Rome\")).timetuple()\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "logging.info(\"=== Logging started (fresh current run) ===\")\n",
    "logging.info(\"Previous run (if any): %s\", PREV_LOG if PREV_LOG.exists() else \"none\")\n",
    "\n",
    "if added_src:\n",
    "    logging.info(\"heat_forecast not installed; added src/ to sys.path\")\n",
    "else:\n",
    "    logging.info(\"heat_forecast imported without modifying sys.path (likely installed)\")\n",
    "\n",
    "OPTUNA_DIR = BASE_DIR / \"results\" / \"finetuning\" / \"tft\"\n",
    "OPTUNA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(\"BASE_DIR (make sure it's '*/heat-forecast/', else cd and re-run): %s\", BASE_DIR)\n",
    "logging.info(\"LOG_DIR: %s\", LOG_DIR)\n",
    "logging.info(\"OPTUNA_DIR: %s\", OPTUNA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da68407",
   "metadata": {},
   "source": [
    "Ensure [compatibility with Numba](https://numba.readthedocs.io/en/stable/user/installing.html#numba-support-info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af26523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, numpy, numba\n",
    "logging.info(\"=== Current Environment ===\")\n",
    "logging.info(\"Python : %s\", sys.version.split()[0])\n",
    "logging.info(\"NumPy  : %s\", numpy.__version__)\n",
    "logging.info(\"Numba  : %s\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9851600",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7098e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Magic Commands ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "os.environ[\"OPTUNA_LOGGING_DISABLE_DEFAULT_HANDLER\"] = \"1\" # prevent Optuna from attaching its handler\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import torch\n",
    "import optuna\n",
    "import copy\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "import yaml\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Plotting Configuration ---\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['axes.grid.which'] = 'both'\n",
    "\n",
    "# --- Helper to detect error cause by keyboard interrupt ---\n",
    "# Pytorch Lightning sometimes wraps KeyboardInterrupt in another exception\n",
    "def is_keyboard_interrupt_like(exc: BaseException) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if exc is a KeyboardInterrupt or was caused by one\n",
    "    (e.g. NameError raised while handling KeyboardInterrupt).\n",
    "    \"\"\"\n",
    "    cur = exc\n",
    "    visited = set()\n",
    "    while cur is not None and cur not in visited:\n",
    "        visited.add(cur)\n",
    "        if isinstance(cur, KeyboardInterrupt):\n",
    "            return True\n",
    "        # check both context and cause\n",
    "        cur = cur.__context__ or cur.__cause__\n",
    "    return False\n",
    "\n",
    "# --- YAML Customization ---\n",
    "from heat_forecast.utils.yaml import safe_dump_yaml\n",
    "\n",
    "# --- Safe File Deletion Helper ---\n",
    "from heat_forecast.utils.fileshandling import remove_tree\n",
    "\n",
    "# --- Project-Specific Imports ---\n",
    "from heat_forecast.utils.cv_utils import get_cv_params_for_test\n",
    "from heat_forecast.pipeline.tft import (\n",
    "    TFTModelConfig, DataConfig, FeatureConfig, NormalizeConfig, TFTRunConfig, TFTPipeline,\n",
    "    TrainConfig\n",
    ")\n",
    "\n",
    "from heat_forecast.utils.optuna import (\n",
    "    OptunaStudyConfig, run_study, continue_study, describe_suggester, rename_study, clone_filtered_study\n",
    ")\n",
    "\n",
    "from heat_forecast.utils.plotting import plotly_cutoffs_with_exog\n",
    "\n",
    "logging.info(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83274db",
   "metadata": {},
   "source": [
    "Import pre-elaborated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'heat.csv'\n",
    "aux_path = BASE_DIR / 'data' / 'timeseries_preprocessed' / 'auxiliary.csv'\n",
    "heat_df = pd.read_csv(heat_path, parse_dates=['ds'])\n",
    "aux_df = pd.read_csv(aux_path, parse_dates=['ds'])\n",
    "logging.info(\"Loaded heat data: %s\", heat_path.relative_to(BASE_DIR))\n",
    "logging.info(\"Loaded auxiliary data: %s\", aux_path.relative_to(BASE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be409cf",
   "metadata": {},
   "source": [
    "Set device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2724af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ecb9e",
   "metadata": {},
   "source": [
    "Set a seed (set to None to skip):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "logging.info(f\"Using seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a777d",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3df5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'F1'\n",
    "config = TFTRunConfig(\n",
    "    model=TFTModelConfig(\n",
    "        hidden_size=64, \n",
    "        dropout=0.3, \n",
    "        input_chunk_length=168, output_chunk_length=168,\n",
    "        torch_device_str=\"auto\"\n",
    "    ),\n",
    "    data=DataConfig(stride=1), # high stride to make training faster for this example\n",
    "    train=TrainConfig(\n",
    "        n_epochs=25,\n",
    "        es_rel_min_delta=0.1, es_warmup_epochs=5,\n",
    "    ),\n",
    "    features=FeatureConfig(),\n",
    "    norm=NormalizeConfig(),\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "tft_pipe = TFTPipeline(\n",
    "    config=config, \n",
    "    target_df=heat_df[heat_df['unique_id'] == id], \n",
    "    aux_df=aux_df[aux_df['unique_id'] == id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_pipe.generate_vars()\n",
    "_ = tft_pipe.describe_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tft_pipe.describe_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e90565",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = None\n",
    "end_train = pd.Timestamp(\"2023-10-31 23:00\")  \n",
    "end_val = pd.Timestamp(\"2024-04-01 23:00\")\n",
    "out = tft_pipe.fit(end_train=end_train, end_val=end_val, start_train=start_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d86e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = tft_pipe.predict_many(n=168, start=end_train+pd.Timedelta(hours=1), end=end_val, stride_hours=168)\n",
    "plotly_cutoffs_with_exog(\n",
    "    target_df=heat_df[heat_df['unique_id'] == id],\n",
    "    cv_df=preds_df,\n",
    "    aux_df=aux_df[aux_df['unique_id'] == id],\n",
    "    start_offset=24*3,\n",
    "    end_offset=24*3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd792e6",
   "metadata": {},
   "source": [
    "## Optuna tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80e43",
   "metadata": {},
   "source": [
    "We tune the model using **Optuna**, a Python framework for hyperparameter optimization that supports both **grid search** and more efficient, adaptive methods. With Optuna we can easily run evaluations over a fixed hyperparameter grid, or use its **TPE (Tree-structured Parzen Estimator)** sampler to explore continuous, large and / or conditional search spaces in a data-driven way. The TPE sampler fits separate probabilistic models to \"good\" and \"bad\" parameter sets and selects new trials that maximize the ratio of the two (i.e. promising configurations).\n",
    "\n",
    "For more details on TPE and Optuna, see:\n",
    "\n",
    "* [Optuna main site](https://optuna.org/)\n",
    "* [TPESampler documentation](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) \n",
    "* [This article, for a deeper understanding of the TPESampler](https://arxiv.org/abs/2304.11127)\n",
    "\n",
    "Overview of the following subsections:\n",
    "* **\"Start a new study\"**: how to create and run a new optimization study from scratch.\n",
    "* **\"Review past studies\"**: how to load a completed Optuna study in order to delete, copy, continue, or inspect its results in detail.\n",
    "\n",
    "A detailed review and commentary of the tuning results is provided in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680fb2a",
   "metadata": {},
   "source": [
    "### Start a new study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f4e7e",
   "metadata": {},
   "source": [
    "As a first step to start a study, select the time series (`unique_id`) you want to optimize. Next, choose a **suggester function** (or define a new one in `heat_forecast/suggesters/lstm.py`) that specifies the hyperparameter search space. Finally, configure the Optuna study by specifying the sampler, pruner, study name, optimization objective, and other parameters that control the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6822f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Select series ---\n",
    "unique_id = 'F1'\n",
    "\n",
    "# --- Step 2: Define search space ---\n",
    "suggester_name = \"tft_v4_F1\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# Set path based on unique_id\n",
    "db_path = OPTUNA_DIR / f\"optuna_{unique_id}.db\"  # single DB file for each id\n",
    "storage_url = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "# Describe suggester\n",
    "desc = describe_suggester(suggester_name)\n",
    "logging.info(f\"Suggester used:\\n{desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ceb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Configure study ---\n",
    "optuna_cfg = OptunaStudyConfig(\n",
    "    # General\n",
    "    study_name='study_v4_F1',\n",
    "    objective=\"avg_near_best\", # \"best\" or \"last\" (based on val metric)\n",
    "    n_trials=None,               # number of trials, use None for grid search\n",
    "    timeout=None,              # timeout for the study (max time per trial in seconds)\n",
    "    seed=SEED,                 # seed for reproducibility of the optuna sampler\n",
    "    storage=storage_url,       # storage URL for the study\n",
    "    pruner=\"nop\",              # type of pruner: \"percentile\", \"median\", \"nop\"\n",
    "    sampler=\"grid\",             # type of sampler: \"tpe\", \"grid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507489ca",
   "metadata": {},
   "source": [
    "Run the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Run the study ---\n",
    "do_run = True\n",
    "\n",
    "# === Do not edit below ===\n",
    "if do_run:\n",
    "    # Set path based on unique_id\n",
    "    db_path = OPTUNA_DIR / f\"optuna_{unique_id}.db\"  # single DB file for each id\n",
    "    storage_url = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "    # Set base configuration of the pipeline\n",
    "    base_cfg = TFTRunConfig(\n",
    "        model = TFTModelConfig(),\n",
    "        data = DataConfig(),\n",
    "        features = FeatureConfig(),\n",
    "        train = TrainConfig(),\n",
    "        norm = NormalizeConfig(),\n",
    "        seed = SEED,\n",
    "    )\n",
    "\n",
    "    start_train = pd.Timestamp(\"2019-09-30 23:00\")  \n",
    "    start_val = None # -> pd.Timestamp(\"2023-11-01 00:00\") \n",
    "    end_train = pd.Timestamp(\"2023-10-31 23:00\")  \n",
    "    end_val = pd.Timestamp(\"2024-04-01 23:00\")\n",
    "\n",
    "    optuna_cfg.storage = storage_url\n",
    "\n",
    "    study = run_study(\n",
    "        unique_id,\n",
    "        heat_df, \n",
    "        aux_df, \n",
    "        base_cfg,\n",
    "        start_train=start_train, end_train=end_train, \n",
    "        start_val=start_val, end_val=end_val,\n",
    "        optuna_cfg=optuna_cfg,\n",
    "        suggest_config_name=suggester_name,\n",
    "    )\n",
    "    print(\"Best value (val loss):\", study.best_value)\n",
    "    print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab133c48",
   "metadata": {},
   "source": [
    "### Review past studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e9f56",
   "metadata": {},
   "source": [
    "In this section, we load an existing study and choose whether to continue, delete, rename, copy, or inspect it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276d060",
   "metadata": {},
   "source": [
    "#### Load and/or modify a study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af955d8",
   "metadata": {},
   "source": [
    "See available studies for a fixed ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose unique_id\n",
    "unique_id = 'F1'\n",
    "\n",
    "# === Do not edit below ===\n",
    "# Set path based on unique_id\n",
    "db_path = OPTUNA_DIR / f\"optuna_{unique_id}.db\"  # single DB file for each id\n",
    "storage_url = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "# Get all study summaries\n",
    "study_summaries = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "# Print study summaries\n",
    "study_summaries = sorted(study_summaries, key=lambda s: s.study_name)\n",
    "lines = [f\"Study name: {s.study_name}, trials: {s.n_trials}\" for s in study_summaries]\n",
    "logging.info(\"Available studies:\\n\\n\" + \"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8045e1",
   "metadata": {},
   "source": [
    "Choose a study to continue/delete/review by selecting its name below. Then view a description of the search space used for that study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"study_v4_F1\"\n",
    "\n",
    "# === Do not edit below ===\n",
    "# View detailed description of the search space for the study (the suggester documentation)\n",
    "study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "desc = describe_suggester(study.user_attrs.get(\"suggest_config_name\", \"\"))\n",
    "parent_study = study.user_attrs.get(\"_parent_study\", \"\")\n",
    "txt = f\"parent study ({parent_study}):\" if parent_study else 'this study:'\n",
    "logging.info(f\"Suggester used in {txt} \\n{desc}\\n\")\n",
    "if parent_study:\n",
    "    filter = yaml.dump(study.user_attrs.get('_substudy_filter', ''), indent=4, sort_keys=False)\n",
    "    logging.info(f\"This substudy was obtained though the filter: \\n{filter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b36d2",
   "metadata": {},
   "source": [
    "Optionally continue the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_continue = True\n",
    "\n",
    "# Choose how many trials to add\n",
    "n_new_trials = 15\n",
    "\n",
    "# Choose combos to enqueue (or None to skip)\n",
    "# e.g. [{\"model.hidden_size\": 50, \"model.num_layers\": 2}, {\"model.hidden_size\": 80, \"model.num_layers\": 3}]\n",
    "combos_to_enqueue = None\n",
    "trials_per_combo = 0  # how many times to repeat each combo (default is 1)\n",
    "\n",
    "# === Do not edit below ===\n",
    "if do_continue:\n",
    "    # Continue the loaded study with additional trials\n",
    "    study = continue_study(\n",
    "        study_name,\n",
    "        storage_url,\n",
    "        n_new_trials=n_new_trials,\n",
    "        target_df=heat_df,\n",
    "        aux_df=aux_df,\n",
    "        combos_to_enqueue=combos_to_enqueue,\n",
    "        trials_per_combo=trials_per_combo,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071e02d",
   "metadata": {},
   "source": [
    "Optionally create a sub-study selecting trials based on a condition on a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa461881",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_create = False\n",
    "\n",
    "if do_create:\n",
    "    study = create_substudy_by_param(\n",
    "        storage_url=storage_url,\n",
    "        src_study_name=study_name,\n",
    "        dst_study_name=\"substudy_lags_24_168_preliminary_v3_F1\",\n",
    "        param_name=\"features.lags_key\",\n",
    "        equals=\"7days_1day\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e638d4",
   "metadata": {},
   "source": [
    "Optionally delete the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f315863",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_delete = False\n",
    "\n",
    "if do_delete:\n",
    "    optuna.delete_study(\n",
    "        study_name=\"study_v4_F1\", # change to `study_name` if you are really sure you want to proceed \n",
    "        storage=storage_url,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabe956",
   "metadata": {},
   "source": [
    "Optionally rename the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_rename = False\n",
    "\n",
    "if do_rename:\n",
    "    study = rename_study(\n",
    "        old_name=\"\",\n",
    "        new_name=\"\",\n",
    "        storage_url=storage_url,\n",
    "        keep_old=False,\n",
    "        dry_run=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c8b42",
   "metadata": {},
   "source": [
    "#### View study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b84290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heat_forecast.utils.optuna import (\n",
    "    trials_df, trials_df_for_display, summarize_params_coverage, \n",
    "    plot_intermediate_values, plot_optimization_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aeed29",
   "metadata": {},
   "source": [
    "View best trials in the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of all trials\n",
    "df, val_name = trials_df(study)\n",
    "\n",
    "# Filter if needed\n",
    "#df = df[df['params_model.num_layers'] == 2]\n",
    "#df = df[df['params_model.hidden_size'] == 64]\n",
    "\n",
    "# === Do not edit below ===\n",
    "logging.info(f\"Trials total={len(df)}, complete={sum(df['state']=='COMPLETE')}, pruned={sum(df['state']=='PRUNED')}, \" \\\n",
    "             f\"fail={sum(df['state']=='FAIL')}, running={sum(df['state']=='RUNNING')}, waiting={sum(df['state']=='WAITING')}\")\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.max_rows\", None):\n",
    "    display(trials_df_for_display(df, val_name).head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145cda4",
   "metadata": {},
   "source": [
    "View optimization history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39930dd1",
   "metadata": {},
   "source": [
    "View the intermediate validation losses for each trial. The function also supports filtering curves based on specific trial parameters or attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_values(\n",
    "    study, \n",
    "    # --- Apply custom filtering here if needed ---\n",
    "    #include_params={'features.lags_key': 'none'}, \n",
    "    #predicate=lambda t: t.params.get(\"train.learning_rate\") > 5e-4 and t.params.get(\"model.dropout\") < 0.1,\n",
    "    dim_excluded=True,\n",
    "    dim_factor=0.1,\n",
    "    semilogy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ddc68",
   "metadata": {},
   "source": [
    "Analysis of coverage of the parameters space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "num_sty, cat_sty = summarize_params_coverage(study, df, val_name)\n",
    "\n",
    "with pd.option_context(\"display.float_format\", lambda v: f\"{v:,.4f}\"):\n",
    "    if num_sty:\n",
    "        logging.info(\"Coverage summary of numeric parameters:\")\n",
    "        display(num_sty)\n",
    "    else:\n",
    "        logging.info(\"No numeric parameters found for the study.\")\n",
    "    if cat_sty:\n",
    "        logging.info(\"Coverage summary of categorical parameters:\")\n",
    "        display(cat_sty)\n",
    "    else:\n",
    "        logging.info(\"No categorical parameters found for the study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bcf78f",
   "metadata": {},
   "source": [
    "Counts of best epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6607301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# === Do not edit below ===\n",
    "best_epochs = [\n",
    "    t.user_attrs[\"best_epoch\"]\n",
    "    for t in study.trials\n",
    "    if t.state == optuna.trial.TrialState.COMPLETE and \"best_epoch\" in t.user_attrs\n",
    "]\n",
    "\n",
    "s = pd.Series(pd.to_numeric(best_epochs, errors=\"coerce\")).dropna().astype(int)\n",
    "if not s.any():\n",
    "    logging.info(\"No best_epoch found.\")\n",
    "else:\n",
    "    # count each integer and include missing integers with count=0\n",
    "    lo, hi = int(s.min()), int(s.max())\n",
    "    counts = s.value_counts().sort_index()\n",
    "    counts = counts.reindex(range(lo, hi + 1), fill_value=0)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(9, 3.5), constrained_layout=True)\n",
    "    ax.bar(counts.index, counts.values, width=0.8)\n",
    "    ax.set_xlabel(\"Best epoch\")\n",
    "    ax.set_ylabel(\"Count of trials\")\n",
    "    ax.set_title(f\"Best epoch counts\")\n",
    "    ax.set_xticks(counts.index)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))  # integer ticks only\n",
    "\n",
    "    # log best\n",
    "    logging.info(f\"Best epoch by median: {s.median() :.0f}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af097ed",
   "metadata": {},
   "source": [
    "#### View results marginalized on single hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a02de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heat_forecast.utils.optuna import (\n",
    "    plot_marginals_1d, plot_param_importances, display_marginals_1d\n",
    ")\n",
    "import optuna.importance as imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c405d",
   "metadata": {},
   "source": [
    "View fANOVA importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do not edit below ===\n",
    "# get importances\n",
    "imps = imp.get_param_importances(study)\n",
    "imps = pd.Series(imps, dtype=float).sort_values(ascending=False)\n",
    "\n",
    "# plot\n",
    "if imps.any():\n",
    "    fig, ax = plot_param_importances(imps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207989b1",
   "metadata": {},
   "source": [
    "Compute and plot 1D marginal distributions. Many of the following functions also accept arguments such as `non_params_to_allow`, which lets you include selected trial user attributes in the marginal computations (treating them as parameters), and `objective`, which allows you to replace the objective value with any other numeric user attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e56647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marginals_1d(\n",
    "    df, val_name,\n",
    "    bins_numeric=7,\n",
    "    non_params_to_allow=[\"user_attrs_n_params\"],\n",
    "    #objective=\"user_attrs_avg_near_best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cdcbe",
   "metadata": {},
   "source": [
    "Plot only a subset of marginals (e.g. most important):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82026b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_params = imps[:1].index.tolist()\n",
    "\n",
    "plot_marginals_1d(\n",
    "    df, val_name,\n",
    "    params=most_imp_params,\n",
    "    bins_numeric=12,\n",
    "    #non_params_to_allow=[\"user_attrs_n_params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd54ec",
   "metadata": {},
   "source": [
    "Detailed summaries for each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10          # Will show the fraction of trials for each parameter choice that belongs to the top_k trials\n",
    "top_frac = 0.20     # Will show the fraction of trials for each parameter choice that belongs to the top_frac trials\n",
    "params = None\n",
    "\n",
    "tbls_sty = display_marginals_1d(\n",
    "    df, val_name,\n",
    "    #params=[],\n",
    "    non_params_to_allow=[\"user_attrs_n_params\"],\n",
    "    #objective=\"user_attrs_avg_near_best\",\n",
    "    top_k=top_k,\n",
    "    top_frac=top_frac,\n",
    "    bins_numeric=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798e4c6",
   "metadata": {},
   "source": [
    "#### Study interactions between hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from heat_forecast.utils.optuna import marginal_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adbcdc",
   "metadata": {},
   "source": [
    "Show parallel coordinate plot (mostly userful when using continuous params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aee8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frac = 0.2   # color only top_frac% of trials\n",
    "params = None    # choose params to plot\n",
    "# params = imps[:6].index.tolist()  # alternative: pick only the most important\n",
    "\n",
    "# === Do not edit below ===\n",
    "fig = plot_parallel_coordinate(study, params=params)\n",
    "\n",
    "vals = df[val_name].dropna().to_list()\n",
    "th = np.quantile(vals, top_frac)\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study, params=params)\n",
    "fig.data[0].dimensions[0].constraintrange = [min(vals), th]\n",
    "fig.update_coloraxes(cmin=min(vals), cmax=max(vals))  \n",
    "\n",
    "html = fig.to_html(include_plotlyjs=\"inline\", full_html=False)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcbb1c1",
   "metadata": {},
   "source": [
    "Below we can visualize pairwise relationships between parameters or user-defined attributes by creating 2D marginal plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heat_forecast.utils.optuna import plot_marginals_2d\n",
    "\n",
    "fig, pivots = plot_marginals_2d(\n",
    "    df, val_name,\n",
    "    #objective=\"user_attrs_avg_near_best\",\n",
    "    #params=[\"model.hidden_size\", \"model.num_layers\"], #imps.index[:3].tolist(),\n",
    "    #as_first=\"model.num_layers\", #imps.index[0],\n",
    "    statistic=\"median\",\n",
    "    binning=\"quantile\",\n",
    "    show_text=True,\n",
    "    bins_a=7,\n",
    "    bins_b=7,\n",
    "    non_params_to_allow=[\"user_attrs_n_params\"]\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ab8a3",
   "metadata": {},
   "source": [
    "Display the tables plotted above, or choose a different statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['params_model.hidden_size', 'params_model.num_layers'])['user_attrs_n_params'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic = \"std\"\n",
    "n_max = 20 # max number of tables to display\n",
    "\n",
    "# === Do not edit below ===\n",
    "for key, pivs in list(pivots.items())[:n_max]:\n",
    "    logging.info(f\"2D marginal for {key}, statistic = '{statistic}':\")\n",
    "    piv = pivs.get(statistic)\n",
    "    display(piv if piv is not None else f\"(Not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821ea7e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae03b7",
   "metadata": {},
   "source": [
    "Final configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02070f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define final configurations for each series and horizon ---\n",
    "base_final_cfgs = TFTRunConfig(\n",
    "    model = TFTModelConfig(\n",
    "        input_chunk_length=168+72,\n",
    "        output_chunk_length=24,\n",
    "        dropout=0.15,\n",
    "        hidden_size=40,\n",
    "        num_attention_heads=1,\n",
    "        lstm_layers=1,\n",
    "    ),\n",
    "    data = DataConfig(\n",
    "        stride=1,\n",
    "        batch_size=64\n",
    "    ),\n",
    "    features = FeatureConfig(),\n",
    "    train = TrainConfig(\n",
    "        lr = 1e-3,\n",
    "        n_epochs = 7,\n",
    "        gradient_clip_val=10.0,\n",
    "        loss_fn_str=\"L1\"\n",
    "    ),\n",
    "    norm = NormalizeConfig(),\n",
    "    seed = SEED,\n",
    ")\n",
    "\n",
    "def final_cfgs(unique_id: str, horizon_type: str) -> TFTRunConfig:\n",
    "    # Define final configurations for different series\n",
    "    if unique_id not in ('F1', 'F2', 'F3', 'F4', 'F5') or horizon_type not in ('day', 'week'):\n",
    "        raise ValueError(\"Invalid unique_id or horizon_type.\")\n",
    "    cfg = copy.deepcopy(base_final_cfgs)\n",
    "    if unique_id == 'F1':\n",
    "        cfg.model.output_chunk_length = 168 if horizon_type == 'week' else 24\n",
    "        cfg.model.dropout =            0.15 if horizon_type == 'week' else 0.05\n",
    "        cfg.train.lr =                 3e-3 if horizon_type == 'week' else 2.5e-3\n",
    "        cfg.train.n_epochs =              4 if horizon_type == 'week' else 6\n",
    "        return cfg\n",
    "    else:\n",
    "        raise NotImplementedError(\"Final configuration not defined for this unique_id yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f471ef8",
   "metadata": {},
   "source": [
    "Code for final testing with the tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = True\n",
    "grid = list(product(['F1'], ['day']))\n",
    "\n",
    "# Set a run_id here for any grid element if you want to resume that run from the last existing checkpoint\n",
    "run_ids_dict: Dict[Tuple[str, str], str] = {\n",
    "    #('F1', 'day'): \n",
    "}\n",
    "\n",
    "# === Do not edit below ===\n",
    "if do_test:\n",
    "    for id, horizon_type in tqdm(grid, desc=\"Test\", leave=True):\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        # --- Create directory for test results ---\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        run_id_was_provided = bool(run_ids_dict.get((id, horizon_type), None))\n",
    "        run_id = run_ids_dict.get((id, horizon_type), None) or f\"{id}_{horizon_type}_test_tft_{timestamp}\"\n",
    "        path = BASE_DIR / \"results\" / \"test\" / \"tft\" / run_id\n",
    "        checkpoint_path = path / \"checkpoint\"\n",
    "        metadata['run_id'] = run_id\n",
    "\n",
    "        try:\n",
    "            if run_id_was_provided:\n",
    "                if not path.exists():\n",
    "                    raise FileNotFoundError(\n",
    "                        f\"Expected existing directory sfor provided run_id '{run_id}', but none was found at: {path}\"\n",
    "                    )\n",
    "                logging.info(f\"Retrieving directory for test results: {path.relative_to(BASE_DIR)}\")\n",
    "            else:\n",
    "                path.mkdir(parents=True, exist_ok=False)\n",
    "                logging.info(f\"Created directory for test results: {path.relative_to(BASE_DIR)}\")\n",
    "\n",
    "            # --- Set params for cv ---\n",
    "            out = get_cv_params_for_test(horizon_type)\n",
    "            metadata['for_cv'] = {\n",
    "                'step_size': out['step_size'],\n",
    "                'test_hours': out['test_hours'],\n",
    "                'end_test_cv': str(out['end_test_actual']),\n",
    "                'n_windows': out['n_windows'],\n",
    "                'refit': out['refit'],\n",
    "                'n_fits': out['n_fits'],\n",
    "            }\n",
    "\n",
    "            # ------------- Run cross-validation with the optimal parameters -------------\n",
    "            # create pipeline and generate futures\n",
    "            heat_id_df = heat_df[heat_df['unique_id'] == id]\n",
    "            aux_id_df = aux_df[aux_df['unique_id'] == id]\n",
    "            config = final_cfgs(id, horizon_type)\n",
    "            pipe = TFTPipeline(\n",
    "                target_df=heat_id_df, \n",
    "                config=config, \n",
    "                aux_df=aux_id_df, \n",
    "            )\n",
    "            metadata['model_config'] = config.to_dict()\n",
    "            metadata['device'] = DEVICE.type\n",
    "\n",
    "            # Run cv\n",
    "            t0 = pd.Timestamp.now()\n",
    "            cv_df = pipe.cross_validation(\n",
    "                test_size=out['test_hours'],  # Test size in hours\n",
    "                end_test=out['end_test_actual'],  # End of the test period\n",
    "                step_size=out['step_size'],   # Step size in hours\n",
    "                refit=out['refit'],  # Do not refit the model on each window\n",
    "                verbose=True,\n",
    "                checkpoint_path=checkpoint_path\n",
    "            )\n",
    "            t1 = pd.Timestamp.now()\n",
    "\n",
    "            avg_elapsed = (t1 - t0).total_seconds() / out['n_fits']\n",
    "            metadata['avg_el_per_fit'] = avg_elapsed\n",
    "\n",
    "            cv_df.to_parquet(path / \"cv_df.parquet\", compression=\"snappy\")\n",
    "\n",
    "            metadata_path = path / 'metadata.yaml'\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                safe_dump_yaml(\n",
    "                    metadata,\n",
    "                    f,\n",
    "                    indent=4, \n",
    "                )\n",
    "\n",
    "            logging.info(f\"✓ Artifacts saved successfully for id={id}, horizon={horizon_type}.\")\n",
    "\n",
    "        except BaseException as e:\n",
    "            if is_keyboard_interrupt_like(e):\n",
    "                logging.warning(\"✗ Detected KeyboardInterrupt. Not cleaning to allow later resumption.\")\n",
    "            else:\n",
    "                logging.exception(\"✗ Error during test for id=%s, horizon=%s. Not cleaning to allow later resumption.\", id, horizon_type)\n",
    "            raise\n",
    "\n",
    "    logging.info(f\"✓ Test completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"F1_day_cpu_times_tft_20251119T152713\"\n",
    "n_warmup = 2\n",
    "n = 8\n",
    "\n",
    "# === Do not edit below ===\n",
    "results_dir = BASE_DIR / \"results\" / \"times\" / \"tft\" / run_id\n",
    "times_path = results_dir / \"times.pkl\"\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(times_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "training_times = np.array(data.get(\"train\", [])[:])\n",
    "inference_times = np.array(data.get(\"inference\", [])) * 1000  # Convert to milliseconds\n",
    "if len(training_times) < n+n_warmup or len(inference_times) < n+n_warmup:\n",
    "    raise ValueError(f\"Not enough runs recorded for stats: \"\n",
    "                     f\"training_times={len(training_times)}, inference_times={len(inference_times)}, required={n+n_warmup}\")\n",
    "logging.info(f\"Using n_warmup={n_warmup} to skip initial runs for stats.\")\n",
    "logging.info(f\"Training times (s): \\n\"\n",
    "             f\"n={n}, \\tmean={training_times[n_warmup:n_warmup+n].mean():.3f}, \\tstd={training_times[n_warmup:n_warmup+n].std():.3f}\")\n",
    "logging.info(f\"Inference times (ms): \\n\"\n",
    "             f\"n={n}, \\tmean={inference_times[n_warmup:n_warmup+n].mean():.3f}, \\tstd={inference_times[n_warmup:n_warmup+n].std():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
